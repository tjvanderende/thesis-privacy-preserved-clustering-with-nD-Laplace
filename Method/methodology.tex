\chapter{Methodology}

To gain insights into the proposed methods for researching the appliance of (ND)-Laplace for cluster algorithms we conducted experiments.
The experiment results are used to evaluate our method against other literature.
In this chapter we explain:
\begin{enumerate}

  \item Datasets
  \item Environmental setup.
  \item For each research question: Description of the different experiments.
  \item For each research question: Results.
\end{enumerate}

\section{Datasets}
For this research, we will use a synthetic dataset for all three research questions.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
  \begin{tabular}{@{}lllll@{}}
    \toprule
    Records & Centers & Dimensions & Standard deviation & Research \\ \midrule
    200     & 4       & 2          & 0.60               & RQ 1     \\ \bottomrule
    200     & 4       & 3          & 0.60               & RQ 2     \\ \bottomrule
    200     & 4       & 5          & 0.60               & RQ 2     \\ \bottomrule
  \end{tabular}
\end{table}

Research question 3 uses a "real-world" dataset to properly assess the different dataset properties that are the subject of this research question.
\todo[inline]{Describe datasets (RQ3)}
\section{Environmental setup}
For running the experiments we make use of 16GB ram memory and i7-10750H 2.6Ghz processor.
The experiments are run using a Docker container which runs a pre-configured distribution of Linux Alpine.
It includes a pre-installed Anaconda environment for python \footnote{https://github.com/devcontainers/images/tree/main/src/anaconda}\footnote{tag: mcr.microsoft.com/devcontainers/anaconda:0-3}.
We run the container using the dev-container feature for visual-studio code \footnote{https://code.visualstudio.com/docs/devcontainers/containers}.
This allows us to create a reproducible experiment environment.
\newpage
\subsection{Libraries \& code versions}
We use python version 3.9.13 with Jupyter notebook for creating a reproducible experimental environment.
The packages for python are:
\begin{enumerate}
  \item Scikit-learn: 1.0.*
  \item Yellow-brick: 1.5
  \item Numpy: 1.24.*
  \item Pandas: 1.4.*
  \item Seaborn: 0.11.*
  \item Mathplotlib: 3.5.*
\end{enumerate}

\section{Methods}
This section explains what methods/ algorithms we used and how we evaluate them.
\subsection{Clustering methods}
\todo[inline]{Exact parameters we used for the algorithms}
\todo[inline]{Which algorithms we used}
\subsection{Evaluation}
With differential privacy, it is a trade-off of utility versus privacy.
Therefore, for the evaluation of the 2D/3D-Laplace algorithms, we compare both criteria to achieve a consensus between utility and privacy.
\subsubsection{Utility}
The utility of the cluster algorithm is decided by calculating the \gls{ami} between the baseline cluster algorithms.
The clustering algorithm is trained using the plain data and functions as the ground truth \citep{9646464,sun_distributed_2019}.
Because of this, we are being able to calculate the \gls{ami} and compare the centroids between the non-private and privately trained clusters.
To reduce the possible bias of results we executed them 10 times for multiple privacy budgets and report the average for each \citep{9679364}.
\todo[inline]{Explain why AMI and not another}

The second way to measure utility is to calculate the error between the non-private and perturbed data \citep{9679364,sun_distributed_2019,xia_distributed_2020-1}.
There are several methods to do this (See \ref{theory:evaluate}), but we use the \gls{aee}.
As with \gls{ami} we run the calculations for multiple privacy budgets 10 times and report the average for each budget.
\todo[inline]{Explain why AEE and not RE}

\subsubsection{Privacy}
The most important one here is the preserving of \gls{gi}.
This validates that we applied the algorithms in the right way and automatically inherit the strong privacy guarantees provided by \gls{gi} (\ref{algo:2d-geo-indistinguishability}).
A disadvantage of this method is that it cannot be used to achieve a clear representation of privacy (it is either "yes" or "no").
Therefore, we also calculate the average Euclidean distance between the non-private and perturbed data.

\subsection{Research question 1}

\mycomment{
  We propose several solutions for open issues based on the theoretical framework. \newline
  \subsubsection{Choosing r: } Based, on the idea of chatzikokolakis et al. to lower the size of the radius if the place is crowded, we can do the same with clustering.
  For this, we could use a metric like the standard division.
  This metric does exactly this, by providing the deviation from the mean:

  This metric increases based on clutteredness of the data, which allows us to generate a radius $r$ automatically regardless of domain.
  Therefore, we depend on the configurability of epsilon entirely on privacy level $l$.
  The generic standard deviation can be defined as:
  \begin{equation}
    \sigma = \sqrt{\frac{\sum{(x_i - \mu)^2}}{n}}
  \end{equation}
  The $\sigma$ being our diameter $d$, the radius $r$ is then calculated as $\frac{d}{2}$. \newline
}
\subsubsection{Truncation: }
We explained the theory for truncation earlier in paragraph \ref{theory:truncation}.
The methods proposed work correctly for a geographic map where other (historic) locations for remapping are available.

However, it is difficult to apply this to data clustering.
The number of data points is not known beforehand, so we may remap to a location that is too far away.
This way we lose important distance information, which hurts the clustering.
Also, the truncation threshold is so clear (the points are outside the known 2D domain), that we do not have to rely on historical data for remapping.
Our algorithm can be much simpler by re-calculating the noise until it will be within the domain:
\mycomment{
  \begin{equation}
    T(x_{max}, x_{min}, z, x_0) \begin{cases} z &\text{if } 0 < 1 \\ T(x_{max}, x_{min}, planarLaplace(epsilon, x_0), x_0)  &\text{else} \end{cases}
  \end{equation}
}
\input{Method/algorithms/truncation-rq1.tex}
This algorithm uses $x_{min}$ and $x_{max}$ to re-calculate the points within the domain using respectively the minimum X/Y and maximum X/Y.
An example of this is visualized:
\begin{figure}[h]
  \includegraphics[width=0.8\textwidth]{Method/images/truncation-rq1.png}
  \label{fig:truncation}
  \centering
  \caption{Representation of the remapping algorithm for clustering for points $z_3$ and $z_4$ }
\end{figure}

\subsubsection{Probability metric $K(x)(Z)$}
\todo[inline]{Explain the probability metric $K$ we used}

\newpage
\subsubsection{Algorithm}
The full algorithm for the perturbation:
\input{Method/algorithms/rq1.tex}
%% We apply the theory for planar laplace proposed by \citep{DBLP:journals/corr/abs-1212-1984}

\subsection{Research question 2}
\todo[inline]{Starts after RQ1}
\subsection{Research question 3}
\todo[inline]{Starts after RQ2}
\section{Results}
\subsection{Research question 1}
For research question 1 the results are 2-dimensional plotted using a line diagram.
\subsubsection{Utility}
\subsubsection{Privacy}

\subsection{Research question 2}
\subsection{Research question 3}