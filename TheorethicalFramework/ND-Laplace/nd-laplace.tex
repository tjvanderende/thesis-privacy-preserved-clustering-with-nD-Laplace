\chapter{nD-Laplace}
In this chapter, we delve deeper into geo-indistinguishability and the various mechanisms that work with it.
We also explain the theory behind the nD-Laplace mechanism and how we modify it and apply it for clustering.
%This explanation is build-up out of several sections, each dedicated to 2, 3, and n-dimensional data.
Because we frequently discuss 2, 3, and dimensional data in our thesis, we decided to prefix the respective Laplace method with this information.
Therefore, for the rest of the thesis, we use the following names:
\begin{enumerate}
  \item 2D-Laplace: This refers to the planar Laplace mechanism \citep{DBLP:journals/corr/abs-1212-1984}.
  \item 3D-Laplace: This refers to the spherical Laplace mechanism \citep{9646489}.
  \item nD-Laplace
\end{enumerate}
For each mechanism, we explain the equation for \gls{gi}, the mechanism, and the data truncation.

\input{TheorethicalFramework/ND-Laplace/2d-laplace.tex}
\input{TheorethicalFramework/ND-Laplace/3d-laplace.tex}
\newpage
\section{nD-Laplace}
As mentioned in the previous chapter, the paper introduced by Min et al. can handle 3-dimensional data.
A small recap: a point $(r, \theta, \psi)$ gives us the three coordinates of a location on the 3-dimensional sphere.
An important property is that these coordinates can be generated separately \citep{DBLP:journals/corr/abs-1212-1984, 9646489}.
The $r$ gives us the radius or distance from $(\theta, \psi)$ to the center of the sphere \footnote{https://mathworld.wolfram.com/SphericalCoordinates.html}.
So, instead of having just these two coordinates, we can extend this to n-dimensions by considering an n-hypersphere \citep{fernandes_generalised_2019, 9646489}.
The theorems for \gls{gi} are extended by Fernandes et al. to support n-dimensional data:
%To this end, in addition to points $\theta$ and $\psi$, we consider $\theta \in S^n$, where $S$ is a unit hypersphere.
\begin{theorem}
  Let $d_x$ be a pseudo-metric on $X$ and let $K: X \rightarrow Z$ be a mechanism satisfying $\epsilon-d_x-privacy$. \\
  $K(x)(Z) \leq e^{\epsilon \cdot d_x (x, x')} \cdot K(x')(Z), \forall x, x' \in X, Z \subseteq X$.
  \label{theorem:nd-laplace}
\end{theorem}
This theorem extends $\epsilon$-geo-indistinguishability to $\epsilon-d_x$-privacy as a more general notion of distinguishability \citep{fernandes_generalised_2019}:
Here, $d_x$ is a pseudo metric which was provided by Chatzikokalakis et al. as elastic privacy definition for location privacy \citep{chatzikokolakis_constructing_2015}. For the use with the Euclidean distance, the privacy definition is defined as $d_{euc}$. Due to the flexibility of $d_x$ the theorem \ref{theorem:nd-laplace} also holds for $d_{euc}$.  Furthermore, $\epsilon-d_{euc}$ provides the same privacy definition as geo-indistinguishability \citep{chatzikokolakis_constructing_2015}. 
Therefore, $d_{euc}$ is adopted by this thesis from now on, to be used for n-dimensions to establish a more general notion of privacy. \newline

The 2D-Laplace mechanism is referenced as a plane, and the coordinates can be generated separately as $(r, \theta)$ \citep{fernandes_generalised_2019,DBLP:journals/corr/abs-1212-1984}.
Hence, generating the nD-variant can be seen as generating multiple $(r, \theta)$ pairs \citep{fernandes_generalised_2019}.
Where $r$ is the radial distance from the origin $x_0$ and $\theta$ is the angle randomly selected from a unit hypersphere.
The selection of $r$ is according to the Gamma distribution, with shape $n$ and scale $\delta > 0$ \cite{fernandes_generalised_2019}:
\begin{equation}
  Gam^n_\delta(r) := \frac{r^{n-1}\cdot^{-\frac{r}{\delta}}}{\delta^n(n-1!)}
\end{equation}
Then, $\theta$ is drawn from a uniform distribution of a unit hypersphere $S$ \citep{fernandes_generalised_2019}:
\begin{equation}
  Uniform^n(\theta) := \frac{\gamma(\frac{n}{2})}{n \cdot \pi ^{\frac{n}{2}}}
\end{equation}
This approach is similar to the other variants of the Laplace mechanism.
Only now, we select points from an $n$-dimensional hypersphere  \footnote{https://mathworld.wolfram.com/SpherePointPicking.html} instead of a unit sphere \ref{eq:3d-laplace-2}. \newline

In the next step, the spherical coordinate representation is converted to Cartesian coordinates \citep{fernandes_generalised_2019}:
It is comparable to the way it was done in the previous chapters; however, as there are an $n$-amount of angles, the equation is repeated and slightly different:
\begin{align}
  x_1 = r * cos (\theta_1)                                          \\
  x_2 = r * sin (\theta_1) * cos (\theta_2)                         \\
  x_{n} = r * sin(\theta_1) … sin(\theta_{n-2}) *cos (\theta_{n-1}) \\
  x_n = r * sin(\theta_{n-1}) * sin(\theta_{n-2}) * sin(\theta_{n-1})
  \label{eq:nd-laplace-cartesian}
\end{align}
\todo[inline]{Needs some extra explanation}
The combination of sections 1 and 2 of this chapter provides a good overview of the solution using a similar image as the 2D and 3D variants (figure \ref{fig:nd-laplace-overview}).
\begin{figure}[H]
  \includesvg[width=0.6\textwidth]{TheorethicalFramework/ND-Laplace/Images/nd_laplace}
  \caption{Overview of the kD-Laplace mechanism}
  \label{fig:nd-laplace-overview}
\end{figure}
\subsection{Privacy versus utility} \label{theory:privacy-utility-nd}
\todo[inline]{Needs refactor}
If we continue adding dimensions, we notice the noise is shrinking proportionally.
We must first examine the formula for a hypersphere’s volume to understand this behavior.
\begin{equation}
  S_n = \frac{2 \pi^{n/2}}{\gamma(\frac{1}{2}n)}
\end{equation}
$\gamma$ is the Gamma distribution determined based on the number of dimensions $n$ \footnote{https://mathworld.wolfram.com/Hypersphere.html}.
\todo[inline]{What are these numbers?}
As the amount of dimensions increases, the most volume is located on the hypersphere surface.
When we convert the points to Cartesian coordinates, some will be located at the center (e.g., 0.5), while others will be close to the surface (e.g., 0.0).
However, as the number of dimensions increases, most will be close to the surface (e.g., 0.99).
The decreasing amount of volume is illustrated using this figure:
\begin{figure}[H]
  \includegraphics[width=0.8\textwidth]{TheorethicalFramework/ND-Laplace/Images/volume.png}
  \caption{Illustration of the decreasing volume while increasing the number of dimensions}
  \label{fig:curse-of-dimensionality}
\end{figure}

The noise decreases as the dimensions increase, increasing utility.
Hence, observing the behavior of privacy relative to utility is intriguing.
This behavior will be further emphasized in a later stage of this research.
\newpage
\subsection{Grid-remapping}
In the previous sections, we introduced the 2D, 3D, and nD Laplace mechanisms.
The current section will be used to formalize nD-Laplace with grid-remapping (disretization \& truncation) and introduce modifications to improve the mechanisms for clustering for n-dimensional data.

\subsubsection{Discretization and truncation} \label{theory:nd-laplace-truncation}
To recall the working of 2D and 3D-Laplace remapping, we give a short summary of these two sections. The discretized 2D version operates on a plane and approximates on a grid $G$, while the 3D version works in a sphere and approximates the data using a cuboid grid  $G_C$.
Given a set of perturbed points $Z$ we can truncate the points that are outside the domain by remapping them to points within $G$ ($Z = X \cap G$) \citep{DBLP:journals/corr/abs-1212-1984}.
Here, $X$ represents other non-private data points reported locally by the same user. 
The following part aims at extending the theorems for 2D and 3D-Laplace for discritezation and truncation to n-dimensions. \newline

The theorems for 2D (See  Theorem\ref{theorem:discretization} and 3D-Laplace (See Theorem \ref{theorem:3d-discretization} include a device precision. This variable is the hardware precision of a GPS provided by the user's device (e.g. a mobile phone). For the purpose of clustering, we omit this for the formulation of truncation with n-dimensions.

We extend the reasoning of Min et al. behind extending 2D-Laplace to 3D-Laplace as this applies to nD-Laplace as well. 
Let $v > w > h$ be a cuboid grid $G_c$, equal to the one provided by Min et al \citep{9646489}. 
We are be-able to extend this by providing a hypercube ($n$-cube) with n-dimensions denoted as $G_n = u_1, u_2, u_3 ... u_n$. This $n$-cube has $2^n$ sides, but can essentially be seen as a generalization of a 3-cube to $n$-dimensions \footnote{https://mathworld.wolfram.com/Hypercube.html}.
An issue arises with the definition of the grid-units, where Min et al. and Andres et al. both consider the possibility of unequal grid-units, for the purpose of generalisation \citep{9646489, DBLP:journals/corr/abs-1212-1984}.
However, for a $n$-cube this is generalisation is harder to justify, because \todo{Add reasoning}.
For this reason, we define $v, w, h = u_1$. We choose $u_1$, but in general this can be any given value $u \in G_n$. 
Another important property, is the maximum diameter defined as $r_M$ \citep{9646489}.
This property can also be converted to support n-dimensions, by calculating the diameter of $G_n$. 
This is the maximum distance between any pair of nodes  [HARARY1988277].:  
The Euclidean distance diameter is defined as (ref):
\begin{equation}
    d_e(G_n) = r_H = \sqrt{n}
\end{equation}

Where, $n$ is the amount of dimensions. 
\newpage  
Finally, we are be-able to establish our own theorem for the discretization and truncation of nD-Laplace:
\begin{theorem}
    Assume $u$ to be a side length, Let $G_u$ be an $n$-dimensional hypercube of $u$ size, Let $r_H < u$ and let $w = \frac{u^2}{r^2_H \cdot sin (\theta)} > \frac{u}{r_H}.$ Let $\epsilon, \epsilon' \in R^+$ such that \\
    $\epsilon' + \frac{1}{u} ln \frac{w + n \cdot e^{\epsilon' \cdot u}}{w - n \cdot e^{\epsilon' \cdot u}} \leq \epsilon$ \\
    Then $K\epsilon'$ satisfies $\epsilon$-geo-indistinguishability within range
    of $r_H$. That is to say, if $d_n(x, z)$ and $d_{euc}(x, x') \leq r_H$ then, 
      $K(x)(z) \leq e^{\epsilon \cdot d_{euc} (x, x')} \cdot K(x')(z)$ forall $x, x' \in X, z \in Z$ \citep{chatzikokolakis_constructing_2015}
\end{theorem}
Since we specify the hypercube to have equal sides of $u_i$, we are be-able to simplify the theorem.
%We use the proof provided by Min et al. in some extend by satisfying preliminary variables \citep{9646489}.
By satisfying the conditions for $r_H$ and $u$, we use the proof provided for 3-dimensions and use it for $n$-dimensions \citep{9646489}.
If we select a $u$ smaller then the max diameter $r_H$, we can satisfy $d_{euc}(x_n, x0) < r_H < u$. Which is according to the proof provided by Min et al.
Moreover, $w = \frac{u^2}{r_H^2 \cdot sin(\theta)} > \frac{u}{r_H}$ is also satisfied as the sizes of $G_n$ are equal. 
Given that these pre-conditions apply, we can reuse the proof from 3-dimensions for n-dimensions. \newline

The utility of this method depends on the number of grid cells in $G_n$ since a smaller distance will result in more frequent mapping to the surface of the grid.
When $\epsilon$ is very low (and thus $z$ is farther away from $x_0$), the data points are more likely to map to the grid surface (\ref{fig:3d-laplace-noise}, \ref{fig:3d-laplace-example}), and if the grid cells width is very high this impacts utility.
So, the number of grid cells could possibly the utility, but this comes at the cost of significantly increased space complexity. Also, when the number of dimensions increases, the number of grid cells grows quadratic. \newline

\subsubsection{Optimization}
To make grid-remapping practically possible with n-dimensions, the data structure has to be more efficient to search spatial data. For this purpose, we adopt the idea proposed by Chatzikokolakis et al. of using a kd-tree to search the grid efficiently \citep{chatzikokolakis_efficient_2017}.
Their research describes the theoretical utilization of a kd-tree for searching nearby points for a given point.
For this reason, we aim to apply this practically by using a kd-tree for the following tasks:
\begin{enumerate}
  \item Finding nearby points for $q \in G_c$ (Section: \ref{theory:grid-remapping}).
  \item Finding nearby points for $x \in X$ and $z \in Z$ (Section: \ref{theory:optimal-remapping}).
\end{enumerate}
The usage of kd-tree does not impact any privacy guarantee provided in the previous section. It is just a more efficient way of searching in n-dimensional spatial structures. Also, for visualization purposes, this section will primarily focus on 2D data. \newline
The next section explains the concept and underlying idea of kd-trees.
Then, we delve into its application for grid remapping.
Finally, we expand this to a practical application
\newpage
\subsubsection*{Kd-trees} \label{theory:kd-trees}
A kd-tree algorithm can search a grid for nearby points \citep{bentley_multidimensional_1975}.
It can do so by recursively splitting the grid into a binary tree to search for grid coordinates \citep{washington_k-d_2002}.
In addition, it preserves spatial information of the data so it can be utilized to find nearby points using Euclidean distance (nearest neighbor search).
The following example provides an idea of how this works (Figure \ref{fig:kd-tree-theory}):
\begin{figure}[H]
  \includegraphics[width=0.8\textwidth]{TheorethicalFramework/ND-Laplace/Images/kd-tree-part1.png}
  \caption{Representation of constructing a kd-tree with 2 dimensions \citep{washington_k-d_2002}.}
  \label{fig:kd-tree-theory}
\end{figure}
Take, for example, the 2D Laplace algorithm that utilizes a plane (left side).
The data points can be divided based on their x and y coordinates.
Each coordinate becomes a node in the binary tree, and the grid is divided based on these splits.
The binary tree allows us to search the grid efficiently.
An example of this is provided in the following image (Figure \ref{fig:kd-tree-searching-theory}):
\begin{figure}[H]
  \includegraphics[width=0.8\textwidth]{TheorethicalFramework/ND-Laplace/Images/kd-tree-part2.png}
  \caption{Representation of searching a kd-tree with 2 dimensions \citep{washington_k-d_2002}.}
  \label{fig:kd-tree-searching-theory}
\end{figure}
In the example, we are searching for all points that fall within the radius of a random query point.
The most significant advantage is that this dramatically reduces the complexity of searching.
Constructing the kd-tree costs equal to grid-remapping $O(kn)$, where $k$ is the number of dimensions and $n$ is the dataset size.
But, searching for the nearest neighbor is a logarithmic function with time complexity of $O(\log n)$ \citep{washington_k-d_2002} (See Figure \ref{theory:big-o-graph} for an overview of the Big-O notation).
This approach is a significant improvement over searching manually, which would have the same complexity as constructing the tree.

\subsubsection{Grid remapping with kd-tree} \label{theory:grid-remapping}
The kd-tree search method is beneficial for the optimizations we are striving for.
This optimization extends grid-remapping for 2d \citep{DBLP:journals/corr/abs-1212-1984} and 3d data \citep{9646489} to n-dimensions. \newline
We have illustrated the three steps required for this below:
\begin{figure}[H]
  \includegraphics[width=1\textwidth]{TheorethicalFramework/ND-Laplace/Images/KD-tree.png}
  \caption{Representation utilizing a kd-tree for applying grid-remapping \citep{DBLP:journals/corr/abs-1212-1984}}
  \label{fig:kd-tree}
\end{figure}
%When using a kd-tree to search for a data point $z_i$, the algorithm begins with an unbalanced binary tree.
%The root is split by the x-axis, and since 4.5 is greater than 3.5, we go to the right.
%This means that we no longer need to consider the left (greyed out) part of the grid.
%We continue traversing the tree until we find the nearest point based on Euclidean distance.
For easy visualization, the process shown above is with 2-dimensional data.
However, this can be extended to n-dimensional using the kd-tree optimization.
The goal is to remap a perturbed point $z \in Z$, outside the original domain $X$, to the closest point in $X$ or $G$ \citep{DBLP:journals/corr/abs-1212-1984}.

Firstly, a grid is generated, where each (blue) point represents the center of a grid cell.
Together, these centroids form the grid dataset, denoted as $G$.
The yellow points and $x_i$ are part of the original collection, denoted as $X$.
Here, $r$ represents the radius used to generate a private version of the data point $x_i$, named $z_i$, based on 2D-Laplace.
In the illustration, you can observe that $z_i$ falls outside the original domain of $X$, so it needs to be remapped.
We accomplish this by utilizing the nearest-neighbor search from the kd-tree algorithm, allowing us to search in $X \cup G$.
Using this algorithm, we can effectively remap point $z \in Z$ to either $X$ or $G$ based on the closest Euclidean distance (Algorithm \ref{alg:grid-remapping-laplace} and Algorithm \ref{alg:find-outside-domain-laplace}).

Although the methodology works for scaling grid-remapping for n-dimensional data, it can still cause scalability issues.
It still does not scale well with the number of dimensions and a more significant number of grid cells.
Therefore, we are looking for a solution that might work better in practice

\input{TheorethicalFramework/ND-Laplace/algorithms/outside-domain.tex}
\input{TheorethicalFramework/ND-Laplace/algorithms/generating-meshgrid-and-remap.tex}
\newpage
\subsection{Density remapping} \label{theory:optimal-remapping}
As discussed, the remapping will be performance intensive to provide good utility, so we adopt the optimal remapping \citep{chatzikokolakis_efficient_2017}.
This thesis will refer to this as density remapping because this better explains the application.

According to the grid-remapping methodology, a point $z_i$ is mapped to the center of the grid cell.
The utility is improved by reducing the distance between $z_i$ and $x_i$, as this is closer to the original data point.
But, to consider the privacy of the data, this can only be done if there is enough indistinguishability between $z_i$ and $x_i$.
To this end, we adopt the remapping approach of Chatzikokolakis et al. to remap based on the data density \citep{chatzikokolakis_efficient_2017}.
The remapping algorithm works on the idea of crowded places \ref{2d:optimizing}, with the intuition that a crowded place leverages indistinguishability by crowdedness (density) \citep{chatzikokolakis_efficient_2017}.  \newline

The approach is visualized using the following figure:
\begin{figure}[H]
  \includesvg{TheorethicalFramework/ND-Laplace/Images/master-thesis-Page-12.svg}
  \label{fig:optimal-remapping}
  \caption{Representation of density remapping \citep{chatzikokolakis_efficient_2017}}
\end{figure}
The first step is calculating $B_r(z_i)$, which refers to all the data points within the radius $r$ around the data point $z_i$ \citep{chatzikokolakis_efficient_2017}.
Then, the algorithm collects the data points around $x_i$, again using $r$, to determine the density of $x_i$.
This is the convex hull (collection) of all the original data points within the radius $r$ around $x_i$ and is denoted as $Q_r (x_i)$.
Finally, we obtain $Q_r$ which is a union of $Q_r (x_i)$ and $B_r(z_i)$.
Now that we have the sets of points around $x_i$ and $z_i$, we can calculate the density \citep{chatzikokolakis_efficient_2017}:
\begin{equation}
  \forall x'_i \in Q_r \quad \sigma(x_i) = \frac{w(x'_i)e^{-\epsilon d(x'_i, z_i)}}{\sum{_{q\in Q_r} w(q)e^{-\epsilon d(q, z_i)}}}
  \label{eq:optimal-remapping-formula-1}
\end{equation}
$w(q)$ is the weight of a point $q \in Q_r$, and $w(x_i)$ the weight of a point $x_i \in X$.
Here, the weight can be a point of interest \citep{chatzikokolakis_efficient_2017}.
But, in the context of our study, we consider the points within $r$ of $q$ or $x_i$ to be the weight.

In equation \ref{eq:optimal-remapping-formula-1}, the $w(x)$ and $w(q)$ are normalization factors for estimating the likelihood of a point $x'_i$ being close to $z_i$ and $q$ being close to $z_i$.
So, this remapping is according to the original definition of the 2D Laplace \gls{pdf} definition \ref{eq:polar-laplace-pdf}.
The outcome of the formula is a collection of values that indicate the degree of density $sigma(x_i)$.
The new value $z'_i$ is calculated by taking the mean of the $sigma(x_i)$ \citep{chatzikokolakis_efficient_2017}:
\begin{equation}
  z_i' = \sum_{x'_i \in Q_r} \sigma(x'_i) * x'_i
  \label{eq:optimal-remapping-formula-2}
\end{equation}
By applying this formula, the new $z_i'$ is closer to $x_i$ to minimize the expected loss of utility \citep{chatzikokolakis_efficient_2017}.
Much like the 2D and 3D Laplace methods, which use the likelihood of a point $z_i$ being close to $x_i$ \citep{DBLP:journals/corr/abs-1212-1984, 9646489}.
%The first step is to calculate the coefficients for each point $x \in X$ by multiplying the density $\sigma(x)$ with the original point $x$.

%The probability that $z'$ will be closer to $x$ is higher, and this probability is calculated based on the original value $x \in X$ to first determine the coefficient.
%Finally, the new $z'$ is calculated by taking the mean value of $\sigma(x)$ \citep{chatzikokolakis_efficient_2017}.
%As described by chatzikokolakis et al., the 
%Chatzikokolakis et al's work considers a prior set of data point $Q \in R^n$.
%Which are other data points that belong to the user.
%We are interested in the data points that are within the radius $r$ around $z$.
%This is denoted as $B_r(z)$, which is the vector of all points within radius $r$ around $z$.
%In addition to this, there is also a $Q_r$ that is a convex hull of all nearby points.
%Hence, this is described as $Q_r = B_r \cap Q$.
%The final intuition here is that $r$ is automatically generated based on crowdedness (see circle inside figure \ref{fig:optimal-remapping}).
%The last step is to take the mean value of $\sigma(x)$.
%Unfortunately, optimal remapping is not possible for users that do not have sufficient data (e.g. new users).
%The remapping is not applied for these users and is also not applied for $z$ if it is within the domain of $X$.
\input{TheorethicalFramework/ND-Laplace/algorithms/optimal-remapping.tex}
\newpage
\subsection{Putting it together: kd-Laplace}
Now that we have defined everything, we can write the algorithm in a step-by-step manner:
\input{TheorethicalFramework/ND-Laplace/algorithms/nd-laplace.tex}

It is important to note that with the introduction of this algorithm, it is no longer a non-interactive method, as the data points now interact with each other.
Therefore, we expect the introduction of optimal grid remapping with kd-tree will bring more utility \citep{wang_comprehensive_2020, xiongComprehensiveSurveyLocal2020}.
The introduction of the kd-tree is also why we present our method as \textit{kd-Laplace}, where $k$ is the number of dimensions.
%\todo[inline]{Also refactor this}

%\subsection{Extending to $d_x$-privacy}
%\todo[inline]{Find if this is possible}

%Constructing elastic distinguishability metrics for location privacy
\newpage

\input{TheorethicalFramework/ND-Laplace/mechanism-design.tex}
