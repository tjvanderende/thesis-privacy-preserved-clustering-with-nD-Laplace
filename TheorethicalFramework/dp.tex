\input{TheorethicalFramework/Differential privacy/dp-math-symbols.tex}
\section{Differential privacy} \label{section:dp}
\todo[inline]{Explain general notion of privacy}
\glsaddall
\leading{10pt}
\printglossary[type=differential-privacy, nonumberlist]
\begin{figure}[h]
  \includegraphics{TheorethicalFramework/Differential privacy/master-thesis-Differential privacy illustration.png}
  \caption{Randomization function $K$ gives $\epsilon$-differential privacy for all elements in $D_1$ and $D_2$ if they differ at most one element. \citep{dwork_differential_2006}}
  \label{fig:definition-dp}
\end{figure}
The privacy budget $\epsilon$ determines the amount of noise that is added.
\subsection{Laplace algorithm}
One way to achieve $\epsilon$-DP is using sampling noise from the Laplace or Gaussian distributions.
\todo[inline]{Explain gaussian / laplace distributions}
The noise is then based on the sensitivity of a function $f$.
This is the maximal possible change when adding or removing a single record \citep{friedman_data_2010, dwork_differential_2006}.
\begin{equation}
  \Delta f = max_{D_1, D_2} ||f(D_1) - f(D_2)||
\end{equation}
% This means, that if the sensitivity is low, the noise is as well.
% The metric is combined with the privacy budget $\epsilon$ to control the noise that is being added by a mechanism like Laplace \citep{friedman_data_2010}.
\todo[inline]{Explain differential privacy implementation with La place distribution}



\subsection{Local differential privacy}
\subsection{Geo-indistinguishability}
\subsection{Attacks on privacy}
\todo[inline]{In progress}
Jayaraman et al. describe inference attacks as an important attack type for machine learning \citep{jayaraman_evaluating_nodate}.
The study evaluates two types of attacks:
\begin{enumerate}
  \item Membership inference: An adversary attempts to infer the original data point $x \in X$ from a given data point $z \in Z$.
        The adversary has access to a point $z$, the size of the dataset $|Z|$ and a distribution D where $Z$ was drawn from \citep{yeom_privacy_2018}.
  \item Attribute inference:
\end{enumerate}
The experiments are focused on using two frameworks for evaluating privacy and the privacy is expressed using a "privacy leakage" \citep{jayaraman_evaluating_nodate}.
Where privacy leakage is described as the advantage of attack's advantage for an inference attack \cite{yeom_privacy_2018}.
The first framework and the simplest one \citep{jayaraman_evaluating_nodate} is that of Yeom et al.
Both inference attacks can be calculated using this framework.
They assume that an attacker knows the standard error and has access to the perturbation dataset.
So, their algorithm is be-able to extract the truth label by minimizing the loss \citep{yeom_privacy_2018}.
The attacker is be-able to brute-force every possible value $z \in Z$ until the target model's loss is the smallest possible \cite{jayaraman_evaluating_nodate}.
The second framework was established by Shokri et al. and is more like a black-box approach.
This method evolves around the attacker generating a shadow model, with as goal to overfit.
If the data is fed with real data the score is higher than similar data, which means the real data can be inferred \citep{shokri_membership_2017,jayaraman_evaluating_nodate}
Although these metrics are of interest for differential privacy, they both require ground truth labels (e.g for supervised learning).
\todo[inline]{In progress}
%The above attacks mainly target the clustering method after they have been trained. 
%$Various attacks are more focused on data, like 
\subsection{Evaluation methods} \label{theory:evaluation-dp}
It is possible to evaluate and measure the impact of the noise between two distributions by calculating the error between the non-private and private data \citep{del_rey_comprehensive_2020-1}.
Two metrics that are proposed by the same study are Mean Squared Error (MSE) and Mean Average Error (MAE).
These metrics can be used to calculate the error between $X$ and the perturbed dataset $Z$. \newline

Just as it is possible to measure the utility, this can also be done with privacy.
When performing a privacy algorithm, it can be proven whether a method meets the privacy requirements.
These are metrics such as $\epsilon$-differential-privacy (\ref{fig:definition-dp}) and $\epsilon$-geo-indistinguishability (see next chapter \ref{algo:2d-geo-indistinguishability}).
Although these methods give an idea of privacy, it can only be "yes" or "no".
Furthermore, it can give a distorted image, since a chance of 70\% also gives a "yes" according to the definition of geo-indistinguishability \citep{oya_is_2017}.
In other words, to gain more insight into the amount of privacy (such as with MSE or MAE), other metrics are needed.

For this reason, Oya et al. introduced a metric for geo-indistinguishability that makes it possible to give percentages in their study \citep{oya_is_2017}:
As an example, an adversary is given that guesses between two locations: $x \in X$ and  $x' \in X$.
\begin{equation}
  p_e (x, x', z) \leq p^*_e = \frac{1}{1 + e^{e * d(x, x')}}
  \label{eq:geo-as-an-error}
\end{equation}
Where privacy level $p^*_e$ is the lower bound of the probability of an adversary guessing correctly.
The method is called $\epsilon$-geo-indistinguishability as error.
Based on this metric, it can be calculated that an adversary has an average of 90\% chance to guess a location correctly.
In that case, the algorithm would be $\epsilon$-geo-indistinguishability, but in practice not.