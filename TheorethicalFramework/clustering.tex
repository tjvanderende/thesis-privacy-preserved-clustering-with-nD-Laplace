\section{Clustering}

\subsection{Methods}
\todo[inline]{Describe each cluster method with important parameters that could influence the outcome}
\subsection{Evaluation methods} \label{theory:evaluate}
Clustering comparison measures are important in cluster analysis for external validation by comparing clustering solutions to a "ground truth" clustering \citep{vinh_information_nodate-2}.
These external validity indices are a common way to assess the quality of unsupervised machine learning methods like clustering \citep{warrens_understanding_2022}.
A method that could be used for this is the Rand Index \cite{rand_objective_1971}.
It is a commonly applied method for comparing two different cluster algorithms \cite{wagner_comparing_nodate}.
An improvement of this method is adjusted for chance by considering the similarity of pairwise cluster comparisons \cite{vinh_information_nodate-2}.
Both the Rand Index (RI) and Adjusted Rand Index (ARI) \citep{hubert_comparing_1985-1} report a value between 0 and 1.
Where 0 is for no-similarity and 1 for identical clusters.
Alternatives for RI are the Fowles-Mallows Index and Mirkin Metric.
However, these two methods have their disadvantages. Respectively, being sensitive to a few clusters and cluster sizes \citep{wagner_comparing_nodate}.
The ARI metric suffers from cluster size imbalance as well, so it only provides not a lot of information on smaller clusters \citep{warrens_understanding_2022}.
Instead, they recommend using the cluster index metric that was proposed by Fr√§nti et al. \citep{franti_centroid_2014}.

Another popular group of methods are the information theoric-based measures \cite{vinh_information_nodate-2}.
This metric measures the information between centroids \cite{vinh_information_nodate-2}.
\todo[inline]{AMI versus other methods}

In addition to the cluster evaluation, it is also possible to evaluate and measure the impact of the noise between two distributions.

\todo[inline]{RE versus other methods}
