@book{abdi_principal_2010,
  title = {Principal {{Component Analysis}}},
  author = {Abdi, Herv{\'e} and Williams, Lynne J.},
  year = {2010},
  abstract = {Principal component analysis (pca) is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the pca model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. Pca can be generalized as correspondence analysis (ca) in order to handle qualitative variables and as multiple factor analysis (mfa) in order to handle heterogenous sets of variables. Mathematically, pca depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition (svd) of rectangular matrices.}
}

@article{abdullah_advanced_2017,
  title = {Advanced {{Encryption Standard}} ({{AES}}) {{Algorithm}} to {{Encrypt}} and {{Decrypt Data}}},
  author = {Abdullah, Ako},
  year = {2017},
  month = jun,
  abstract = {ABSTRACT\textemdash{} Advanced Encryption Standard (AES) algorithm is one on the most common and widely symmetric block cipher algorithm used in worldwide. This algorithm has an own particular structure to encrypt and decrypt sensitive data and is applied in hardware and software all over the world. It is extremely difficult to hackers to get the real data when encrypting by AES algorithm. Till date is not any evidence to crake this algorithm. AES has the ability to deal with three different key sizes such as AES 128, 192 and 256 bit and each of this ciphers has 128 bit block size. This paper will provide an overview of AES algorithm and explain several crucial features of this algorithm in details and demonstration some previous researches that have done on it with comparing to other algorithms such as DES, 3DES, Blowfish etc.}
}

@article{abood_survey_2018,
  title = {A {{Survey}} on {{Cryptography Algorithms}}},
  author = {Abood, Omar and Guirguis, Shawkat},
  year = {2018},
  month = jul,
  journal = {International Journal of Scientific and Research Publications},
  volume = {8},
  pages = {495--516},
  doi = {10.29322/IJSRP.8.7.2018.p7978}
}

@article{abood_survey_2018-1,
  title = {A {{Survey}} on {{Cryptography Algorithms}}},
  author = {Abood, Omar and Guirguis, Shawkat},
  year = {2018},
  month = jul,
  journal = {International Journal of Scientific and Research Publications},
  volume = {8},
  pages = {495--516},
  doi = {10.29322/IJSRP.8.7.2018.p7978},
  abstract = {With the major advancements in the field of technology and electronics, one persistent obstacle has proven to be one of the major challenges, namely : Data Security. To get connected securely and quickly through the electronic data transfer through the web, the data should be encrypted. Encryption is the process of transforming plain text into ciphered-text, which cannot be understood or altered easily by undesirable people. It can also be defined as the science that uses mathematics in data encryption and decryption operations. In this paper, we discuss several important algorithms used for the encryption and decryption of data in all fields, to make a comparative study for most important algorithms in terms of data security effectiveness, key size, complexity and time, etc. This research focused on different types of cryptography algorithms that are existing, like AES, DES, TDES, DSA, RSA, ECC, EEE and CR4\ldots etc.}
}

@incollection{abrahamsson_devops_2016,
  title = {{{DevOps Adoption Benefits}} and {{Challenges}} in {{Practice}}: {{A Case Study}}},
  shorttitle = {{{DevOps Adoption Benefits}} and {{Challenges}} in {{Practice}}},
  booktitle = {Product-{{Focused Software Process Improvement}}},
  author = {{Riungu-Kalliosaari}, Leah and M{\"a}kinen, Simo and Lwakatare, Lucy Ellen and Tiihonen, Juha and M{\"a}nnist{\"o}, Tomi},
  editor = {Abrahamsson, Pekka and Jedlitschka, Andreas and Nguyen Duc, Anh and Felderer, Michael and Amasaki, Sousuke and Mikkonen, Tommi},
  year = {2016},
  volume = {10027},
  pages = {590--597},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-49094-6_44},
  abstract = {DevOps is an approach in which traditional software engineering roles are merged and communication is enhanced to improve the production release frequency and maintain software quality. There seem to be benefits in adopting DevOps but practical industry experiences have seldom been reported. We conducted a qualitative multiple-case study and interviewed the representatives of three software development organizations in Finland. The responses indicate that with DevOps, practitioners can increase the frequency of releases and improve test automation practices. DevOps was seen to encourage collaboration between departments which boosts communication and employee welfare. Continuous releases enable a more experimental approach and rapid feedback collection. The challenges include communication structures that hinder cross-department collaboration and having to address the cultural shift. Dissimilar development and production environments were mentioned as some of the technical barriers. DevOps might not also be suitable for all industries. Ambiguity in the definition of DevOps makes adoption difficult since organizations might not know which practices they should implement for DevOps.},
  isbn = {978-3-319-49093-9 978-3-319-49094-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YIGW2PAA\\Riungu-Kalliosaari e.a. - 2016 - DevOps Adoption Benefits and Challenges in Practic.pdf}
}

@inproceedings{adrian_imperfect_2015,
  title = {Imperfect Forward Secrecy: {{How Diffie-Hellman}} Fails in Practice},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Adrian, David and Bhargavan, Karthikeyan and Durumeric, Zakir and Gaudry, Pierrick and Green, Matthew and Halderman, J Alex and Heninger, Nadia and Springall, Drew and Thom{\'e}, Emmanuel and Valenta, Luke},
  year = {2015},
  pages = {5--17},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7V6U69UG\\Adrian e.a. - 2015 - Imperfect forward secrecy How Diffie-Hellman fail.pdf}
}

@inproceedings{adrian_imperfect_2015-1,
  title = {Imperfect Forward Secrecy: {{How Diffie-Hellman}} Fails in Practice},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Adrian, David and Bhargavan, Karthikeyan and Durumeric, Zakir and Gaudry, Pierrick and Green, Matthew and Halderman, J Alex and Heninger, Nadia and Springall, Drew and Thom{\'e}, Emmanuel and Valenta, Luke},
  year = {2015},
  pages = {4}
}

@book{agresti_introduction_2018,
  title = {An {{Introduction}} to {{Categorical Data Analysis}}},
  author = {Agresti, Alan},
  year = {2018},
  month = nov,
  publisher = {{John Wiley \& Sons}},
  abstract = {A valuable new edition of a standard reference The use of statistical methods for categorical data has increased dramatically, particularly for applications in the biomedical and social sciences. An Introduction to Categorical Data Analysis, Third Edition summarizes these methods and shows readers how to use them using software. Readers will find a unified generalized linear models approach that connects logistic regression and loglinear models for discrete data with normal regression for continuous data. Adding to the value in the new edition is: \textbullet{} Illustrations of the use of R software to perform all the analyses in the book \textbullet{} A new chapter on alternative methods for categorical data, including smoothing and regularization methods (such as the lasso), classification methods such as linear discriminant analysis and classification trees, and cluster analysis \textbullet{} New sections in many chapters introducing the Bayesian approach for the methods of that chapter \textbullet{} More than 70 analyses of data sets to illustrate application of the methods, and about 200 exercises, many containing other data sets \textbullet{} An appendix showing how to use SAS, Stata, and SPSS, and an appendix with short solutions to most odd-numbered exercises Written in an applied, nontechnical style, this book illustrates the methods using a wide variety of real data, including medical clinical trials, environmental questions, drug use by teenagers, horseshoe crab mating, basketball shooting, correlates of happiness, and much more. An Introduction to Categorical Data Analysis, Third Edition is an invaluable tool for statisticians and biostatisticians as well as methodologists in the social and behavioral sciences, medicine and public health, marketing, education, and the biological and agricultural sciences.},
  googlebooks = {ukNxDwAAQBAJ},
  isbn = {978-1-119-40526-9},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{akoka_entity-relationship_1996,
  title = {Entity-Relationship and Object-Oriented Model Automatic Clustering},
  author = {Akoka, J. and {Comyn-Wattiau}, I.},
  year = {1996},
  month = oct,
  journal = {Data \& Knowledge Engineering},
  volume = {20},
  number = {2},
  pages = {87--117},
  issn = {0169023X},
  doi = {10.1016/S0169-023X(96)00007-9},
  abstract = {Automatic clustering of semantic models allows a multilevel abstraction of the same reality and reduces design complexity. This paper is concerned with the development and application of automatic clustering of Entity-Relationship (E-R) diagrams and object models. This paper does not consider physical clustering which aims at improving performance of databases but deals with conceptual clustering whose objective is to facilitate the understanding of the database schema. We provide an automatization of conceptual schema clustering leading to a unification of past approaches. Automatization is achieved through the definition of semantic distances between concepts and the use of a clustering algorithm. For entity-relationship clustering, we define three different distances (visual, hierarchical and cohesive) depending on the semantic richness. Object model clustering is based on structural, semantic and communication characteristics of objects. Our approach has been implemented and applied to a great number of examples, leading to interesting results. Applications of automatic clustering in several areas and their potential as a communication, documentation and design tools for E-R diagrams and obiect oriented models are described and discussed.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\L5D9NC3U\\Akoka en Comyn-Wattiau - 1996 - Entity-relationship and object-oriented model auto.pdf}
}

@inproceedings{al-debagy_comparative_2018,
  title = {A {{Comparative Review}} of {{Microservices}} and {{Monolithic Architectures}}},
  booktitle = {2018 {{IEEE}} 18th {{International Symposium}} on {{Computational Intelligence}} and {{Informatics}} ({{CINTI}})},
  author = {{Al-Debagy}, Omar and Martinek, Peter},
  year = {2018},
  month = nov,
  pages = {000149--000154},
  publisher = {{IEEE}},
  address = {{Budapest, Hungary}},
  doi = {10.1109/CINTI.2018.8928192},
  abstract = {Microservices' architecture is getting attention in the academic community and the industry, and mostly is compared with monolithic architecture. Plenty of the results of these research papers contradict each other regarding the performance of these architectures. Therefore, these two architectures are compared in this paper, and some specific configurations of microservices' applications are evaluated as well in the term of service discovery. Monolithic architecture in concurrency testing showed better performance in throughput by 6\% when compared to microservices architecture. The load testing scenario did not present significant difference between the two architectures. Furthermore, a third test comparing microservices applications built with different service discovery technologies such as Consul and Eureka showed that applications with Consul presented better results in terms of throughput.},
  isbn = {978-1-72811-117-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9EJ8XICN\\Al-Debagy en Martinek - 2018 - A Comparative Review of Microservices and Monolith.pdf}
}

@article{al-smadi_arma_2004,
  title = {{{ARMA Model}} Order Estimation Using Third Order Computations},
  author = {{Al-Smadi}, Adnan},
  year = {2004},
  month = dec,
  journal = {International Journal of General Systems},
  volume = {33},
  number = {6},
  pages = {611--620},
  issn = {03081079},
  doi = {10.1080/03081070512331318338},
  abstract = {A novel approach for estimating the order of a non-Gaussian autoregressive moving-average process is proposed. The proposed method uses third order computations, and is based on the minimum eigenvalue of a family of covariance matrices derived from the observed output data. The algorithm uses data matrices rather than calculated cumulants. Hence, we avoid the non-stationary effects, which is due to finite-length observations. Examples are given to demonstrate the performance of the proposed algorithm.},
  keywords = {ABSTRACT algebra,ALGORITHMS,ARMA,GAUSSIAN measures,MATRICES,Model order,Non-Gaussian,Non-stationary effects,STOCHASTIC processes,UNIVERSAL algebra},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CAX6WH8D\\Al-Smadi - 2004 - ARMA Model order estimation using third order comp.pdf}
}

@article{alfaras_fast_2019,
  title = {A {{Fast Machine Learning Model}} for {{ECG-Based Heartbeat Classification}} and {{Arrhythmia Detection}}},
  author = {Alfaras, Miquel and Soriano, Miguel C. and Ort{\'i}n, Silvia},
  year = {2019},
  month = jul,
  journal = {Frontiers in Physics},
  volume = {7},
  issn = {2296-424X},
  doi = {10.3389/fphy.2019.00103},
  abstract = {DOAJ is an online directory that indexes and provides access to quality open access, peer-reviewed journals.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TIIWH58J\\Alfaras e.a. - 2019 - A Fast Machine Learning Model for ECG-Based Heartb.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\QBJ6FY7P\\8819c6c1d48c456caebd40aab9a765a8.html}
}

@inproceedings{alshuqayran_systematic_2016,
  title = {A Systematic Mapping Study in Microservice Architecture},
  booktitle = {2016 {{IEEE}} 9th {{International Conference}} on {{Service-Oriented Computing}} and {{Applications}} ({{SOCA}})},
  author = {Alshuqayran, Nuha and Ali, Nour and Evans, Roger},
  year = {2016},
  pages = {44--51},
  publisher = {{IEEE}},
  isbn = {1-5090-4781-6}
}

@article{alshuqayran_systematic_nodate,
  title = {A {{Systematic Mapping Study}} in {{Microservice Architecture}}},
  author = {Alshuqayran, Nuha and Ali, Nour and Evans, Roger},
  pages = {8},
  abstract = {The accelerating progress of network speed, reliability and security creates an increasing demand to move software and services from being stored and processed locally on users' machines to being managed by third parties that are accessible through the network. This has created the need to develop new software development methods and software architectural styles that meet these new demands. One such example in software architectural design is the recent emergence of the microservices architecture to address the maintenance and scalability demands of online service providers. As microservice architecture is a new research area, the need for a systematic mapping study is crucial in order to summarise the progress so far and identify the gaps and requirements for future studies. In this paper we present a systematic mapping study of microservices architectures and their implementation. Our study focuses on identifying architectural challenges, the architectural diagrams/views and quality attributes related to microsevice systems.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MV575Q8U\\Alshuqayran e.a. - A Systematic Mapping Study in Microservice Archite.pdf}
}

@article{alsultan_keystroke_nodate,
  title = {{Keystroke Dynamics Authentication: A Survey of Free-text Methods}},
  author = {Alsultan, Arwa and Warwick, Kevin},
  langid = {Engels}
}

@inproceedings{alves_deriving_2010,
  title = {Deriving Metric Thresholds from Benchmark Data},
  booktitle = {{{IEEE International Conference}} on {{Software Maintenance}}, {{ICSM}}},
  author = {Alves, Tiago and Ypma, Christiaan and Visser, Joost},
  year = {2010},
  month = oct,
  pages = {1--10},
  doi = {10.1109/ICSM.2010.5609747},
  abstract = {A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations. Previously proposed methodologies for systematically deriving metric thresholds have made unjustified assumptions about the statistical properties of source code metrics. As a result, the general applicability of the derived thresholds is jeopardized. We designed a method that determines metric thresholds empirically from measurement data. The measurement data for different software systems are pooled and aggregated after which thresholds are selected that (i) bring out the metric's variability between systems and (ii) help focus on a reasonable percentage of the source code volume. Our method respects the distributions and scales of source code metrics, and it is resilient against outliers in metric values or system size. We applied our method to a benchmark of 100 object-oriented software systems, both proprietary and open-source, to derive thresholds for metrics included in the SIG maintainability model.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\FFR8QL2X\\Alves e.a. - 2010 - Deriving metric thresholds from benchmark data.pdf}
}

@book{aly_survey_2005,
  title = {Survey on {{Multiclass Classification Methods}}},
  author = {Aly, Mohamed},
  year = {2005},
  abstract = {Supervised classification algorithms aim at producing a learning model from a labeled training set. Various successful techniques have been proposed to solve the problem in the binary classification case. The multiclass classification case is more delicate, as many of the algorithms were introduced basically to solve binary classification problems. In this short survey we investigate the various techniques for solving the multiclass classification problem. 1}
}

@inbook{ambriola_introduction_1993,
  title = {{{AN INTRODUCTION TO SOFTWARE ARCHITECTURE}}},
  booktitle = {Series on {{Software Engineering}} and {{Knowledge Engineering}}},
  author = {Garlan, David and Shaw, Mary},
  year = {1993},
  month = dec,
  volume = {2},
  pages = {2},
  publisher = {{WORLD SCIENTIFIC}},
  doi = {10.1142/9789812798039_0001},
  abstract = {This work was funded in part by the Department of Defense Advanced Research Project Agency under grant MDA972-92-J-1002, by National Science Foundation Grants CCR-9109469 and CCR-9112880, and by a grant from Siemens Corporate Research. It was also funded in part by the Carnegie Mellon University School of Computer Science and Software Engineering Institute (which is sponsored by the U.S. Department of Defense). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government, the Department of Defense, the National Science Foundation, Siemens Corporation, or Carnegie Mellon University.},
  collaborator = {Ambriola, Vincenzo and Tortora, Genoveffa},
  isbn = {978-981-02-1594-1 978-981-279-803-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2YDNW7SN\\Garlan en Shaw - 1993 - AN INTRODUCTION TO SOFTWARE ARCHITECTURE.pdf}
}

@article{antal_keystroke_2015,
  title = {Keystroke {{Dynamics}} on {{Android Platform}}},
  author = {Antal, Margit and Szab{\'o}, L{\'a}szl{\'o} Zsolt and L{\'a}szl{\'o}, Izabella},
  year = {2015},
  month = jan,
  journal = {Procedia Technology},
  series = {8th {{International Conference Interdisciplinarity}} in {{Engineering}}, {{INTER-ENG}} 2014, 9-10 {{October}} 2014, {{Tirgu Mures}}, {{Romania}}},
  volume = {19},
  pages = {820--826},
  issn = {2212-0173},
  doi = {10.1016/j.protcy.2015.02.118},
  abstract = {Currently people store more and more sensitive data on their mobile devices. Therefore it is highly important to strengthen the existing authentication mechanisms. The analysis of typing patterns, formally known as keystroke dynamics is useful to enhance the security of password-based authentication. Moreover, touchscreen allows adding features ranging from pressure of the screen or finger area to the classical time-based features used for keystroke dynamics. In this paper we examine the effect of these additional touchscreen features to the identification and verification performance through our dataset of 42 users. Results show that these additional features enhance the accuracy of both processes.},
  langid = {english},
  keywords = {Behavioral Biometric,Keystroke Dynamics,Security,Touch Features,User Authentication},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\W2DN7AS9\\Antal e.a. - 2015 - Keystroke Dynamics on Android Platform.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\A3PGYPHE\\S221201731500119X.html}
}

@book{anton_elementary_1987,
  title = {Elementary Linear Algebra},
  author = {Anton, Howard},
  year = {1987},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-84819-6 978-0-471-85223-0},
  langid = {english},
  annotation = {OCLC: 13580207},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TJSZBLZ7\\Anton - 1987 - Elementary linear algebra.pdf}
}

@book{anzul_doing_2003,
  title = {Doing {{Qualitative Research}}: {{Circles Within Circles}}},
  author = {Anzul, M. and Ely, M. and Freidman, T. and Garner, D. and {McCormack-Steinmetz}, A.},
  year = {2003},
  series = {Teachers' {{Library}}},
  publisher = {{Taylor \& Francis}},
  isbn = {978-1-135-38662-7}
}

@book{arthur_big_2013,
  title = {Big {{Data Marketing}}: {{Engage Your Customers More Effectively}} and {{Drive Value}}},
  shorttitle = {Big {{Data Marketing}}},
  author = {Arthur, Lisa},
  year = {2013},
  publisher = {{John Wiley \& Sons, Incorporated}},
  address = {{Somerset, UNITED STATES}},
  abstract = {Leverage big data insights to improve customer experiences and insure business success Many of today's businesses find themselves caught in a snarl of internal data, paralyzed by internal silos, and executing antiquated marketing approaches. As a result, consumers are losing patience, shareholders are clamoring for growth and differentiation, and marketers are left struggling to untangle the massive mess. Big Data Marketing provides a strategic road map for executives who want to clear the chaos and start driving competitive advantage and top line growth. Using real-world examples, non-technical language, additional downloadable resources, and a healthy dose of humor, Big Data Marketing will help you discover the remedy offered by data-driven marketing. Explains how marketers can use data to learn what they need to know Details strategies to drive marketing relevance and Return On Marketing Investment (ROMI) Provides a five-step approach in the journey to a more data-driven marketing organization Author Lisa Arthur, the Chief Marketing Officer for Teradata Applications, the leader in integrated marketing software, meets with thousands of CMOs and marketing professionals annually through public speaking and events Big Data Marketing reveals patterns in your customers' behavior and proven ways to elevate customer experiences. Leverage these insights to insure your business's success.},
  isbn = {978-1-118-73405-6},
  keywords = {Internet marketing.,Marketing -- Data processing.,Marketing -- Management.,Marketing research -- Statistical methods.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\IIIM9UWK\\detail.html}
}

@book{arthur_big_2013-1,
  title = {Big {{Data Terminology}} (p.47)},
  author = {Arthur, Lisa},
  year = {2013},
  publisher = {{John Wiley \& Sons, Incorporated}},
  address = {{Somerset, UNITED STATES}},
  isbn = {978-1-118-73405-6},
  keywords = {Internet marketing.,Marketing -- Data processing.,Marketing -- Management.,Marketing research -- Statistical methods.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NX4R53YJ\\reader.html}
}

@misc{auth0.com_jwt.io_nodate,
  title = {{{JWT}}.{{IO}} - {{JSON Web Tokens Introduction}}},
  author = {{auth0.com}},
  journal = {Introduction to JSON Web Tokens},
  abstract = {Learn about JSON Web Tokens, what are they, how they work, when and why you should use them.},
  howpublished = {http://jwt.io/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GTVS7E8T\\introduction.html}
}

@article{avila_applications_2017,
  title = {Applications Based on Service-Oriented Architecture ({{SOA}}) in the Field of Home Healthcare},
  author = {Avila, Karen and Sanmartin, Paul and Jabba, Daladier and Jimeno, Miguel},
  year = {2017},
  journal = {Sensors},
  volume = {17},
  number = {8},
  pages = {1703},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@inproceedings{b_beurdouche_messy_2015,
  title = {A {{Messy State}} of the {{Union}}: {{Taming}} the {{Composite State Machines}} of {{TLS}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {{B. Beurdouche} and {K. Bhargavan} and {A. Delignat-Lavaud} and {C. Fournet} and {M. Kohlweiss} and {A. Pironti} and {P. -Y. Strub} and {J. K. Zinzindohoue}},
  year = {2015},
  month = may,
  pages = {535--552},
  doi = {10.1109/SP.2015.39},
  isbn = {2375-1207}
}

@inproceedings{bachmann_introduction_2001,
  title = {Introduction to the Attribute Driven Design Method},
  booktitle = {Software {{Engineering}}, {{International Conference}} On},
  author = {Bachmann, Felix and Bass, Len},
  year = {2001},
  pages = {0745--0745},
  publisher = {{Citeseer}},
  isbn = {0-7695-1050-7}
}

@article{bachmann_introduction_nodate,
  title = {Introduction to the {{Attribute Driven Design Method}}},
  author = {Bachmann, Felix and Bass, Len},
  pages = {2},
  abstract = {This tutorial will introduce the Attribute Driven Design (ADD) method. ADD is a method for designing the software architecture of a system or collection of systems based on an explicit articulation of the quality attribute goals for the system(s). The method is appropriate for any quality attributes but has been particularly elaborated for the attributes of performance, modifiability, security, reliability/availability and usability. The method has been used for designing the software architecture of products ranging from embedded to information systems.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\N8AM2D6A\\Bachmann en Bass - Introduction to the Attribute Driven Design Method.pdf}
}

@article{bachmann_introduction_nodate-1,
  title = {Introduction to the {{Attribute Driven Design Method}}},
  author = {Bachmann, Felix and Bass, Len},
  pages = {2},
  abstract = {This tutorial will introduce the Attribute Driven Design (ADD) method. ADD is a method for designing the software architecture of a system or collection of systems based on an explicit articulation of the quality attribute goals for the system(s). The method is appropriate for any quality attributes but has been particularly elaborated for the attributes of performance, modifiability, security, reliability/availability and usability. The method has been used for designing the software architecture of products ranging from embedded to information systems.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\96WB7GHW\\Bachmann en Bass - Introduction to the Attribute Driven Design Method.pdf}
}

@incollection{badr_hyperledger_2018,
  title = {Hyperledger {{Fabric}}\&\#{{xA0}};},
  booktitle = {Blockchain {{By Example}}},
  author = {Badr, Bellaj and Horrocks, Richard and Wu, Xun},
  year = {2018},
  month = nov,
  publisher = {{Packt Publishing}},
  abstract = {Implement decentralized blockchain applications to build scalable DappsKey FeaturesUnderstand the blockchain ecosystem and its terminologies Implement smart contracts, wallets, and consensus protocols Design and develop decentralized applications using Bitcoin, Ethereum, and Hyperledger Book DescriptionThe Blockchain is a revolution promising a new world without middlemen. Technically, it is an immutable and tamper-proof distributed ledger of all transactions across a peer-to-peer network. With this book, you will get to grips with the blockchain ecosystem to build real-world projects. This book will walk you through the process of building multiple blockchain projects with different complexity levels and hurdles. Each project will teach you just enough about the field's leading technologies, Bitcoin, Ethereum, Quorum, and Hyperledger in order to be productive from the outset. As you make your way through the chapters, you will cover the major challenges that are associated with blockchain ecosystems such as scalability, integration, and distributed file management. In the concluding chapters, you'll learn to build blockchain projects for business, run your ICO, and even create your own cryptocurrency. Blockchain by Example also covers a range of projects such as Bitcoin payment systems, supply chains on Hyperledger, and developing a Tontine Bank Every is using Ethereum. By the end of this book, you will not only be able to tackle common issues in the blockchain ecosystem, but also design and build reliable and scalable distributed systems. What you will learnGrasp decentralized technology fundamentals to master blockchain principles Build blockchain projects on Bitcoin, Ethereum, and Hyperledger Create your currency and a payment application using Bitcoin Implement decentralized apps and supply chain systems using Hyperledger Write smart contracts, run your ICO, and build a Tontine decentralized app using Ethereum Implement distributed file management with blockchain Integrate blockchain into existing systems in your organizationWho this book is forIf you are keen on learning how to build your own blockchain decentralized applications from scratch, then this book is for you. It explains all the basic concepts required to develop intermediate projects and will teach you to implement the building blocks of a blockchain ecosystem.Downloading the example code for this book You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com. If you purchased this book elsewhere, you can visit http://www.PacktPub.com/support and register to have the files e-mailed directly to you.},
  isbn = {978-1-78847-391-0},
  langid = {english}
}

@article{baggen_standardized_2012,
  title = {Standardized Code Quality Benchmarking for Improving Software Maintainability},
  author = {Baggen, Robert and Correia, Jos{\'e} Pedro and Schill, Katrin and Visser, Joost},
  year = {2012},
  journal = {Software Quality Journal},
  volume = {20},
  number = {2},
  pages = {287--307},
  publisher = {{Springer}},
  issn = {0963-9314}
}

@article{bajaj_paas_2020,
  title = {Paas {{Providers And Their Offerings}}},
  author = {Bajaj, Deepali and Bharti, Urmil and Goel, Anita and Gupta, S},
  year = {2020},
  month = dec,
  journal = {International Journal of Scientific \& Technology Research},
  volume = {VOLUME 9},
  pages = {4009--4017},
  abstract = {Platform-as-a-Service (PaaS) is a cloud service model that provides an environment to develop, deploy and operate software applications. PaaS provides preconfigured capabilities like, programming languages, supported frameworks, hosting and runtime environments, application development tools, and database servers managed for developing web applications as well as mobile applications. For an organization, selection of PaaS provider is a crucial task. Since PaaS providers offer a variety of development and deployment features, it becomes difficult to select from among them. Also, the organization may get locked to a particular provider and it is not easy to migrate to a different provider due to legal constraints and technical incompatibilities. In this paper, we present a study of currently available PaaS offerings. We identify the important PaaS characteristics that must be assessed before selecting a PaaS offering. We present a comparative study of different PaaS providers on basis of the identified characteristics. Our study benefits enterprise architects, IT managers and development teams to select PaaS offering best suited to their requirements.}
}

@article{bakken_s._nursing_2015,
  title = {Nursing Needs Big Data and Big Data Needs Nursing},
  author = {{Bakken S.}},
  year = {2015},
  journal = {J Nurs Scholarsh},
  volume = {47},
  pages = {477--484},
  langid = {english}
}

@article{bambrick_relationships_2005,
  title = {Relationships between {{BMI}}, Waist Circumference, Hypertension and Fasting Glucose: {{Rethinking}} Risk Factors in {{Indigenous}} Diabetes},
  author = {Bambrick, Dr Hilary Jane},
  year = {2005},
  volume = {5},
  number = {4},
  pages = {16},
  abstract = {Objective: To determine whether the body mass index (BMI) threshold defined for obesity (30kg/m2) adequately reflects risk in an Aboriginal community with a high rate of Type 2 diabetes. Methods: Data about five diabetes risk factors (age, BMI, waist circumference (WC), hypertension and family history) and fasting glucose (FG) were obtained from a random sample of 117 Aboriginal adults (62 women and 55 men) never diagnosed with diabetes. Linear regression between BMI, WC and FG, and sensitivity and specificity analyses in predicting elevated FG and hypertension were conducted. Results: BMI{$\geq$}30kg/m2 and central obesity assessed by WC (women{$\geq$}88cm; men{$\geq$}102cm) were strongly and positively associated. Among women, central obesity was near universal, occurring at BMIs below the `healthy' range of 20-25. WC was linearly associated with other diabetes risk factors. WC{$\geq$}88cm was more sensitive but less specific than BMI{$\geq$}30 in predicting elevated FG and hypertension among women, while BMI{$\geq$}25 among men tended to be both more sensitive and more specific than both BMI{$\geq$}30 and WC{$\geq$}102cm. Conclusions: In women, central obesity is a better predictor of diabetes and CVD risk than BMI{$\geq$}30, which is not a reliable indicator. BMI{$\geq$}25 was a good predictor in men. Implications: BMI is a useful clinical tool to identify individuals at risk, but to be relevant the guidelines defining risk may need to be reduced for the Aboriginal population. For women, a BMI{$\geq$}25 could more adequately reflect risk, while the current WC of 88cm remains appropriate. For men, a reduction in both BMI to {$\geq$}25 and WC to 90cm may better reflect diabetes and CVD risk.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4KFC3PSM\\Bambrick - 2005 - Relationships between BMI, waist circumference, hy.pdf}
}

@misc{bar_van_maarseveen_blockchain_nodate,
  title = {Blockchain \& Gaming | Komodo as Game Changer},
  author = {{Bar van Maarseveen}},
  journal = {Slides},
  abstract = {A presentation created with Slides.},
  howpublished = {https://slides.com/barv/komodo\_game\_changer},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\Y7AYSBKK\\komodo_game_changer.html}
}

@inproceedings{bar-yam_when_2003,
  title = {When Systems Engineering Fails-toward Complex Systems Engineering},
  booktitle = {{{SMC}}'03 {{Conference Proceedings}}. 2003 {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}}. {{Conference Theme}} - {{System Security}} and {{Assurance}} ({{Cat}}. {{No}}.{{03CH37483}})},
  author = {{Bar-Yam}, Y.},
  year = {2003},
  volume = {2},
  pages = {2021--2028},
  publisher = {{IEEE}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/ICSMC.2003.1244709},
  abstract = {We review the lessons learned from problems with systems engineering over the past couple of decades and suggest that there are two effective strategies for overcoming them: (1) restricting the conventional systems engineering process to not-too-complex projects, and (2) adopting an evolutionary paradigm for complex systems engineering that involves rapid parallel exploration and a context designed to promote change through competition between design/implementation groups with field testing of multiple variants. The second approach is an extension of many of the increasingly popular variants of systems engineering today.},
  isbn = {978-0-7803-7952-7},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SBSAZDPP\\Bar-Yam - 2003 - When systems engineering fails-toward complex syst.pdf}
}

@article{barroso_web_2003,
  title = {Web Search for a Planet: The Google Cluster Architecture},
  shorttitle = {Web Search for a Planet},
  author = {Barroso, L.A. and Dean, J. and Holzle, U.},
  year = {2003},
  month = mar,
  journal = {IEEE Micro},
  volume = {23},
  number = {2},
  pages = {22--28},
  issn = {0272-1732},
  doi = {10.1109/MM.2003.1196112},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5LUHQJY8\\Barroso e.a. - 2003 - Web search for a planet the google cluster archit.pdf}
}

@misc{beauregard_circuit_2003,
  title = {Circuit for {{Shor}}'s Algorithm Using 2n+3 Qubits},
  author = {Beauregard, Stephane},
  year = {2003},
  month = feb,
  number = {arXiv:quant-ph/0205095},
  eprint = {quant-ph/0205095},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {We try to minimize the number of qubits needed to factor an integer of n bits using Shor's algorithm on a quantum computer. We introduce a circuit which uses 2n+3 qubits and O(n\^3 lg(n)) elementary quantum gates in a depth of O(n\^3) to implement the factorization algorithm. The circuit is computable in polynomial time on a classical computer and is completely general as it does not rely on any property of the number to be factored. Keywords: Factorization, quantum circuits, modular arithmetics},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\F2N6TR2M\\Beauregard - 2003 - Circuit for Shor's algorithm using 2n+3 qubits.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\J9DAVJT5\\0205095.html}
}

@misc{bedrijfsbegeleider_interview_2018,
  title = {{Interview bedrijfsbegeleider 2018/10/30}},
  author = {{Bedrijfsbegeleider}},
  year = {2018},
  month = oct,
  langid = {dutch}
}

@misc{bedrijfsbegeleiders_interview_2018,
  title = {Interview Met Bedrijfleiders},
  author = {{Bedrijfsbegeleiders}},
  year = {2018},
  month = apr
}

@article{beke_comparison_nodate,
  title = {On the {{Comparison}} of {{Software Quality Attributes}} for {{Client-side}} and {{Server-side Rendering}}},
  author = {Beke, Mathias},
  pages = {106},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\FSY7NDHH\\Beke - On the Comparison of Software Quality Attributes f.pdf}
}

@incollection{bell_machine_2014,
  title = {Machine {{Learning}}: {{Hands-On}} for {{Developers}} and {{Technical Professionals}}},
  booktitle = {Machine {{Learning}}: {{Hands-On}} for {{Developers}} and {{Technical Professionals}}},
  author = {Bell, Jason},
  year = {2014},
  month = nov,
  pages = {18},
  publisher = {{John Wiley \& Sons}},
  abstract = {Dig deep into the data with a hands-on guide to machine learning Machine Learning: Hands-On for Developers and Technical Professionals provides hands-on instruction and fully-coded working examples for the most common machine learning techniques used by developers and technical professionals. The book contains a breakdown of each ML variant, explaining how it works and how it is used within certain industries, allowing readers to incorporate the presented techniques into their own work as they follow along. A core tenant of machine learning is a strong focus on data preparation, and a full exploration of the various types of learning algorithms illustrates how the proper tools can help any developer extract information and insights from existing data. The book includes a full complement of Instructor's Materials to facilitate use in the classroom, making this resource useful for students and as a professional reference. At its core, machine learning is a mathematical, algorithm-based technology that forms the basis of historical data mining and modern big data science. Scientific analysis of big data requires a working knowledge of machine learning, which forms predictions based on known properties learned from training data. Machine Learning is an accessible, comprehensive guide for the non-mathematician, providing clear guidance that allows readers to:  Learn the languages of machine learning including Hadoop, Mahout, and Weka Understand decision trees, Bayesian networks, and artificial neural networks Implement Association Rule, Real Time, and Batch learning Develop a strategic plan for safe, effective, and efficient machine learning  By learning to construct a system that can learn from data, readers can increase their utility across industries. Machine learning sits at the core of deep dive data analysis and visualization, which is increasingly in demand as companies discover the goldmine hiding in their existing data. For the tech professional involved in data science, Machine Learning: Hands-On for Developers and Technical Professionals provides the skills and techniques required to dig deeper.},
  googlebooks = {Rc6SBQAAQBAJ},
  isbn = {978-1-118-88906-0},
  langid = {english},
  keywords = {Computers / Programming / Algorithms,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{benvenuto_application_2020,
  title = {Application of the {{ARIMA}} Model on the {{COVID-2019}} Epidemic Dataset},
  author = {Benvenuto, Domenico and Giovanetti, Marta and Vassallo, Lazzaro and Angeletti, Silvia and Ciccozzi, Massimo},
  year = {2020},
  month = apr,
  journal = {Data in Brief},
  volume = {29},
  pages = {105340},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2020.105340},
  abstract = {Coronavirus disease 2019 (COVID-2019) has been recognized as a global threat, and several studies are being conducted using various mathematical models to predict the probable evolution of this epidemic. These mathematical models based on various factors and analyses are subject to potential bias. Here, we propose a simple econometric model that could be useful to predict the spread of COVID-2019. We performed Auto Regressive Integrated Moving Average (ARIMA) model prediction on the Johns Hopkins epidemiological data to predict the epidemiological trend of the prevalence and incidence of COVID-2019. For further comparison or for future perspective, case definition and data collection have to be maintained in real time.},
  langid = {english},
  keywords = {ARIMA model,COVID-2019 epidemic,Forecast,Infection control},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4FLMSP6R\\Benvenuto e.a. - 2020 - Application of the ARIMA model on the COVID-2019 e.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\SHHXJCKH\\S2352340920302341.html}
}

@article{bergstra_random_2012,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  number = {10},
  pages = {281--305}
}

@inproceedings{bhargavan_content_2017,
  title = {Content Delivery over {{TLS}}: A Cryptographic Analysis of Keyless {{SSL}}},
  shorttitle = {Content Delivery over {{TLS}}},
  booktitle = {2017 {{IEEE European Symposium}} on {{Security}} and {{Privacy}} ({{EuroS}}\&{{P}})},
  author = {Bhargavan, Karthikeyan and Boureanu, Ioana and Fouque, Pierre-Alain and Onete, Cristina and Richard, Benjamin},
  year = {2017},
  month = apr,
  pages = {1--16},
  doi = {10.1109/EuroSP.2017.52},
  abstract = {The Transport Layer Security (TLS) protocol is designed to allow two parties, a client and a server, to communicate securely over an insecure network. However, when TLS connections are proxied through an intermediate middlebox, like a Content Delivery Network (CDN), the standard endto- end security guarantees of the protocol no longer apply. In this paper, we investigate the security guarantees provided by Keyless SSL, a CDN architecture currently deployed by CloudFlare that composes two TLS 1.2 handshakes to obtain a proxied TLS connection. We demonstrate new attacks that show that Keyless SSL does not meet its intended security goals. These attacks have been reported to CloudFlare and we are in the process of discussing fixes. We argue that proxied TLS handshakes require a new, stronger, 3-party security definition. We present 3(S)ACCEsecurity, a generalization of the 2-party ACCE security definition that has been used in several previous proofs for TLS. We modify Keyless SSL and prove that our modifications guarantee 3(S)ACCE-security, assuming ACCE-security for the individual TLS 1.2 connections. We also propose a new design for Keyless TLS 1.3 and prove that it achieves 3(S)ACCEsecurity, assuming that the TLS 1.3 handshake implements an authenticated 2-party key exchange. Notably, we show that secure proxying in Keyless TLS 1.3 is computationally lighter and requires simpler assumptions on the certificate infrastructure than our proposed fix for Keyless SSL. Our results indicate that proxied TLS architectures, as currently used by a number of CDNs, may be vulnerable to subtle attacks and deserve close attention.},
  keywords = {Computer architecture,Cryptography,Middleboxes,Middleware,Protocols,Servers},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\3M96TKQI\\Bhargavan e.a. - 2017 - Content delivery over TLS a cryptographic analysi.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\5THG5WSA\\8013427.html}
}

@book{birnbaum_machine_2014,
  title = {Machine {{Learning Proceedings}} 1993: {{Proceedings}} of the {{Tenth International Conference}} on {{Machine Learning}}, {{University}} of {{Massachusetts}}, {{Amherst}}, {{June}} 27-29, 1993},
  shorttitle = {Machine {{Learning Proceedings}} 1993},
  author = {Birnbaum, Lawrence A.},
  year = {2014},
  month = may,
  publisher = {{Morgan Kaufmann}},
  abstract = {Machine Learning Proceedings 1993},
  googlebooks = {TrqjBQAAQBAJ},
  isbn = {978-1-4832-9862-7},
  langid = {english},
  keywords = {Computers / Intelligence (AI) \& Semantics}
}

@book{birnbaum_machine_2014-1,
  title = {Machine {{Learning Proceedings}} 1993: {{Proceedings}} of the {{Tenth International Conference}} on {{Machine Learning}}, {{University}} of {{Massachusetts}}, {{Amherst}}, {{June}} 27-29, 1993},
  shorttitle = {Machine {{Learning Proceedings}} 1993},
  author = {Birnbaum, Lawrence A.},
  year = {2014},
  month = may,
  publisher = {{Morgan Kaufmann}},
  abstract = {Machine Learning Proceedings 1993},
  googlebooks = {TrqjBQAAQBAJ},
  isbn = {978-1-4832-9862-7},
  langid = {english},
  keywords = {Computers / Intelligence (AI) \& Semantics}
}

@article{bishop_model-based_2013,
  title = {Model-Based Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2013},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {371},
  number = {1984},
  pages = {20120222},
  doi = {10.1098/rsta.2012.0222},
  abstract = {Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\K8AWNHUV\\Bishop - 2013 - Model-based machine learning.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\8Z6WJMXU\\rsta.2012.html}
}

@article{blake_complexity_2004,
  title = {On the Complexity of the Discrete Logarithm and {{Diffie}}\textendash{{Hellman}} Problems},
  author = {Blake, Ian F. and Garefalakis, Theo},
  year = {2004},
  month = apr,
  journal = {Festschrift for Harald Niederreiter, Special Issue on Coding and Cryptography},
  volume = {20},
  number = {2},
  pages = {148--170},
  issn = {0885-064X},
  doi = {10.1016/j.jco.2004.01.002},
  abstract = {The discrete logarithm problem plays a central role in cryptographic protocols and computational number theory. To establish the exact complexity, not only of the discrete logarithm problem but also of its relatives, the Diffie\textendash Hellman (DH) problem and the decision DH problem, is of some importance. These problems can be set in a variety of groups, and in some of these they can assume different characteristics. This work considers the bit complexity of the DH and the decision DH problems. It was previously shown by Boneh and Venkatesan that it is as hard to compute O(n) of the most significant bits of the DH function, as it is to compute the whole function, implying that if the DH function is difficult then so is computing this number of bits of it. The main result of this paper is to show that if the decision DH problem is hard then computing the two most significant bits of the DH function is hard. To place the result in perspective a brief overview of relevant recent advances on related problems is given.},
  keywords = {Cryptography,Discrete logarithms}
}

@book{bolstad_introduction_2016,
  title = {Introduction to {{Bayesian Statistics}}},
  author = {Bolstad, William M. and Curran, James M.},
  year = {2016},
  month = oct,
  publisher = {{John Wiley \& Sons}},
  abstract = {"...this edition is useful and effective in teaching Bayesian inference at both elementary and intermediate levels. It is a well-written book on elementary Bayesian inference, and the material is easily accessible. It is both concise and timely, and provides a good collection of overviews and reviews of important tools used in Bayesian statistical methods." There is a strong upsurge in the use of Bayesian methods in applied statistical analysis, yet most introductory statistics texts only present frequentist methods. Bayesian statistics has many important advantages that students should learn about if they are going into fields where statistics will be used. In this third Edition, four newly-added chapters address topics that reflect the rapid advances in the field of Bayesian statistics. The authors continue to provide a Bayesian treatment of introductory statistical topics, such as scientific data gathering, discrete random variables, robust Bayesian methods, and Bayesian approaches to inference for discrete random variables, binomial proportions, Poisson, and normal means, and simple linear regression. In addition, more advanced topics in the field are presented in four new chapters: Bayesian inference for a normal with unknown mean and variance; Bayesian inference for a Multivariate Normal mean vector; Bayesian inference for the Multiple Linear Regression Model; and Computational Bayesian Statistics including Markov Chain Monte Carlo. The inclusion of these topics will facilitate readers' ability to advance from a minimal understanding of Statistics to the ability to tackle topics in more applied, advanced level books. Minitab macros and R functions are available on the book's related website to assist with chapter exercises. Introduction to Bayesian Statistics, Third Edition also features:  Topics including the Joint Likelihood function and inference using independent Jeffreys priors and join conjugate prior The cutting-edge topic of computational Bayesian Statistics in a new chapter, with a unique focus on Markov Chain Monte Carlo methods Exercises throughout the book that have been updated to reflect new applications and the latest software applications Detailed appendices that guide readers through the use of R and Minitab software for Bayesian analysis and Monte Carlo simulations, with all related macros available on the book's website  Introduction to Bayesian Statistics, Third Edition is a textbook for upper-undergraduate or first-year graduate level courses on introductory statistics course with a Bayesian emphasis. It can also be used as a reference work for statisticians who require a working knowledge of Bayesian statistics.},
  googlebooks = {BxfkDAAAQBAJ},
  isbn = {978-1-118-09156-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{bond_big_2018,
  title = {Big {{Data}}},
  author = {Bond, Christine},
  year = {2018},
  journal = {International Journal of Pharmacy Practice},
  volume = {26},
  number = {1},
  pages = {1--2},
  issn = {2042-7174},
  doi = {10.1111/ijpp.12434},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RACZSKLK\\Bond - 2018 - Big Data.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\A58D6QA2\\ijpp.html}
}

@article{borges_comparison_2020,
  title = {A {{Comparison}} of {{Security}} and Its {{Performance}} for {{Key Agreements}} in {{Post-Quantum Cryptography}}},
  author = {Borges, F{\'a}bio and Reis, Paulo Ricardo and Pereira, Diogo},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {142413--142422},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3013250},
  abstract = {Nowadays, we are surrounded by devices collecting and transmitting private information. Currently, the two main mathematical problems that guarantee security on the Internet are the Integer Factorization Problem and the Discrete Logarithm Problem. However, Shor's quantum algorithm can easily solve both problems. Therefore, research into cryptographic algorithms that run in classical computers and are resistant to quantum computers is extremely necessary. This area is known as post-quantum cryptography and usually studies asymmetric cryptography. By means of asymptotic analysis, the purpose of this paper is to provide an evaluation of security and its performance for the types of cryptographic systems considered safe against quantum attacks in the second-round NIST Post-Quantum Standardization Process, namely isogeny cryptosystems based on supersingular elliptic curves, error correction code-based encryption system, and lattice-based ring learning with errors. We performed a security comparison of Key Agreements protocols based on these three post-quantum cryptographic primitives and compared them with Discrete Logarithm Problem and Integer Factorization Problem. The comparison of security and its performance is presented by security level, the former by complexity analyses to achieve theoretical minimum key sizes, and the latter by simulation to assess a practical performance comparison. In the complexity analysis, as we increase the security level and then the size of the cryptographic keys increases, techniques based on isogeny outperform all other post-quantum algorithms in relation to key sizes at practical security level. In the performance comparison, the results show that the code-based protocol presents the best results among the others.},
  keywords = {algorithms,asymmetric cryptography,asymptotic analysis,complexity,Complexity theory,Computers,Cryptography,Elliptic curves,key agreement,Post-quantum cryptography,Protocols,Quantum computing},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\STMSU5QM\\Borges e.a. - 2020 - A Comparison of Security and its Performance for K.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\IBEMB36P\\9153901.html}
}

@article{borjesson_successful_2004,
  title = {Successful Process Implementation},
  author = {Borjesson, A. and Mathiassen, L.},
  year = {2004},
  month = jul,
  journal = {IEEE Software},
  volume = {21},
  number = {4},
  pages = {36--44},
  issn = {0740-7459},
  doi = {10.1109/MS.2004.27},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZB6KRIYV\\Borjesson en Mathiassen - 2004 - Successful process implementation.pdf}
}

@article{bouveyron_high-dimensional_2007,
  title = {High-Dimensional Data Clustering},
  author = {Bouveyron, C. and Girard, S. and Schmid, C.},
  year = {2007},
  month = sep,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {1},
  pages = {502--519},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2007.02.009},
  abstract = {Clustering in high-dimensional spaces is a difficult problem which is recurrent in many domains, for example in image analysis. The difficulty is due to the fact that high-dimensional data usually exist in different low-dimensional subspaces hidden in the original space. A family of Gaussian mixture models designed for high-dimensional data which combine the ideas of subspace clustering and parsimonious modeling are presented. These models give rise to a clustering method based on the expectation\textendash maximization algorithm which is called high-dimensional data clustering (HDDC). In order to correctly fit the data, HDDC estimates the specific subspace and the intrinsic dimension of each group. Experiments on artificial and real data sets show that HDDC outperforms existing methods for clustering high-dimensional data.},
  langid = {english},
  keywords = {Gaussian mixture models,High-dimensional data,Model-based clustering,Parsimonious models,Subspace clustering},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NQDSL4GT\\Bouveyron e.a. - 2007 - High-dimensional data clustering.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\AHS3JX7U\\S0167947307000692.html}
}

@book{bowers_medical_2008,
  title = {Medical Statistics from Scratch: An Introduction for Health Professionals},
  shorttitle = {Medical Statistics from Scratch},
  author = {Bowers, David},
  year = {2008},
  edition = {2. ed},
  publisher = {{Wiley}},
  address = {{Chichester [u.a]}},
  isbn = {978-0-470-51301-9},
  langid = {english},
  annotation = {OCLC: 255921756},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PT8M5P5T\\Bowers - 2008 - Medical statistics from scratch an introduction f.pdf}
}

@book{box_time_2015,
  title = {Time Series Analysis: Forecasting and Control},
  author = {Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
  isbn = {1-118-67492-8}
}

@article{box_time_nodate,
  title = {Time {{Series Analysis}}},
  author = {Box, George E P},
  pages = {709},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HB2G39PK\\Box - Time Series Analysis.pdf}
}

@inproceedings{boykov_interactive_2001,
  title = {Interactive Graph Cuts for Optimal Boundary \& Region Segmentation of Objects in {{N-D}} Images},
  booktitle = {Proceedings {{Eighth IEEE International Conference}} on {{Computer Vision}}. {{ICCV}} 2001},
  author = {Boykov, Y.Y. and Jolly, M.-P.},
  year = {2001},
  volume = {1},
  pages = {105--112},
  publisher = {{IEEE Comput. Soc}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1109/ICCV.2001.937505},
  abstract = {In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as ``object'' or ``background'' to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both ``object'' and ``background'' segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm in [2].},
  isbn = {978-0-7695-1143-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\DALYBGZE\\Boykov en Jolly - 2001 - Interactive graph cuts for optimal boundary & regi.pdf}
}

@misc{brains_list_2018,
  title = {List of {{Mobile App Performance Metrics}} to {{Gauge}} the {{Success}} of {{App}}},
  author = {Brains, Hidden},
  year = {2018},
  month = may,
  journal = {Medium},
  abstract = {Whatever be the purpose of creating mobile app, it is the performance of app that defines its success. When discussing about the\ldots},
  keywords = {achtergrondinfo},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\R6BAQDMN\\list-of-mobile-app-performance-metrics-to-gauge-the-success-of-app-77d4a57de46.html}
}

@article{breiman_random_2001,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  publisher = {{Springer}},
  issn = {0885-6125}
}

@misc{brett_machine_nodate,
  title = {Machine {{Learning}} with {{R}} : {{Expert Techniques}} for {{Predictive Modeling}}, 3rd {{Edition}}},
  author = {Brett, Lantz},
  howpublished = {https://web-b-ebscohost-com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzIxMDYzMDRfX0FO0?sid=ef91a461-61ea-4efc-87ff-c117c6327856@sessionmgr101\&vid=0\&format=EB\&rid=1},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2FRZ274I\\bmxlYmtfXzIxMDYzMDRfX0FO0.html}
}

@article{breunig_lof_nodate,
  title = {{{LOF}}: {{Identifying Density-Based Local Outliers}}},
  author = {Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  pages = {12},
  abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using realworld datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KQPB7JJC\\Breunig e.a. - LOF Identifying Density-Based Local Outliers.pdf}
}

@article{brito_rest_2020,
  title = {{{REST}} vs {{GraphQL}}: {{A Controlled Experiment}}},
  shorttitle = {{{REST}} vs {{GraphQL}}},
  author = {Brito, Gleison and Valente, Marco Tulio},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.04761 [cs]},
  eprint = {2003.04761},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {GraphQL is a novel query language for implementing service-based software architectures. The language is gaining momentum and it is now used by major software companies, such as Facebook and GitHub. However, we still lack empirical evidence on the real gains achieved by GraphQL, particularly in terms of the effort required to implement queries in this language. Therefore, in this paper we describe a controlled experiment with 22 students (10 undergraduate and 12 graduate), who were asked to implement eight queries for accessing a web service, using GraphQL and REST. Our results show that GraphQL requires less effort to implement remote service queries when compared to REST (9 vs 6 minutes, median times). These gains increase when REST queries include more complex endpoints, with several parameters. Interestingly, GraphQL outperforms REST even among more experienced participants (as is the case of graduate students) and among participants with previous experience in REST, but no previous experience in GraphQL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WF4GDTMI\\Brito en Valente - 2020 - REST vs GraphQL A Controlled Experiment.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\TVV3DSAQ\\2003.html}
}

@article{cerny_contextual_2017,
  title = {Contextual {{Understanding}} of {{Microservice Architecture}}: {{Current}} and {{Future Directions}}},
  author = {Cerny, Tomas and Donahoo, Michael J and Trnka, Michal},
  year = {2017},
  volume = {17},
  number = {4},
  pages = {17},
  abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JTI4C7T3\\Cerny e.a. - 2017 - Contextual Understanding of Microservice Architect.pdf}
}

@article{cerny_contextual_2018,
  title = {Contextual Understanding of Microservice Architecture: Current and Future Directions},
  shorttitle = {Contextual Understanding of Microservice Architecture},
  author = {{\v C}ern{\'y}, Tom and Donahoo, Michael and Trnka, Michal},
  year = {2018},
  month = jan,
  journal = {ACM SIGAPP Applied Computing Review},
  volume = {17},
  pages = {29--45},
  doi = {10.1145/3183628.3183631},
  abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.}
}

@article{cerny_contextual_2018-1,
  title = {Contextual Understanding of Microservice Architecture: Current and Future Directions},
  shorttitle = {Contextual Understanding of Microservice Architecture},
  author = {Cerny, Tomas and Donahoo, Michael J. and Trnka, Michal},
  year = {2018},
  month = jan,
  journal = {ACM SIGAPP Applied Computing Review},
  volume = {17},
  number = {4},
  pages = {29--45},
  issn = {1559-6915},
  doi = {10.1145/3183628.3183631},
  abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.},
  keywords = {architectures,microservices,self-contained systems,SOA,survey,systematic mapping study},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\F83TCEAG\\Cerny e.a. - 2018 - Contextual understanding of microservice architect.pdf}
}

@article{cerny_contextual_2018-2,
  title = {Contextual Understanding of Microservice Architecture: Current and Future Directions},
  shorttitle = {Contextual Understanding of Microservice Architecture},
  author = {{\v C}ern{\'y}, Tom and Donahoo, Michael and Trnka, Michal},
  year = {2018},
  month = jan,
  journal = {ACM SIGAPP Applied Computing Review},
  volume = {17},
  pages = {29--45},
  doi = {10.1145/3183628.3183631},
  abstract = {Current industry trends in enterprise architectures indicate movement from Service-Oriented Architecture (SOA) to Microservices. By understanding the key differences between these two approaches and their features, we can design a more effective Microservice architecture by avoiding SOA pitfalls. To do this, we must know why this shift is happening and how key SOA functionality is addressed by key features of the Microservice-based system. Unfortunately, Microservices do not address all SOA shortcomings. In addition, Microservices introduce new challenges. This work provides a detailed analysis of the differences between these two architectures and their features. Next, we describe both research and industry perspectives on the strengths and weaknesses of both architectural directions. Finally, we perform a systematic mapping study related to Microservice research, identifying interest and challenges in multiple categories from a range of recent research.}
}

@misc{chandramouli_security_2019,
  title = {Security {{Strategies}} for {{Microservices-based Application Systems}}},
  author = {Chandramouli, Ramaswamy},
  year = {2019},
  month = aug,
  publisher = {{Special Publication (NIST SP), National Institute of Standards and Technology, Gaithersburg, MD}},
  doi = {10.6028/NIST.SP.800-204},
  langid = {english}
}

@article{chandramouli_security_nodate,
  title = {Security {{Strategies}} for {{Microservices-based Application Systems}}},
  author = {Chandramouli, Ramaswamy},
  pages = {50},
  abstract = {Microservices architecture is increasingly being used to develop application systems since its smaller codebase facilitates faster code development, testing, and deployment as well as optimization of the platform based on the type of microservice, support for independent development teams, and the ability to scale each component independently. Microservices generally communicate with each other using Application Programming Interfaces (APIs), which requires several core features to support complex interactions between a substantial number of components. These core features include authentication and access management, service discovery, secure communication protocols, security monitoring, availability/resiliency improvement techniques (e.g., circuit breakers), load balancing and throttling, integrity assurance techniques during induction of new services, and handling of session persistence. Additionally, the core features could be bundled or packaged into architectural frameworks such as API gateways and service mesh. The purpose of this document is to analyze the multiple implementation options available for each individual core feature and configuration options in architectural frameworks, develop security strategies that counter threats specific to microservices, and enhance the overall security profile of the microservices-based application.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JVFX2L87\\Chandramouli - Security Strategies for Microservices-based Applic.pdf}
}

@book{chatfield_time-series_2000,
  title = {Time-Series Forecasting},
  author = {Chatfield, Chris},
  year = {2000},
  publisher = {{CRC press}},
  isbn = {1-4200-3620-3}
}

@article{chawla_smote_2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {16},
  eprint = {1106.1813},
  eprinttype = {arxiv},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MTLV2RYG\\Chawla e.a. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\B3YUJVFR\\1106.html}
}

@article{chawla_smote_2002-1,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  year = {2002},
  month = jun,
  journal = {Journal of Artificial Intelligence Research},
  volume = {16},
  eprint = {1106.1813},
  eprinttype = {arxiv},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4LGDL9NN\\1106.html}
}

@article{chen_continuous_2017,
  title = {Continuous {{Delivery}}: {{Overcoming}} Adoption Challenges},
  author = {Chen, Lianping},
  year = {2017},
  journal = {Journal of Systems and Software},
  volume = {128},
  pages = {72--86},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2017.02.013},
  abstract = {Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted CD have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting CD can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different from\textemdash or even conflict with\textemdash our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to CD requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling CD as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual CD pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing CD at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for CD adoption.},
  keywords = {Adoption,Agile Software Development,Continuous Delivery,Continuous Deployment,Continuous Software Engineering,DevOps}
}

@article{chen_entity-relationship_1976,
  title = {The Entity-Relationship Model: {{Toward}} a Unified View of Data},
  shorttitle = {The Entity-Relationship Model},
  author = {Chen, Peter Pin-shan},
  year = {1976},
  journal = {ACM Transactions on Database Systems},
  abstract = {A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set}
}

@inproceedings{cito_feedback_2019,
  title = {Feedback from {{Operations}} to {{Software Development}}\textemdash{{A DevOps Perspective}} on {{Runtime Metrics}} and {{Logs}}},
  booktitle = {Software {{Engineering Aspects}} of {{Continuous Development}} and {{New Paradigms}} of {{Software Production}} and {{Deployment}}},
  author = {Cito, J{\"u}rgen and Wettinger, Johannes and Lwakatare, Lucy Ellen and Borg, Markus and Li, Fei},
  editor = {Bruel, Jean-Michel and Mazzara, Manuel and Meyer, Bertrand},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {184--195},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-06019-0_14},
  abstract = {DevOps achieve synergy between software development and operations engineers. This synergy can only happen if the right culture is in place to foster communication between these roles. We investigate the relationship between runtime data generated during production and how this data can be used as feedback in the software development process. For that, we want to discuss case study organizations that have different needs on their operations-to-development feedback pipeline, from which we abstract and propose a more general, higher-level feedback process. Given such a process, we discuss a technical environment required to support this process. We sketch out different scenarios in which feedback is useful in different phases of the software development life-cycle.},
  isbn = {978-3-030-06019-0},
  langid = {english},
  keywords = {DevOps,Feedback,Software engineering},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YEIV3HAK\\Cito e.a. - 2019 - Feedback from Operations to Software Development—A.pdf}
}

@article{claesen_hyperparameter_2015,
  title = {Hyperparameter {{Search}} in {{Machine Learning}}},
  author = {Claesen, Marc and De Moor, Bart},
  year = {2015},
  month = apr,
  journal = {arXiv:1502.02127 [cs, stat]},
  eprint = {1502.02127},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,G.1.6,I.2.6,I.2.8,I.5,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QGBTTQZ9\\1502.html}
}

@article{coleman_using_1994,
  title = {Using {{Metrics}} to {{Evaluate Software System Maintainability}}},
  author = {Coleman, Don and Ash, Dan and Lowther, Bruce and Oman, Paul},
  year = {1994},
  month = sep,
  journal = {Computer},
  volume = {27},
  pages = {44--49},
  doi = {10.1109/2.303623},
  abstract = {Software metrics have been much criticized in the last few years, sometimes justly but more often unjustly, because critics misunderstand the intent behind the technology. Software complexity metrics, for example, rarely measure the ``inherent complexity'' embedded in software systems, but they do a very good job of comparing the relative complexity of one portion of a system with another. In essence, they are good modeling tools. Whether they are also good measuring tools depends on how consistently and appropriately they are applied}
}

@article{contreras_arima_2003,
  title = {{{ARIMA}} Models to Predict Next-Day Electricity Prices},
  author = {Contreras, J. and Espinola, R. and Nogales, F.J. and Conejo, A.J.},
  year = {2003},
  month = aug,
  journal = {IEEE Transactions on Power Systems},
  volume = {18},
  number = {3},
  pages = {1014--1020},
  issn = {0885-8950},
  doi = {10.1109/TPWRS.2002.804943},
  abstract = {Price forecasting is becoming increasingly relevant to producers and consumers in the new competitive electric power markets. Both for spot markets and long-term contracts, price forecasts are necessary to develop bidding strategies or negotiation skills in order to maximize benefit. This paper provides a method to predict next-day electricity prices based on the ARIMA methodology. ARIMA techniques are used to analyze time series and, in the past, have been mainly used for load forecasting, due to their accuracy and mathematical soundness. A detailed explanation of the aforementioned ARIMA models and results from mainland Spain and Californian markets are presented.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MK78AGLM\\Contreras e.a. - 2003 - ARIMA models to predict next-day electricity price.pdf}
}

@book{craig_larman_applying_2004,
  title = {Applying {{UML}} and Patterns - {{An}} Introduction to {{Object-Oriented Analysis}} and {{Design}} and Iterative Development},
  author = {{Craig Larman}},
  year = {2004},
  publisher = {{Pearson education}},
  address = {{Upper Saddle River}}
}

@book{craig_larman_applying_2005,
  title = {Applying {{UML}} and Patterns - {{An}} Introduction to {{Object-Oriented Analysis}} and {{Design}} and Iterative Development},
  author = {{Craig Larman}},
  year = {2005},
  publisher = {{Prentice Hall}}
}

@article{cross_validating_2019,
  title = {Validating Quantum Computers Using Randomized Model Circuits},
  author = {Cross, Andrew W. and Bishop, Lev S. and Sheldon, Sarah and Nation, Paul D. and Gambetta, Jay M.},
  year = {2019},
  month = sep,
  journal = {Physical Review A},
  volume = {100},
  number = {3},
  eprint = {1811.12926},
  eprinttype = {arxiv},
  primaryclass = {quant-ph},
  pages = {032328},
  issn = {2469-9926, 2469-9934},
  doi = {10.1103/PhysRevA.100.032328},
  abstract = {We introduce a single-number metric, quantum volume, that can be measured using a concrete protocol on near-term quantum computers of modest size (\$n\textbackslash lesssim 50\$), and measure it on several state-of-the-art transmon devices, finding values as high as 16. The quantum volume is linked to system error rates, and is empirically reduced by uncontrolled interactions within the system. It quantifies the largest random circuit of equal width and depth that the computer successfully implements. Quantum computing systems with high-fidelity operations, high connectivity, large calibrated gate sets, and circuit rewriting toolchains are expected to have higher quantum volumes. The quantum volume is a pragmatic way to measure and compare progress toward improved system-wide gate error rates for near-term quantum computation and error-correction experiments.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9G8HWXG7\\Cross e.a. - 2019 - Validating quantum computers using randomized mode.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\I2V5U25A\\1811.html}
}

@inproceedings{cukier_devops_2013,
  title = {{{DevOps}} Patterns to Scale Web Applications Using Cloud Services},
  booktitle = {Proceedings of the 2013 Companion Publication for Conference on {{Systems}}, Programming, \& Applications: Software for Humanity - {{SPLASH}} '13},
  author = {Cukier, Daniel},
  year = {2013},
  pages = {143--152},
  publisher = {{ACM Press}},
  address = {{Indianapolis, Indiana, USA}},
  doi = {10.1145/2508075.2508432},
  abstract = {Scaling a web applications can be easy for simple CRUD software running when you use Platform as a Service Clouds (PaaS). But if you need to deploy a complex software, with many components and a lot users, you will need have a mix of cloud services in PaaS, SaaS and IaaS layers. You will also need knowledge in architecture patterns to make all these software components communicate accordingly. In this article, we share our experience of using cloud services to scale a web application. We show usage examples of load balancing, session sharing, e-mail delivery, asynchronous processing, logs processing, monitoring, continuous deployment, realtime user monitoring (RUM). These are a mixture of development and system operations (DevOps) that improved our application availability, scalability and performance.},
  isbn = {978-1-4503-1995-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C3957E53\\Cukier - 2013 - DevOps patterns to scale web applications using cl.pdf}
}

@inproceedings{cukier_devops_2013-1,
  title = {{{DevOps Patterns}} to {{Scale Web Applications Using Cloud Services}}},
  booktitle = {Proceedings of the 2013 {{Companion Publication}} for {{Conference}} on {{Systems}}, {{Programming}}, \&amp; {{Applications}}: {{Software}} for {{Humanity}}},
  author = {Cukier, Daniel},
  year = {2013},
  series = {{{SPLASH}} '13},
  pages = {143--152},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2508075.2508432},
  abstract = {Scaling a web applications can be easy for simple CRUD software running when you use Platform as a Service Clouds (PaaS). But if you need to deploy a complex software, with many components and a lot users, you will need have a mix of cloud services in PaaS, SaaS and IaaS layers. You will also need knowledge in architecture patterns to make all these software components communicate accordingly. In this article, we share our experience of using cloud services to scale a web application. We show usage examples of load balancing, session sharing, e-mail delivery, asynchronous processing, logs processing, monitoring, continuous deployment, realtime user monitoring (RUM). These are a mixture of development and system operations (DevOps) that improved our application availability, scalability and performance.},
  isbn = {978-1-4503-1995-9},
  keywords = {APIs,AWS,cloud,cloud computing,devops,ELB,ELO7,email,IAAS,load balancing,paas,rest,S3,SAAS,scalability,scalable,tomcat,web services}
}

@inproceedings{cukier_devops_2013-2,
  title = {{{DevOps}} Patterns to Scale Web Applications Using Cloud Services},
  booktitle = {Proceedings of the 2013 Companion Publication for Conference on {{Systems}}, Programming, \& Applications: Software for Humanity - {{SPLASH}} '13},
  author = {Cukier, Daniel},
  year = {2013},
  pages = {143--152},
  publisher = {{ACM Press}},
  address = {{Indianapolis, Indiana, USA}},
  doi = {10.1145/2508075.2508432},
  abstract = {Scaling a web applications can be easy for simple CRUD software running when you use Platform as a Service Clouds (PaaS). But if you need to deploy a complex software, with many components and a lot users, you will need have a mix of cloud services in PaaS, SaaS and IaaS layers. You will also need knowledge in architecture patterns to make all these software components communicate accordingly. In this article, we share our experience of using cloud services to scale a web application. We show usage examples of load balancing, session sharing, e-mail delivery, asynchronous processing, logs processing, monitoring, continuous deployment, realtime user monitoring (RUM). These are a mixture of development and system operations (DevOps) that improved our application availability, scalability and performance.},
  isbn = {978-1-4503-1995-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\S7GNJ53H\\Cukier - 2013 - DevOps patterns to scale web applications using cl.pdf}
}

@inproceedings{cukier_devops_2013-3,
  title = {{{DevOps}} Patterns to Scale Web Applications Using Cloud Services},
  author = {Cukier, Daniel},
  year = {2013},
  pages = {143--152},
  publisher = {{ACM}},
  abstract = {Scaling a web applications can be easy for simple CRUD software running when you use Platform as a Service Clouds (PaaS). But if you need to deploy a complex software, with many components and a lot users, you will need have a mix of cloud services in PaaS, SaaS and IaaS layers. You will also need knowledge in architecture patterns to make all these software components communicate accordingly. In this article, we share our experience of using cloud services to scale a web application. We show usage examples of load balancing, session sharing, e-mail delivery, asynchronous processing, logs processing, monitoring, continuous deployment, realtime user monitoring (RUM). These are a mixture of development and system operations (DevOps) that improved our application availability, scalability and performance.},
  isbn = {9781450319959;1450319955;},
  langid = {english},
  keywords = {APIs,AWS,cloud,cloud computing,devops,ELB,ELO7,email,IAAS,load balancing,paas,rest,SAAS,scalability,scalable,tomcat,web services}
}

@misc{daniel_an_find_2018,
  title = {Find {{Out How You Stack Up}} to {{New Industry Benchmarks}} for {{Mobile Page Speed}}},
  author = {{Daniel An}},
  year = {2018},
  month = feb,
  journal = {Think with Google},
  abstract = {It's critical that marketers design fast mobile web experiences. New research shows how various sectors are performing when it comes to mobile page speed.},
  howpublished = {https://www.thinkwithgoogle.com/marketing-resources/data-measurement/mobile-page-speed-new-industry-benchmarks/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QBNYL3H2\\mobile-page-speed-new-industry-benchmarks.html}
}

@incollection{daniel_chudnov_restful_2007,
  title = {{{RESTful HTTP}} - {{What}} Is the Purpose?},
  booktitle = {Computers in {{Libraries}}},
  author = {Daniel Chudnov},
  year = {2007},
  month = jul,
  pages = {1--2}
}

@article{davoudi_kakhki_evaluating_2019,
  title = {Evaluating Machine Learning Performance in Predicting Injury Severity in Agribusiness Industries},
  author = {Davoudi Kakhki, Fatemeh and Freeman, Steven A. and Mosher, Gretchen A.},
  year = {2019},
  month = aug,
  journal = {Safety Science},
  volume = {117},
  pages = {257--262},
  issn = {0925-7535},
  doi = {10.1016/j.ssci.2019.04.026},
  abstract = {Although machine learning methods have been used as an outcome prediction tool in many fields, their utilization in predicting incident outcome in occupational safety is relatively new. This study tests the performance of machine learning techniques in modeling and predicting occupational incidents severity with respect to accessible information of injured workers in agribusiness industries using workers' compensation claims. More than 33,000 incidents within agribusiness industries in the Midwest of the United States for 2008\textendash 2016 were analyzed. The total cost of incidents was extracted and classified from workers' compensation claims. Supervised machine learning algorithms for classification (support vector machines with linear, quadratic, and RBF kernels, Boosted Trees, and Na\"ive Bayes) were applied. The models can predict injury severity classification based on injured body part, body group, nature of injury, nature group, cause of injury, cause group, and age and tenure of injured workers with the accuracy rate of 92\textendash 98\%. The results emphasize the significance of quantitative analysis of empirical injury data in safety science, and contribute to enhanced understanding of injury patterns using predictive modeling along with safety experts' perspectives with regulatory or managerial viewpoints. The predictive models obtained from this study can be used to augment the experience of safety professionals in agribusiness industries to improve safety intervention efforts.},
  langid = {english},
  keywords = {Injury severity classification,Injury severity prediction,Machine learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9EN58TYU\\Davoudi Kakhki e.a. - 2019 - Evaluating machine learning performance in predict.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\3HAKCAPX\\S092575351831107X.html}
}

@article{de_jong_arma_2004,
  title = {The {{ARMA}} Model in State Space Form},
  author = {{de Jong}, Piet and Penzer, Jeremy},
  year = {2004},
  month = oct,
  journal = {Statistics \& Probability Letters},
  volume = {70},
  number = {1},
  pages = {119--125},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2004.08.006},
  abstract = {This article explores alternative state space representations for ARMA models. We advocate representations that have minimal state order and appealing Kalman filter steady state properties. We derive expressions for smoother output and describe concrete connections to classical infinite sample representations.},
  langid = {english},
  keywords = {Filter steady state,Kalman filter smoother,State space model,Time series},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5PGC2UI8\\de Jong en Penzer - 2004 - The ARMA model in state space form.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\84U7L3WA\\S0167715204002330.html}
}

@misc{defensie_organisatie_nodate,
  title = {Organisatie Defensie Materieel Organisatie},
  author = {{Defensie}},
  journal = {Organisatie defensie materieel organisatie}
}

@article{del_rey_comprehensive_2020,
  title = {A {{Comprehensive Survey}} on {{Local Differential Privacy}}},
  author = {Xiong, Xingxing and Liu, Shubo and Li, Dan and Cai, Zhaohui and Niu, Xiaoguang},
  editor = {Del Rey, Angel M.},
  year = {2020},
  month = oct,
  journal = {Security and Communication Networks},
  volume = {2020},
  pages = {8829523},
  publisher = {{Hindawi}},
  issn = {1939-0114},
  doi = {10.1155/2020/8829523},
  abstract = {With the advent of the era of big data, privacy issues have been becoming a hot topic in public. Local differential privacy (LDP) is a state-of-the-art privacy preservation technique that allows to perform big data analysis (e.g., statistical estimation, statistical learning, and data mining) while guaranteeing each individual participant\&\#x2019;s privacy. In this paper, we present a comprehensive survey of LDP. We first give an overview on the fundamental knowledge of LDP and its frameworks. We then introduce the mainstream privatization mechanisms and methods in detail from the perspective of frequency oracle and give insights into recent studied on private basic statistical estimation (e.g., frequency estimation and mean estimation) and complex statistical estimation (e.g., multivariate distribution estimation and private estimation over complex data) under LDP. Furthermore, we present current research circumstances on LDP including the private statistical learning/inferencing, private statistical data analysis, privacy amplification techniques for LDP, and some application fields under LDP. Finally, we identify future research directions and open challenges for LDP. This survey can serve as a good reference source for the research of LDP to deal with various privacy-related scenarios to be encountered in practice.}
}

@article{demmer_richard_2008,
  title = {{{RICHARD FEYNMAN}}: {{SIMULATING PHYSICS WITH COMPUTERS}}},
  shorttitle = {{{RICHARD FEYNMAN}}},
  author = {Demmer, Michael and Fonseca, Rodrigo and Koushanfar, Farinaz},
  year = {2008},
  month = jan,
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\W3UPIMI5\\Demmer e.a. - 2008 - RICHARD FEYNMAN SIMULATING PHYSICS WITH COMPUTERS.pdf}
}

@incollection{dernoncourt_hypterparameter_nodate,
  title = {{Hypterparameter selection}},
  booktitle = {{Secondary Analysis of Electronic Health Records}},
  author = {Dernoncourt, Franck and Nemati, Shamim and Kassis, Elias Baedorf and Ghassemi, Mahdi Mohammed},
  pages = {419--427},
  isbn = {978-3-319-43742-2},
  langid = {Engels}
}

@incollection{dhillon_gold_2017,
  title = {The {{Gold Rush}}: {{Mining Bitcoin}}},
  shorttitle = {The {{Gold Rush}}},
  booktitle = {Blockchain {{Enabled Applications}}: {{Understand}} the {{Blockchain Ecosystem}} and {{How}} to {{Make}} It {{Work}} for {{You}}},
  author = {Dhillon, Vikram and Metcalf, David and Hooper, Max},
  year = {2017},
  month = nov,
  publisher = {{Apress}},
  abstract = {Work with blockchain and understand its potential application beyond cryptocurrencies in the domains of healthcare, Internet of Things, finance, decentralized organizations, and open science. Featuring case studies and practical insights generated from a start-up spun off from the author's own lab, this book covers a unique mix of topics not found in others and offers insight into how to overcome real hurdles that arise as the market and consumers grow accustomed to blockchain based start-ups. You'll start with a review of the historical origins of blockchain and explore the basic cryptography needed to make the blockchain work for Bitcoin. You will then learn about the technical advancements made in the surrounded ecosystem: the Ethereum virtual machine, Solidity, Colored Coins, the Hyperledger Project, Blockchain-as-a-service offered through IBM, Microsoft and more. This book looks at the consequences of machine-to-machine transactions using the blockchain socially, technologically, economically and politically. Blockchain Enabled Applications provides you with a clear perspective of the ecosystem that has developed around the blockchain and the various industries it has penetrated. What You'll LearnImplement the code-base from Fabric and Sawtooth, two open source blockchain-efforts being developed under the Hyperledger ProjectEvaluate the benefits of integrating blockchain with emerging technologies, such as machine learning and artificial intelligence in the cloudUse the practical insights provided by the case studies to your own projects or start-up ideasSet up a development environment to compile and manage projects Who This Book Is For Developers who are interested in learning about the blockchain as a data-structure, the recent advancements being made and how to implement the code-base. Decision makers within large corporations (product managers, directors or CIO level executives) interested in implementing the blockchain who need more practical insights and not just theory.},
  langid = {english}
}

@article{donders_review_2006,
  title = {Review: {{A}} Gentle Introduction to Imputation of Missing Values},
  shorttitle = {Review},
  author = {Donders, A. Rogier T. and {van der Heijden}, Geert J. M. G. and Stijnen, Theo and Moons, Karel G. M.},
  year = {2006},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  number = {10},
  pages = {1087--1091},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2006.01.014},
  abstract = {In most situations, simple techniques for handling missing data (such as complete case analysis, overall mean imputation, and the missing-indicator method) produce biased results, whereas imputation techniques yield valid results without complicating the analysis once the imputations are carried out. Imputation techniques are based on the idea that any subject in a study sample can be replaced by a new randomly chosen subject from the same source population. Imputation of missing data on a variable is replacing that missing by a value that is drawn from an estimate of the distribution of this variable. In single imputation, only one estimate is used. In multiple imputation, various estimates are used, reflecting the uncertainty in the estimation of this distribution. Under the general conditions of so-called missing at random and missing completely at random, both single and multiple imputations result in unbiased estimates of study associations. But single imputation results in too small estimated standard errors, whereas multiple imputation results in correctly estimated standard errors and confidence intervals. In this article we explain why all this is the case, and use a simple simulation study to demonstrate our explanations. We also explain and illustrate why two frequently used methods to handle missing data, i.e., overall mean imputation and the missing-indicator method, almost always result in biased estimates.},
  langid = {english},
  keywords = {Bias,Indicator method,Missing data,Multiple imputation,Precision,Single imputation},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\68BE2T5Y\\Donders e.a. - 2006 - Review A gentle introduction to imputation of mis.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\R7PBVK2W\\S0895435606001971.html}
}

@book{donohoe_software_1999,
  title = {Software {{Architecture}}},
  editor = {Donohoe, Patrick},
  year = {1999},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  volume = {12},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-35563-4},
  isbn = {978-1-4757-6536-6 978-0-387-35563-4},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2J9NQZIJ\\Donohoe - 1999 - Software Architecture.pdf}
}

@incollection{dougherty_supervised_1995,
  title = {Supervised and Unsupervised Discretization of Continuous Features},
  booktitle = {Machine {{Learning Proceedings}} 1995},
  author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
  year = {1995},
  pages = {194--202},
  publisher = {{Elsevier}}
}

@article{dougherty_supervised_nodate,
  title = {Supervised and {{Unsupervised Discretization}} of {{Contionuous Features}}},
  author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
  abstract = {Mehran}
}

@article{dragoni_microservices_2017,
  title = {Microservices: Yesterday, Today, and Tomorrow},
  shorttitle = {Microservices},
  author = {Dragoni, Nicola and Giallorenzo, Saverio and Lafuente, Alberto Lluch and Mazzara, Manuel and Montesi, Fabrizio and Mustafin, Ruslan and Safina, Larisa},
  year = {2017},
  month = apr,
  journal = {arXiv:1606.04036 [cs]},
  eprint = {1606.04036},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Microservices is an architectural style inspired by service-oriented computing that has recently started gaining popularity. Before presenting the current state-of-the-art in the field, this chapter reviews the history of software architecture, the reasons that led to the diffusion of objects and services first, and microservices later. Finally, open problems and future challenges are introduced. This survey primarily addresses newcomers to the discipline, while offering an academic viewpoint on the topic. In addition, we investigate some practical issues and point out some potential solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MIHLZHAM\\Dragoni e.a. - 2017 - Microservices yesterday, today, and tomorrow.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\CP4N8LMI\\1606.html}
}

@misc{drifty_ionic_nodate,
  title = {Ionic {{Framework}}},
  author = {Drifty},
  journal = {Ionic Framework},
  abstract = {Ionic is the app platform for web developers. Build amazing mobile, web, and desktop apps all with one shared code base and open web standards},
  howpublished = {https://ionicframework.com/docs/cli/build/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\S2UBENJP\\build.html}
}

@article{dutta_comparative_2019,
  title = {Comparative {{Study}} of {{Cloud Services Offered}} by {{Amazon}}, {{Microsoft}} \& {{Google}}},
  author = {Dutta, Pranay and Dutta, Prashant},
  year = {2019},
  pages = {6},
  abstract = {With the accelerated development in processing and storage technologies and the boom of the Internet, IT Hardware have become inexpensive, more potent and highly available than ever before. This has triggered the commencement of a new computing model called cloud computing, in which IT-Resources are catered as general utilities that can be hired and released by users through the Internet in an on-demand fashion. According to Gartner Survey Report, The marketplace for public-cloud is predicted to reach from `\$260 billion' in 2017 to around `\$411 billion in 2020'. As more and more IT systems are externalized, it is getting more imminent for us to choose the right cloud service provider for long term success. However, the challenge is to choose the best cloud service provider from the vast number of options like AWS, Azure and GCP to the smaller CSP's. As per Market share the Top 3 Global leaders are AWS, Azure and GCP1. In this paper, we will try to make analysis of between the three market leaders in cloud services namely Amazon, Microsoft \& Google. We will also compare and contrast what AWS, Azure, and GCP offer in terms of storage, compute, management tools etc.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SS9ZBBQ2\\Dutta en Dutta - 2019 - Comparative Study of Cloud Services Offered by Ama.pdf}
}

@article{ebert_devops_2016,
  title = {{{DevOps}}},
  author = {Ebert, Christof and Gallardo, Gorka and Hernantes, Josune and Serrano, Nicolas},
  year = {2016},
  journal = {Ieee Software},
  volume = {33},
  number = {3},
  pages = {94--100},
  publisher = {{IEEE}},
  issn = {0740-7459}
}

@misc{eerdere_stagiair_defensie_stage_2018,
  title = {Stage Bij Defensie Orientatieverslag},
  author = {{Eerdere stagiair Defensie}},
  year = {2018},
  month = mar
}

@misc{elliot_cooper_modsecurity_2018,
  title = {{{ModSecurity}} and Nginx | {{Linux Journal}}},
  author = {{Elliot Cooper}},
  year = {2018},
  month = sep,
  journal = {ModSecurity and Nginx},
  howpublished = {https://www.linuxjournal.com/content/modsecurity-and-nginx},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\DWW3U6ED\\modsecurity-and-nginx.html}
}

@article{erich_qualitative_2017,
  title = {A {{Qualitative Study}} of {{DevOps Usage}} in {{Practice}}},
  author = {Erich, Floris and Amrit, Chintan and Daneva, Maya},
  year = {2017},
  month = jun,
  journal = {Journal of Software: Evolution and Process},
  volume = {00},
  doi = {10.1002/smr.1885},
  abstract = {Organizations are introducing agile and lean software development techniques in operations to increase the pace of their software development process and to improve the quality of their software. They use the term DevOps, a portmanteau of development and operations, as an umbrella term to describe their efforts. In this paper we describe the ways in which organizations implement DevOps and the outcomes they experience. We first summarize the results of a Systematic Literature Review that we performed to discover what researchers have written about DevOps. We then describe the results of an exploratory interview-based study involving six organizations of various sizes that are active in various industries. As part of our findings, we observed that all organizations were positive about their experiences and only minor problems were encountered while adopting DevOps.}
}

@inproceedings{erlingsson_rappor_2014,
  title = {{{RAPPOR}}: {{Randomized Aggregatable Privacy-Preserving Ordinal Response}}},
  shorttitle = {{{RAPPOR}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Erlingsson, {\'U}lfar and Pihur, Vasyl and Korolova, Aleksandra},
  year = {2014},
  month = nov,
  pages = {1054--1067},
  publisher = {{ACM}},
  address = {{Scottsdale Arizona USA}},
  doi = {10.1145/2660267.2660348},
  abstract = {Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports.},
  isbn = {978-1-4503-2957-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GRMRMG69\\Erlingsson e.a. - 2014 - RAPPOR Randomized Aggregatable Privacy-Preserving.pdf}
}

@incollection{evans_domain-driven_2014,
  title = {Domain-{{Driven Design Reference}}: {{Definitions}} and {{Pattern Summaries}}},
  booktitle = {Domain-{{Driven Design Reference}}: {{Definitions}} and {{Pattern Summaries}}},
  author = {Evans, Eric},
  year = {2014},
  pages = {10},
  publisher = {{Dog Ear Publishing}},
  isbn = {1-4575-0119-8}
}

@incollection{evans_notitle_2004,
  booktitle = {Domain-Driven {{Design}}: {{Tackling Complexity}} in the {{Heart}} of {{Software}}},
  author = {Evans, Eric and Evans, Eric J.},
  year = {2004},
  pages = {21--22},
  publisher = {{Addison-Wesley Professional}},
  abstract = {"Eric Evans has written a fantastic book on how you can make the design of your software match your mental model of the problem domain you are addressing.  "His book is very compatible with XP. It is not about drawing pictures of a domain; it is about how you think of it, the language you use to talk about it, and how you organize your software to reflect your improving understanding of it. Eric thinks that learning about your problem domain is as likely to happen at the end of your project as at the beginning, and so refactoring is a big part of his technique.  "The book is a fun read. Eric has lots of interesting stories, and he has a way with words. I see this book as essential reading for software developers--it is a future classic."  --Ralph Johnson, author of Design Patterns  "If you don't think you are getting value from your investment in object-oriented programming, this book will tell you what you've forgotten to do. "Eric Evans convincingly argues for the importance of domain modeling as the central focus of development and provides a solid framework and set of techniques for accomplishing it. This is timeless wisdom, and will hold up long after the methodologies du jour have gone out of fashion."  --Dave Collins, author of Designing Object-Oriented User Interfaces   "Eric weaves real-world experience modeling--and building--business applications into a practical, useful book. Written from the perspective of a trusted practitioner, Eric's descriptions of ubiquitous language, the benefits of sharing models with users, object life-cycle management, logical and physical application structuring, and the process and results of deep refactoring are major contributions to our field."  --Luke Hohmann, author of Beyond Software Architecture   "This book belongs on the shelf of every thoughtful software developer." --Kent Beck  "What Eric has managed to capture is a part of the design process that experienced object designers have always used, but that we have been singularly unsuccessful as a group in conveying to the rest of the industry. We've given away bits and pieces of this knowledge...but we've never organized and systematized the principles of building domain logic. This book is important." --Kyle Brown, author of Enterprise Java(TM) Programming with IBM(R) WebSphere(R)  The software development community widely acknowledges that domain modeling is central to software design. Through domain models, software developers are able to express rich functionality and translate it into a software implementation that truly serves the needs of its users. But despite its obvious importance, there are few practical resources that explain how to incorporate effective domain modeling into the software development process.  Domain-Driven Design fills that need. This is not a book about specific technologies. It offers readers a systematic approach to domain-driven design, presenting an extensive set of design best practices, experience-based techniques, and fundamental principles that facilitate the development of software projects facing complex domains. Intertwining design and development practice, this book incorporates numerous examples based on actual projects to illustrate the application of domain-driven design to real-world software development.  Readers learn how to use a domain model to make a complex development effort more focused and dynamic. A core of best practices and standard patterns provides a common language for the development team. A shift in emphasis--refactoring not just the code but the model underlying the code--in combination with the frequent iterations of Agile development leads to deeper insight into domains and enhanced communication between domain expert and programmer. Domain-Driven Design then builds on this foundation, and addresses modeling and design for complex systems and larger organizations.Specific topics covered include:   Getting all team members to speak the same language Connecting model and implementation more deeply Sharpening key distinctions in a model Managing the lifecycle of a domain object  Writing domain code that is safe to combine in elaborate ways Making complex code obvious and predictable Formulating a domain vision statement  Distilling the core of a complex domain Digging out implicit concepts needed in the model  Applying analysis patterns  Relating design patterns to the model  Maintaining model integrity in a large system Dealing with coexisting models on the same project Organizing systems with large-scale structures Recognizing and responding to modeling breakthroughs  With this book in hand, object-oriented developers, system analysts, and designers will have the guidance they need to organize and focus their work, create rich and useful domain models, and leverage those models into quality, long-lasting software implementations.},
  googlebooks = {xColAAPGubgC},
  isbn = {978-0-321-12521-7},
  langid = {english},
  keywords = {Computers / Programming / Object Oriented,Computers / Software Development \& Engineering / General,Computers / Software Development \& Engineering / Systems Analysis \& Design}
}

@article{fadhilah_iskandar_comparison_2020,
  title = {Comparison between Client-Side and Server-Side Rendering in the Web Development},
  author = {Fadhilah Iskandar, Taufan and Lubis, Muharman and Fabrianti Kusumasari, Tien and Ridho Lubis, Arif},
  year = {2020},
  month = may,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {801},
  number = {1},
  pages = {012136},
  issn = {1757-8981, 1757-899X},
  doi = {10.1088/1757-899X/801/1/012136},
  abstract = {Mandatory servers for universal applications that is accessible to number of users may be a deterrent for the corporation and excessive for small applications even though it could bring the compatibility advantages. Knowing that demand of web application increases to provide convenience and ease of use to the users, client side rendering comes to create software more fast and efficient. It has been done by redirecting the request towards an HTML file then the server will give messages without any content or a loading screen until the device takes all JavaScript to allow the browser compiling everything before displaying the content. Therefore, the purpose of this paper is to analyse the comparison between client side and server side method in the respect of technical aspects in term of first content paint, speed index, time to interactive, first meaningful paint, first idle CPU and estimated input latency that present better performance with 2.1s, 2.0s, 2.2s, 2.1s, 2.2s and 20ms respectively on server side. It also provide better result based on Google Audit with 100\% performance, 48\% accessibility, 93\% best practice and 89\% of search engine optimization (SEO).},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\IM8IXMYD\\Fadhilah Iskandar e.a. - 2020 - Comparison between client-side and server-side ren.pdf}
}

@incollection{fjordvald_autobench_2018,
  title = {{Autobench}},
  booktitle = {{Nginx HTTP Server - Fourth Edition}},
  author = {Fjordvald, Martin and Nedelcu, Clement},
  year = {2018},
  month = feb,
  publisher = {{Packt Publishing}},
  isbn = {978-1-78862-355-1},
  langid = {Engels}
}

@book{fjordvald_nginx_2018,
  title = {{Nginx HTTP Server - Fourth Edition}},
  author = {Fjordvald, Martin and Nedelcu, Clement},
  year = {2018},
  month = feb,
  publisher = {{Packt Publishing}},
  isbn = {978-1-78862-355-1},
  langid = {Engels}
}

@article{fuggetta_software_nodate,
  title = {Software {{Process}}},
  author = {Fuggetta, Alfonso and Nitto, Elisabetta Di},
  pages = {12},
  abstract = {This paper is a travelogue of Software Process research and practice in the past 15 years. It is based on the paper written by one of the authors for the FOSE Track at ICSE 2000. Since then, the landscape of Software Process research has significantly evolved: technological breakthroughs and market disruptions have defined new and complex challenges for Software Engineering researchers and practitioners.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\G5NQRLWE\\Fuggetta en Nitto - Software Process.pdf}
}

@misc{gagliardoni_quantum_2021,
  title = {Quantum {{Attack Resource Estimate}}: {{Using Shor}}'s {{Algorithm}} to {{Break RSA}} vs {{DH}}/{{DSA VS ECC}}},
  shorttitle = {Quantum {{Attack Resource Estimate}}},
  author = {Gagliardoni, Tommaso},
  year = {2021},
  month = aug,
  journal = {Kudelski Security Research},
  abstract = {Most security experts are by now aware of the threat that the rise of quantum computing poses to modern cryptography. Shor's quantum algorithm, in particular, provides a large theoretical spe\ldots},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6GI99WG7\\quantum-attack-resource-estimate-using-shors-algorithm-to-break-rsa-vs-dh-dsa-vs-ecc.html}
}

@book{gamma_design_1995,
  title = {Design Patterns: Elements of Reusable Object-Oriented Software},
  author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Johnson, Ralph E and Vlissides, John},
  year = {1995},
  publisher = {{Pearson Deutschland GmbH}},
  isbn = {0-201-63361-2}
}

@book{garcia_data_2015,
  title = {Data Preprocessing in Data Mining},
  author = {Garc{\'i}a, Salvador and Luengo, Juli{\'a}n and Herrera, Francisco},
  year = {2015},
  publisher = {{Springer}},
  isbn = {3-319-10247-8}
}

@incollection{garrigos_challenges_2018,
  title = {Challenges {{When Moving}} from {{Monolith}} to {{Microservice Architecture}}},
  booktitle = {Current {{Trends}} in {{Web Engineering}}},
  author = {Kalske, Miika and M{\"a}kitalo, Niko and Mikkonen, Tommi},
  editor = {Garrig{\'o}s, Irene and Wimmer, Manuel},
  year = {2018},
  volume = {10544},
  pages = {32--47},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-74433-9_3},
  abstract = {One of the more recent avenues towards more flexible installations and execution is the transition from monolithic architecture to microservice architecture. In such architecture, where microservices can be more liberally updated, relocated, and replaced, building liquid software also becomes simpler, as adaptation and deployment of code is easier than when using a monolithic architecture where almost everything is connected. In this paper, we study this type of transition. The objective is to identify the reasons why the companies decide to make such transition, and identify the challenges that companies may face during this transition. Our method is a survey based on di{$\carriagereturn$}erent publications and case studies conducted about these architectural transitions from monolithic architecture to microservices. Our findings reveal that typical reasons moving towards microservice architecture are complexity, scalability and code ownership. The challenges, on the other hand, can be separated to architectural challenges and organizational challenges. The conclusion is that when a software company grows big enough in size and starts facing problems regarding the size of the codebase, that is when microservices can be a good way to handle the complexity and size. Even though the transition provides its own challenges, these challenges can be easier to solve than the challenges that monolithic architecture presents to company.},
  isbn = {978-3-319-74432-2 978-3-319-74433-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\P9RPTTCQ\\Kalske e.a. - 2018 - Challenges When Moving from Monolith to Microservi.pdf}
}

@book{gechev_getting_2017,
  title = {Getting {{Started}} with {{Angular}} - {{Second Edition}}},
  author = {Gechev, Minko},
  year = {2017},
  month = feb,
  edition = {Second},
  publisher = {{Packt Publishing}},
  abstract = {Fast-track your web development skills to build high performance SPA with Angular 2 and beyondAbout This BookUp to date with the latest API changes introduced by Angular 2 and 4Get familiar with the improvements to directives, change detection, dependency injection, router, and moreUnderstand Angular's new component-based architectureStart using TypeScript to supercharge your Angular applicationsWho This Book Is ForDo you want to jump in at the deep end of Angular? Or perhaps you're interested assessing the changes to AngularJS before moving over? If so, then "Getting Started with Angular" is the book for you.To get the most out of the book, you'll need to be familiar with AngularJS 1.x, and have a good understanding of JavaScript.What You Will LearnUnderstand the changes made from AngularJS with side-by-side code samples to help demystify the Angular learning curveStart working with Angular's new method of implementing directivesUse TypeScript to write modern, powerful Angular applicationsDig in to the change detection method, and other architectural changes to make sure you know what's going on under the hood of AngularGet to work with the new router in AngularUse the new features of Angular, including pipes, and the updated features such as forms, services, and dependency injectionLearn about the server-side rendering in Angular to keep your new applications SEO-friendlyEnhance your applications using Ahead-of-Time compilation and Web WorkersIn DetailI'm delighted to see this new update and hope it helps you build amazing things with Angular. - Mi\v{s}ko Hevery, Creator of AngularJS and AngularAngular is the modern framework you need to build performant and robust web applications. This book is the quickest way to upgrade your AngularJS knowledge to the brave new world of Angular, and get grips with the framework.It starts with an overview putting the changes of the framework in context with version 1. After that, you will be taken on a TypeScript crash-course so you can take advantage of Angular in its native, statically-typed environment. You'll explore the new change detection mechanism in detail, how directives and components have changed, how you create applications with Angular, and much more. Next, you'll understand how to efficienly develop forms, use the router, implement communication with HTTP services, and transform data with custom pipes. Finally, we will take a look at the Angular's Ahead-of-Time compiler, angular-cli and other such tools that help us build professional applications.By the end of the book, you'll be ready to start building quick and efficient Angular applications compatible with v2 and v4, that take advantage of all the new features on offer.This book is up to date for the 2.4 release and is compatible with the 4.0 release as well.Style and approachStarting with a comparison between Angular versions, this book is filled with side-by-side code examples to help highlight the changes. Each chapter then looks at major changes to the framework and is filled with small examples and sample code to get you started.Downloading the example code for this book. You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com. If you purchased this book elsewhere, you can visit http://www.PacktPub.com/support and register to have the code file.},
  isbn = {978-1-78712-129-4},
  langid = {english}
}

@book{geron_hands-machine_2017,
  title = {{Hands-On Machine Learning with Scikit-Learn \& TensorFlow}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2017},
  month = mar,
  volume = {1ste},
  publisher = {{O'REILLY}},
  isbn = {978-1-4919-6229-9},
  langid = {Engels}
}

@misc{gertjan_nieuws_nodate,
  title = {{Nieuws}},
  author = {{gertjan}},
  journal = {Payconiq NL},
  abstract = {Lees hier het laatste nieuws over Payconiq Nederland en laat je inspireren over de wereld van mobiel betalen. Je leest hier alles over de laatste trends, initiatieven en ontwikkelingen rondom de Payconiq app.},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BKRR6TC6\\nieuws.html}
}

@misc{gertjan_pay_nodate,
  title = {{Pay fast, pay easy, Payconiq}},
  author = {{gertjan}},
  journal = {Payconiq NL},
  abstract = {Payconiq stelt je in staat te betalen met jouw smartphone. In winkels, online en onderling tussen vrienden. Koppel je bankrekening aan jouw Payconiq account en je bent klaar! Met ruim 45.000 betaalpunten in Nederland en Belgi\"e is Payconiq onderdeel van het dagelijks leven. Ben jij klaar voor de toekomst?},
  howpublished = {https://www.payconiq.nl/nl/},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\IMCF9DYF\\nl.html}
}

@article{geurts_extremely_2006,
  title = {Extremely Randomized Trees},
  author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  year = {2006},
  journal = {Machine learning},
  volume = {63},
  number = {1},
  pages = {3--42},
  publisher = {{Springer}},
  issn = {0885-6125}
}

@article{geurts_extremely_2006-1,
  title = {Extremely Randomized Trees},
  author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  year = {2006},
  journal = {Machine learning},
  volume = {63},
  number = {1},
  pages = {3--42},
  publisher = {{Springer}},
  issn = {0885-6125}
}

@book{geurts_mach_2006,
  title = {Mach {{Learn}} (): {{DOI}} 10.1007/S10994-006-6226-1 {{Extremely}} Randomized Trees},
  shorttitle = {Mach {{Learn}} ()},
  author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  year = {2006},
  abstract = {Abstract This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.}
}

@article{ghosh_outliers_2012,
  title = {Outliers: {{An Evaluation}} of {{Methodologies}}},
  author = {Ghosh, Dhiren and Vogt, Andrew},
  year = {2012},
  pages = {6},
  abstract = {Given a random sample x1, x2, . . . , xn from a population, we examine circumstances that lead to some of the values being deemed outliers and the methodologies proposed to analyze the data set in the presence of these outliers. We review methods of determining outliers and propose general principles for how to proceed. Often a mixture approach is appropriate: most observations seem to follow a pattern or to satisfy a model, while the outliers remain outside the pattern or model and require further investigation.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\D6FJ4JXA\\Ghosh en Vogt - 2012 - Outliers An Evaluation of Methodologies.pdf}
}

@inproceedings{goldberger_neighbourhood_2005,
  title = {Neighbourhood Components Analysis},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam T and Salakhutdinov, Russ R},
  year = {2005},
  pages = {513--520}
}

@article{gonzalez_instance-based_2003,
  title = {Instance-Based Learning in Dynamic Decision Making},
  author = {Gonzalez, Cleotilde and Lerch, Javier F. and Lebiere, Christian},
  year = {2003},
  journal = {Cognitive Science},
  volume = {27},
  number = {4},
  pages = {591--635},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2704_2},
  abstract = {This paper presents a learning theory pertinent to dynamic decision making (DDM) called instancebased learning theory (IBLT). IBLT proposes five learning mechanisms in the context of a decision-making process: instance-based knowledge, recognition-based retrieval, adaptive strategies, necessity-based choice, and feedback updates. IBLT suggests in DDM people learn with the accumulation and refinement of instances, containing the decision-making situation, action, and utility of decisions. As decision makers interact with a dynamic task, they recognize a situation according to its similarity to past instances, adapt their judgment strategies from heuristic-based to instance-based, and refine the accumulated knowledge according to feedback on the result of their actions. The IBLT's learning mechanisms have been implemented in an ACT-R cognitive model. Through a series of experiments, this paper shows how the IBLT's learning mechanisms closely approximate the relative trend magnitude and performance of human data. Although the cognitive model is bounded within the context of a dynamic task, the IBLT is a general theory of decision making applicable to other dynamic environments.},
  langid = {english},
  keywords = {Cognitive modeling,Decision making,Dynamic decision making,Instance-based learning,Water purification plant},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WDZIX3AE\\Gonzalez e.a. - 2003 - Instance-based learning in dynamic decision making.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\RWEIDC6Q\\s15516709cog2704_2.html}
}

@misc{google_app_nodate,
  title = {App Startup Time},
  author = {{Google}},
  journal = {Android Developers},
  howpublished = {https://developer.android.com/topic/performance/vitals/launch-time},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MGAMNGTE\\launch-time.html}
}

@book{gorton_essential_2011,
  title = {Essential {{Software Architecture}}},
  author = {Gorton, Ian},
  year = {2011},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-19176-3},
  isbn = {978-3-642-19175-6 978-3-642-19176-3},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TULP5N38\\Gorton - 2011 - Essential Software Architecture.pdf}
}

@book{gruhn_essence_2018,
  title = {The {{Essence}} of {{Software Engineering}}},
  editor = {Gruhn, Volker and Striemer, R{\"u}diger},
  year = {2018},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-73897-0},
  isbn = {978-3-319-73896-3 978-3-319-73897-0},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JC6MLCQH\\Gruhn en Striemer - 2018 - The Essence of Software Engineering.pdf}
}

@incollection{gutwirth_architecture_2010,
  title = {Architecture {{Is Politics}}: {{Security}} and {{Privacy Issues}} in {{Transport}} and {{Beyond}}},
  shorttitle = {Architecture {{Is Politics}}},
  booktitle = {Data {{Protection}} in a {{Profiled World}}},
  author = {Jacobs, Bart},
  editor = {Gutwirth, Serge and Poullet, Yves and De Hert, Paul},
  year = {2010},
  pages = {289--299},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-90-481-8865-9_18},
  abstract = {This paper discusses the political relevance of ICT-architecture through a review of recent developments in the Netherlands, involving the bumpy introduction of a national smart card for public transport and the plans for electronic traffic pricing based on actual road usage of individual cars. One of the underlying themes is the centralised or decentralised storage of privacy-sensitive data, where centralised informational control supports centralised societal control.},
  isbn = {978-90-481-8864-2 978-90-481-8865-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RZ6QFJMP\\Jacobs - 2010 - Architecture Is Politics Security and Privacy Iss.pdf}
}

@article{guyon_introduction_nodate,
  title = {An {{Introduction}} to {{Variable}} and {{Feature Selection}}},
  author = {Guyon, Isabelle and Elisseeff, Andre},
  pages = {26},
  abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RYMJZBQ7\\Guyon en Elisseeff - An Introduction to Variable and Feature Selection.pdf}
}

@article{haber_how_nodate,
  title = {How to {{Time-Stamp}} a {{Digital Document}}},
  author = {Haber, Stuart and Stornetta, W Scott},
  pages = {13},
  abstract = {The prospect of a world in which all text, audio, picture, and video documents are in digital form on easily modi able media raises the issue of how to certify when a document was created or last changed. The problem is to time-stamp the data, not the medium. We propose computationally practical procedures for digital time-stamping of such documents so that it is infeasible for a user either to back-date or to forward-date his document, even with the collusion of a time-stamping service. Our procedures maintain complete privacy of the documents themselves, and require no record-keeping by the time-stamping service.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VJKUEZET\\Haber en Stornetta - How to Time-Stamp a Digital Document.pdf}
}

@article{haber_how_nodate-1,
  title = {How to {{Time-Stamp}} a {{Digital Document}}},
  author = {Haber, Stuart and Stornetta, W Scott},
  pages = {13},
  abstract = {The prospect of a world in which all text, audio, picture, and video documents are in digital form on easily modi able media raises the issue of how to certify when a document was created or last changed. The problem is to time-stamp the data, not the medium. We propose computationally practical procedures for digital time-stamping of such documents so that it is infeasible for a user either to back-date or to forward-date his document, even with the collusion of a time-stamping service. Our procedures maintain complete privacy of the documents themselves, and require no record-keeping by the time-stamping service.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JXRBU7M8\\Haber en Stornetta - How to Time-Stamp a Digital Document.pdf}
}

@misc{habib_performance_2016,
  title = {{A Performance Analysis of Python WSGI Servers: Part 2 | Blog | AppDynamics}},
  shorttitle = {{A Performance Analysis of Python WSGI Servers}},
  author = {Habib, Omed},
  year = {2016},
  month = may,
  journal = {Application Performance Monitoring Blog | AppDynamics},
  abstract = {Read more in part 2 of this series. In this post, we'll show you the result of our performance benchmark analysis of Python WSGI servers.},
  howpublished = {https://blog.appdynamics.com/engineering/a-performance-analysis-of-python-wsgi-servers-part-2/},
  langid = {og:},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\K6W24IYQ\\a-performance-analysis-of-python-wsgi-servers-part-2.html}
}

@misc{han_data_nodate,
  title = {Data {{Mining}}, {{Southeast Asia Edition}}},
  author = {Han, Jiawei},
  howpublished = {http://web.b.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzE5MzY0N19fQU41?sid=d824afe4-bb0c-4881-ad5e-a3edcecd9e19@pdc-v-sessmgr06\&vid=0\&format=EB\&lpid=lp\_48\&rid=0},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XLCXTUX2\\bmxlYmtfXzE5MzY0N19fQU41.html}
}

@incollection{har-peled_constraint_2003,
  title = {Constraint {{Classification}} for {{Multiclass Classification}} and {{Ranking}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  author = {{Har-Peled}, Sariel and Roth, Dan and Zimak, Dav},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2003},
  pages = {809--816},
  publisher = {{MIT Press}},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KMUALCWG\\2295-constraint-classification-for-multiclass-classification-and-ranking.html}
}

@inproceedings{hasselbring_microservice_2017,
  title = {Microservice {{Architectures}} for {{Scalability}}, {{Agility}} and {{Reliability}} in {{E-Commerce}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Software Architecture Workshops}} ({{ICSAW}})},
  author = {Hasselbring, Wilhelm and Steinacker, Guido},
  year = {2017},
  month = apr,
  pages = {243--246},
  publisher = {{IEEE}},
  address = {{Gothenburg, Sweden}},
  doi = {10.1109/ICSAW.2017.11},
  abstract = {Microservice architectures provide small services that may be deployed and scaled independently of each other, and may employ different middleware stacks for their implementation. Microservice architectures intend to overcome the shortcomings of monolithic architectures where all of the application's logic and data are managed in one deployable unit. We present how the properties of microservice architectures facilitate scalability, agility and reliability at otto.de, which is one of the biggest European e-commerce platforms. In particular, we discuss vertical decomposition into self contained systems and appropriate granularity of microservices as well as coupling, integration, scalability and monitoring of microservices at otto.de. While increasing agility to more than 500 live deployments per week, high reliability is achieved by means of automated quality assurance with continuous integration and deployment.},
  isbn = {978-1-5090-4793-2},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CX998XBX\\Hasselbring en Steinacker - 2017 - Microservice Architectures for Scalability, Agilit.pdf}
}

@article{heer_tour_2010,
  title = {A {{Tour}} through the {{Visualization Zoo}}: {{A}} Survey of Powerful Visualization Techniques, from the Obvious to the Obscure},
  author = {Heer, Jeffrey and Bostock, Michael and Ogievetsky, Vadim},
  year = {2010},
  journal = {ACM queue},
  volume = {8},
  number = {5},
  pages = {20--30},
  issn = {1542-7730},
  doi = {10.1145/1794514.1805128},
  abstract = {Thanks to advances in sensing, networking, and data management, our society is producing digital information at an astonishing rate. According to one estimate, in 2010 alone we will generate 1,200 exabytes -- 60 million times the content of the Library of Congress. Within this deluge of data lies a wealth of valuable information on how we conduct our businesses, governments, and personal lives. To put the information to good use, we must find ways to explore, relate, and communicate the data meaningfully.},
  langid = {english}
}

@inproceedings{heinrich_performance_2017,
  title = {Performance {{Engineering}} for {{Microservices}}: {{Research Challenges}} and {{Directions}}},
  shorttitle = {Performance {{Engineering}} for {{Microservices}}},
  booktitle = {Proceedings of the 8th {{ACM}}/{{SPEC}} on {{International Conference}} on {{Performance Engineering Companion}}},
  author = {Heinrich, Robert and {van Hoorn}, Andr{\'e} and Knoche, Holger and Li, Fei and Lwakatare, Lucy Ellen and Pahl, Claus and Schulte, Stefan and Wettinger, Johannes},
  year = {2017},
  month = apr,
  pages = {223--226},
  publisher = {{ACM}},
  address = {{L'Aquila Italy}},
  doi = {10.1145/3053600.3053653},
  abstract = {Microservices complement approaches like DevOps and continuous delivery in terms of software architecture. Along with this architectural style, several important deployment technologies, such as container-based virtualization and container orchestration solutions, have emerged. These technologies allow to efficiently exploit cloud platforms, providing a high degree of scalability, availability, and portability for microservices.},
  isbn = {978-1-4503-4899-7},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\A6JCYUXV\\Heinrich e.a. - 2017 - Performance Engineering for Microservices Researc.pdf}
}

@inproceedings{heitlager_practical_2007,
  title = {A {{Practical Model}} for {{Measuring Maintainability}}},
  booktitle = {6th {{International Conference}} on the {{Quality}} of {{Information}} and {{Communications Technology}} ({{QUATIC}} 2007)},
  author = {Heitlager, Ilja and Kuipers, Tobias and Visser, Joost},
  year = {2007},
  month = sep,
  pages = {30--39},
  publisher = {{IEEE}},
  address = {{Lisbon, Portugal}},
  doi = {10.1109/QUATIC.2007.8},
  abstract = {The amount of effort needed to maintain a software system is related to the technical quality of the source code of that system. The ISO 9126 model for software product quality recognizes maintainability as one of the 6 main characteristics of software product quality, with adaptability, changeability, stability, and testability as subcharacteristics of maintainability. Remarkably, ISO 9126 does not provide a consensual set of measures for estimating maintainability on the basis of a system's source code. On the other hand, the Maintainability Index has been proposed to calculate a single number that expresses the maintainability of a system.},
  isbn = {978-0-7695-2948-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\I9NL3GWZ\\Heitlager e.a. - 2007 - A Practical Model for Measuring Maintainability.pdf}
}

@inproceedings{heitlager_practical_2007-1,
  title = {A {{Practical Model}} for {{Measuring Maintainability}}},
  booktitle = {6th {{International Conference}} on the {{Quality}} of {{Information}} and {{Communications Technology}} ({{QUATIC}} 2007)},
  author = {Heitlager, Ilja and Kuipers, Tobias and Visser, Joost},
  year = {2007},
  month = sep,
  pages = {30--39},
  publisher = {{IEEE}},
  address = {{Lisbon, Portugal}},
  doi = {10.1109/QUATIC.2007.8},
  abstract = {The amount of effort needed to maintain a software system is related to the technical quality of the source code of that system. The ISO 9126 model for software product quality recognizes maintainability as one of the 6 main characteristics of software product quality, with adaptability, changeability, stability, and testability as subcharacteristics of maintainability. Remarkably, ISO 9126 does not provide a consensual set of measures for estimating maintainability on the basis of a system's source code. On the other hand, the Maintainability Index has been proposed to calculate a single number that expresses the maintainability of a system.},
  isbn = {978-0-7695-2948-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\N7KP8W32\\Heitlager e.a. - 2007 - A Practical Model for Measuring Maintainability.pdf}
}

@book{hill_blockchain_2018,
  title = {Blockchain {{Quick Reference}}},
  author = {Hill, Brenn and Chopra, Samanyu and Valencourt, Paul},
  year = {2018},
  month = aug,
  publisher = {{Packt Publishing}},
  abstract = {Understand the Blockchain revolution and get to grips with Ethereum, Hyperledger Fabric, and Corda.Key FeaturesResolve common challenges and problems faced in the Blockchain domainStudy architecture, concepts, terminologies, and DappsMake smart choices using Blockchain for personal and business investmentsBook DescriptionBlockchain Quick Reference takes you through the electrifying world of blockchain technology and is designed for those who want to polish their existing knowledge regarding the various pillars of the blockchain ecosystem.This book is your go-to guide, teaching you how to apply principles and ideas for making your life and business better. You will cover the architecture, Initial Coin Offerings (ICOs), tokens, smart contracts, and terminologies of the blockchain technology, before studying how they work. All you need is a curious mind to get started with blockchain technology. Once you have grasped the basics, you will explore components of Ethereum, such as ether tokens, transactions, and smart contracts, in order to build simple Dapps. You will then move on to learning why Solidity is used specifically for Ethereum-based projects, followed by exploring different types of blockchain with easy-to-follow examples. All this will help you tackle challenges and problems. By the end of this book, you will not only have solved current and future problems relating to blockchain technology but will also be able to build efficient decentralized applications.What you will learnUnderstand how blockchain architecture components workAcquaint yourself with cryptography and the mechanics behind blockchainApply consensus protocol to determine the business sustainabilityUnderstand what ICOs and crypto-mining are and how they workCreate cryptocurrency wallets and coins for transaction mechanismsUnderstand the use of Ethereum for smart contract and DApp developmentWho this book is forBlockchain Quick Reference is for you if you are a developer who wants to get well-versed with blockchain and its associated concepts and terminologies. You will explore the working mechanism of a decentralized application with the help of examples. Business leaders and blockchain enthusiasts will also find this book useful, as it will help you effectively address challenges and make better personal and business investments.Downloading the example code for this book You can download the example code files for all Packt books you have purchased from your account at http://www.PacktPub.com. If you purchased this book elsewhere, you can visit http://www.PacktPub.com/support and register to have the files e-mailed directly to you.},
  isbn = {978-1-78899-425-5},
  langid = {english}
}

@article{hirse_htmlelement.style_2018,
  title = {{{HTMLELement}}.Style},
  author = {{Hirse}},
  year = {2018},
  month = aug,
  journal = {HTMLElement.style},
  pages = {1}
}

@inproceedings{ho_random_1995,
  title = {Random Decision Forests},
  booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
  author = {Ho, Tin Kam},
  year = {1995},
  volume = {1},
  pages = {278--282},
  publisher = {{IEEE}},
  isbn = {0-8186-7128-9}
}

@misc{hogeschool_utrecht_bijlage_2018,
  title = {Bijlage {{N}} Te Behalen Niveaus van de Professional Skills},
  author = {{Hogeschool Utrecht}},
  year = {2018},
  month = may,
  publisher = {{Hogeschool Utrecht}}
}

@misc{hogeschool_utrecht_bijlage_2018-1,
  title = {Bijlage {{M}} Te Behalen Niveaus van de {{ICT-beroepstaken}}},
  author = {{Hogeschool Utrecht}},
  year = {2018},
  month = may,
  publisher = {{Hogeschool Utrecht}}
}

@article{hossin_review_2015,
  title = {A {{Review}} on {{Evaluation Metrics}} for {{Data Classification Evaluations}}},
  author = {Hossin, Mohammad and M.N, Sulaiman},
  year = {2015},
  month = mar,
  journal = {International Journal of Data Mining \& Knowledge Management Process},
  volume = {5},
  pages = {01--11},
  doi = {10.5121/ijdkp.2015.5201},
  abstract = {Evaluation metric plays a critical role in achieving the optimal classifier during the classification training. Thus, a selection of suitable evaluation metric is an important key for discriminating and obtaining the optimal classifier. This paper systematically reviewed the related evaluation metrics that are specifically designed as a discriminator for optimizing generative classifier. Generally, many generative classifiers employ accuracy as a measure to discriminate the optimal solution during the classification training. However, the accuracy has several weaknesses which are less distinctiveness, less discriminability, less informativeness and bias to majority class data. This paper also briefly discusses other metrics that are specifically designed for discriminating the optimal solution. The shortcomings of these alternative metrics are also discussed. Finally, this paper suggests five important aspects that must be taken into consideration in constructing a new discriminator metric.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JCABKWBQ\\Hossin en M.N - 2015 - A Review on Evaluation Metrics for Data Classifica.pdf}
}

@article{hrusto_closing_2021,
  title = {Closing the {{Feedback Loop}} in {{DevOps Through Autonomous Monitors}} in {{Operations}}},
  author = {Hrusto, Adha and Runeson, Per and Engstr{\"o}m, Emelie},
  year = {2021},
  month = sep,
  journal = {SN Computer Science},
  volume = {2},
  number = {6},
  pages = {447},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00826-y},
  abstract = {DevOps represent the tight connection between development and operations. To address challenges that arise on the borderline between development and operations, we conducted a study in collaboration with a Swedish company responsible for ticket management and sales in public transportation. The aim of our study was to explore and describe the existing DevOps environment, as well as to identify how the feedback from operations can be improved, specifically with respect to the alerts sent from system operations. Our study complies with the basic principles of the design science paradigm, such as understanding and improving design solutions in the specific areas of practice. Our diagnosis, based on qualitative data collected through interviews and observations, shows that alert flooding is a challenge in the feedback loop, i.e. too much signals from operations create noise in the feedback loop. Therefore, we design a solution to improve the alert management by optimizing when to raise alerts and accordingly introducing a new element in the feedback loop, a smart filter. Moreover, we implemented a prototype of the proposed solution design and showed that a tighter relation between operations and development can be achieved, using a hybrid method which combines rule-based and unsupervised machine learning for operations data analysis.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\42Z64BQJ\\Hrusto e.a. - 2021 - Closing the Feedback Loop in DevOps Through Autono.pdf}
}

@misc{httperf_httperf_nodate,
  title = {{httperf}},
  author = {{httperf}},
  howpublished = {https://github.com/httperf/httperf},
  langid = {Engels}
}

@article{huang_effective_2013,
  title = {An Effective Hybrid Learning System for Telecommunication Churn Prediction},
  author = {Huang, Ying and Kechadi, Tahar},
  year = {2013},
  journal = {Expert Systems with Applications},
  volume = {40},
  number = {14},
  pages = {5635--5647},
  publisher = {{Elsevier}},
  issn = {0957-4174}
}

@inproceedings{hung_2004_nodate,
  title = {2004, {{Anthropometric}} Measurements from Photographic Images},
  booktitle = {Proceedings of the 7th {{International}} Conference on Work with {{Computer Systems}} ({{WWCS}}), 29 {{June}} \textendash{} 2 {{July}} 2004, {{Kuala Lumpur}}, {{Malaysia}}, 764 \textendash{} 769. {{3D}} Foot Shape Generation 641 {{LIU}}, {{H}}. 2003, {{Derivation}} Of},
  author = {Hung, Patrick Chi-yuen and Witana, Channa P. and Goonetilleke, Ravindra S.},
  pages = {1239},
  abstract = {Abstract. Traditionally, anthropometric measurements are taken on a person in a standard posture. With the proliferation of e-commerce, it has become necessary to gather anthropometric data more efficiently. This study proposes a means to obtain anthropometric measurements from photographic images. A case study with respect to online apparel retailing has been presented. The method can be extended for any part of the body for custom-made items such as shoes or helmets. The system was tested for ten measurements with 20 male subjects who were also manually measured. Two experimenters "measured " the 20 subjects using the computerized system and the results were then compared with the manual measures. The ANOVA showed significant differences (p {$<$} 0.05) between the two methods for three of the ten measures. The differences can be attributed to an inability to see some of the critical locations when digitizing and the mathematical formulations used to obtain circumferences. Reliability was assessed using conventional as well as the generalizable approaches and these showed that intra-tester reliability was higher than the inter-tester reliability. Further investigations may be required to verify the external validity of the findings.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RUJ4VKK5\\Hung e.a. - 2004, Anthropometric measurements from photographi.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\Y7TBTHQ6\\summary.html}
}

@incollection{hutchison_tutorial_2004,
  title = {A {{Tutorial}} on {{Uppaal}}},
  booktitle = {Formal {{Methods}} for the {{Design}} of {{Real-Time Systems}}},
  author = {Behrmann, Gerd and David, Alexandre and Larsen, Kim G.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Bernardo, Marco and Corradini, Flavio},
  year = {2004},
  volume = {3185},
  pages = {200--236},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30080-9_7},
  abstract = {This is a tutorial paper on the tool Uppaal. Its goal is to be a short introduction on the flavour of timed automata implemented in the tool, to present its interface, and to explain how to use the tool. The contribution of the paper is to provide reference examples and modelling patterns.},
  isbn = {978-3-540-23068-7 978-3-540-30080-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QGGHA8HC\\Behrmann e.a. - 2004 - A Tutorial on Uppaal.pdf}
}

@article{hwang_keystroke_2009,
  title = {Keystroke Dynamics-Based Authentication for Mobile Devices},
  author = {Hwang, Seong-seob and Cho, Sungzoon and Park, Sunghoon},
  year = {2009},
  month = feb,
  journal = {Computers \& Security},
  volume = {28},
  number = {1-2},
  pages = {85--93},
  issn = {01674048},
  doi = {10.1016/j.cose.2008.10.002},
  abstract = {Recently, mobile devices are used in financial applications such as banking and stock trading. However, unlike desktops and notebook computers, a 4-digit personal identification number (PIN) is often adopted as the only security mechanism for mobile devices. Because of their limited length, PINs are vulnerable to shoulder surfing and systematic trial-and-error attacks. This paper reports the effectiveness of user authentication using keystroke dynamics-based authentication (KDA) on mobile devices. We found that a KDA system can be effective for mobile devices in terms of authentication accuracy. Use of artificial rhythms leads to even better authentication performance.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LITHIMTP\\Hwang e.a. - 2009 - Keystroke dynamics-based authentication for mobile.pdf}
}

@book{hyndman_forecasting_2018,
  title = {Forecasting: Principles and Practice},
  author = {Hyndman, Rob J and Athanasopoulos, George},
  year = {2018},
  publisher = {{OTexts}},
  isbn = {0-9875071-1-7}
}

@misc{iso/iec_25010:2011_systems_2018,
  title = {Systems and Software Engineering -- {{Systems}} and Software {{Quality Requirements}} and {{Evaluation}} ({{SQuaRE}}) -- {{System}} and Software Quality Models},
  author = {{ISO/IEC 25010:2011}},
  year = {2018},
  month = jun,
  journal = {ISO/IEC 25010:2011}
}

@incollection{j._acosta-cano_loose_2013,
  title = {{Loose coupling modeling}},
  booktitle = {{Loose Coupling Based Reference Scheme for Shop Floor-Control System/Production-Equipment Integration}},
  author = {{J. Acosta-Cano} and {F. Sastr\'on-B\'aguena}},
  year = {2013},
  month = jun,
  series = {{volume 11}},
  pages = {451},
  publisher = {{Divisi\'on de Estudios de Posgrado Instituto Tecnol\'ogico de Chihuahua}},
  address = {{Chihuahua, M\'exico}},
  langid = {Engels}
}

@inproceedings{jabbar_methods_2014,
  title = {Methods to {{Avoid Over-Fitting}} and {{Under-Fitting}} in {{Supervised Machine Learning}} ({{Comparative Study}})},
  booktitle = {Computer {{Science}}, {{Communication}} and {{Instrumentation Devices}}},
  author = {Jabbar, Haider Khalaf and Khan, Rafiqul Zaman},
  year = {2014},
  pages = {165},
  publisher = {{Research Publishing Services}},
  doi = {10.3850/978-981-09-5247-1_017},
  abstract = {Machine learning is an important task for learning artificial neural networks, and we find in the learning one of the common problems of learning the Artificial Neural Network (ANN) is over-fitting and under-fitting to outlier points. In this paper we performed various methods in avoiding over-fitting and under-fitting; that is penalty and early stopping methods. A comparative study has been presented for the aforementioned methods to evaluate their performance within a range of specific parameters such as; speed of training, over-fitting and under-fitting avoidance, difficulty, capacity, time of training, and their accuracy. Besides these parameters we have included comparison between over-fitting and under-fitting. We found the early stopping method as being better as compared to the penalty method, as it can avoid overfitting and under-fitting with respect to validation time. Besides we find that Under-fitting neural networks perform poorly on both training and test sets, but Over-fitting networks may do very well on training sets though terribly on test sets.},
  isbn = {978-981-09-5247-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WKM2QFDY\\Jabbar en Khan - 2014 - Methods to Avoid Over-Fitting and Under-Fitting in.pdf}
}

@article{jacelon_analyzing_2005,
  title = {Analyzing {{Qualitative Data}}},
  author = {Jacelon, Cynthia S. and O'Dell, Katharine K.},
  year = {2005},
  month = jun,
  journal = {Urologic Nursing},
  volume = {25},
  number = {3},
  pages = {217--220},
  issn = {1053816X},
  abstract = {This article focuses on analyzing qualitative data in nursing research. In qualitative research, the researcher is the instrument of data analysis. The researcher's ability to interpret the data and to present the findings clearly makes a qualitative research study useful. Qualitative data, usually in the form of transcripts of interviews and field notes, pile up rather quickly. The transcript from one interview translates into 20 or 30 pages of single-spaced type. Computers are an integral part of the research process from proposal through reporting findings.},
  keywords = {DATA analysis,INTERVIEWING,MEDICAL research,NURSING research,QUALITATIVE research,RESEARCH},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7MM3A847\\Jacelon en O'Dell - 2005 - Analyzing Qualitative Data.pdf}
}

@inproceedings{jagannathan_privacy-preserving_2005,
  title = {Privacy-Preserving Distributed k-Means Clustering over Arbitrarily Partitioned Data},
  booktitle = {Proceeding of the Eleventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery in Data Mining  - {{KDD}} '05},
  author = {Jagannathan, Geetha and Wright, Rebecca N.},
  year = {2005},
  pages = {593},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1081870.1081942},
  abstract = {Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often obtain better results by pooling their data together. However, the collected data may contain sensitive or private information about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.},
  isbn = {978-1-59593-135-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\A64MLK72\\Jagannathan en Wright - 2005 - Privacy-preserving distributed k-means clustering .pdf}
}

@inproceedings{jagannathan_privacy-preserving_2005-1,
  title = {Privacy-{{Preserving Distributed}} k-{{Means Clustering}} over {{Arbitrarily Partitioned Data}}},
  booktitle = {Proceedings of the {{Eleventh ACM SIGKDD International Conference}} on {{Knowledge Discovery}} in {{Data Mining}}},
  author = {Jagannathan, Geetha and Wright, Rebecca N.},
  year = {2005},
  series = {{{KDD}} '05},
  pages = {593--599},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1081870.1081942},
  abstract = {Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often obtain better results by pooling their data together. However, the collected data may contain sensitive or private information about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.Distributed data mining is concerned with the computation of models from data that is distributed among multiple participants. Privacy-preserving distributed data mining seeks to allow for the cooperative computation of such models without the cooperating parties revealing any of their individual data items. Our paper makes two contributions in privacy-preserving data mining. First, we introduce the concept of arbitrarily partitioned data, which is a generalization of both horizontally and vertically partitioned data. Second, we provide an efficient privacy-preserving protocol for k-means clustering in the setting of arbitrarily partitioned data.},
  isbn = {1-59593-135-X}
}

@article{jaiswal_software_2019,
  title = {Software {{Architecture}} and {{Software Design}}},
  author = {Jaiswal, Manishaben},
  year = {2019},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3772387},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WL22XHQT\\Jaiswal - 2019 - Software Architecture and Software Design.pdf}
}

@article{jamshidi_microservices_2018,
  title = {Microservices: {{The Journey So Far}} and {{Challenges Ahead}}},
  shorttitle = {Microservices},
  author = {Jamshidi, Pooyan and Pahl, Claus and Mendon{\c c}a, Nabor C. and Lewis, James and Tilkov, Stefan},
  year = {2018},
  month = may,
  journal = {IEEE Software},
  volume = {35},
  number = {3},
  pages = {24--35},
  issn = {1937-4194},
  doi = {10.1109/MS.2018.2141039},
  abstract = {Microservices are an architectural approach emerging out of service-oriented architecture, emphasizing self-management and lightweightness as the means to improve software agility, scalability, and autonomy. This article examines microservice evolution from the technological and architectural perspectives and discusses key challenges facing future microservice developments.},
  keywords = {architectural antipatterns,DDD,domain-driven design,legacy systems,MDD,microservices,model-driven development,Service computing,service-oriented architecture,SOA,software development,Software development,software engineering,Special issues and sections},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JCIUTFZD\\Jamshidi e.a. - 2018 - Microservices The Journey So Far and Challenges A.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\UUVWMWKA\\8354433.html}
}

@article{jamshidi_microservices_2018-1,
  title = {Microservices: {{The Journey So Far}} and {{Challenges Ahead}}},
  shorttitle = {Microservices},
  author = {Jamshidi, Pooyan and Pahl, Claus and Mendon{\c c}a, Nabor C. and Lewis, James and Tilkov, Stefan},
  year = {2018},
  month = may,
  journal = {IEEE Software},
  volume = {35},
  number = {3},
  pages = {24--35},
  issn = {1937-4194},
  doi = {10.1109/MS.2018.2141039},
  abstract = {Microservices are an architectural approach emerging out of service-oriented architecture, emphasizing self-management and lightweightness as the means to improve software agility, scalability, and autonomy. This article examines microservice evolution from the technological and architectural perspectives and discusses key challenges facing future microservice developments.},
  keywords = {architectural antipatterns,DDD,domain-driven design,legacy systems,MDD,microservices,model-driven development,Service computing,service-oriented architecture,SOA,software development,Software development,software engineering,Special issues and sections},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\32Y7K3KT\\Jamshidi e.a. - 2018 - Microservices The Journey So Far and Challenges A.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\7ERC53BI\\8354433.html}
}

@inproceedings{japkowicz_class_2000,
  title = {The {{Class Imbalance Problem}}: {{Significance}} and {{Strategies}}},
  shorttitle = {The {{Class Imbalance Problem}}},
  booktitle = {In {{Proceedings}} of the 2000 {{International Conference}} on {{Artificial Intelligence}} ({{ICAI}}},
  author = {Japkowicz, Nathalie},
  year = {2000},
  pages = {111--117},
  abstract = {Although the majority of conceptlearning systems previously designed usually assume that their training sets are well-balanced, this assumption is not necessarily correct. Indeed, there exist many domains for which one class is represented by a large number of examples while the other is represented by only a few. The purpose of this paper is 1) to demonstrate experimentally that, at least in the case of connectionist systems, class imbalances hinder the performance of standard classifiers and 2) to compare the performance of several approaches previously proposed to deal with the problem.  1 Introduction  As the field of machine learning makes a rapid transition from the status of "academic discipline " to that of "applied science", a myriad of new issues, not previously considered by the machine learning community, is now coming into light. One such issue is the class imbalance problem. The class imbalance problem corresponds to domains for which one class is represented by a large n...}
}

@article{jimeno_yepes_feature_2015,
  title = {Feature Engineering for {{MEDLINE}} Citation Categorization with {{MeSH}}},
  author = {Jimeno Yepes, Antonio Jose and Plaza, Laura and {Carrillo-de-Albornoz}, Jorge and Mork, James G and Aronson, Alan R},
  year = {2015},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {16},
  number = {1},
  pages = {113},
  issn = {1471-2105},
  doi = {10.1186/s12859-015-0539-7},
  abstract = {Background: Research in biomedical text categorization has mostly used the bag-of-words representation. Other more sophisticated representations of text based on syntactic, semantic and argumentative properties have been less studied. In this paper, we evaluate the impact of different text representations of biomedical texts as features for reproducing the MeSH annotations of some of the most frequent MeSH headings. In addition to unigrams and bigrams, these features include noun phrases, citation meta-data, citation structure, and semantic annotation of the citations. Results: Traditional features like unigrams and bigrams exhibit strong performance compared to other feature sets. Little or no improvement is obtained when using meta-data or citation structure. Noun phrases are too sparse and thus have lower performance compared to more traditional features. Conceptual annotation of the texts by MetaMap shows similar performance compared to unigrams, but adding concepts from the UMLS taxonomy does not improve the performance of using only mapped concepts. The combination of all the features performs largely better than any individual feature set considered. In addition, this combination improves the performance of a state-of-the-art MeSH indexer. Concerning the machine learning algorithms, we find that those that are more resilient to class imbalance largely obtain better performance. Conclusions: We conclude that even though traditional features such as unigrams and bigrams have strong performance compared to other features, it is possible to combine them to effectively improve the performance of the bag-of-words representation. We have also found that the combination of the learning algorithm and feature sets has an influence in the overall performance of the system. Moreover, using learning algorithms resilient to class imbalance largely improves performance. However, when using a large set of features, consideration needs to be taken with algorithms due to the risk of over-fitting. Specific combinations of learning algorithms and features for individual MeSH headings could further increase the performance of an indexing system.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\G3CFFMCD\\Jimeno Yepes e.a. - 2015 - Feature engineering for MEDLINE citation categoriz.pdf}
}

@article{john_bristowe_what_2017,
  title = {{What is a Hybrid Mobile App?}},
  author = {{John Bristowe}},
  year = {2017},
  month = dec,
  pages = {1},
  langid = {Engels}
}

@article{josse_selecting_2012,
  title = {Selecting the Number of Components in Principal Component Analysis Using Cross-Validation Approximations},
  author = {Josse, Julie and Husson, Fran{\c c}ois},
  year = {2012},
  month = jun,
  journal = {Computational Statistics \& Data Analysis},
  volume = {56},
  number = {6},
  pages = {1869--1879},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2011.11.012},
  abstract = {Cross-validation is a tried and tested approach to select the number of components in principal component analysis (PCA), however, its main drawback is its computational cost. In a regression (or in a non parametric regression) setting, criteria such as the general cross-validation one (GCV) provide convenient approximations to leave-one-out cross-validation. They are based on the relation between the prediction error and the residual sum of squares weighted by elements of a projection matrix (or a smoothing matrix). Such a relation is then established in PCA using an original presentation of PCA with a unique projection matrix. It enables the definition of two cross-validation approximation criteria: the smoothing approximation of the cross-validation criterion (SACV) and the GCV criterion. The method is assessed with simulations and gives promising results.},
  keywords = {Cross-validation,Generalized cross-validation,Number of components,PCA,Smoothing matrix}
}

@misc{justin_ellingwood_how_2016,
  type = {{corporation}},
  title = {{How to serve flask applications with uWSGI and Nginx on Ubuntu 16.04}},
  author = {{Justin Ellingwood}},
  year = {2016},
  month = may,
  journal = {https://www.digitalocean.com},
  howpublished = {https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uwsgi-and-nginx-on-ubuntu-16-04},
  langid = {Engels}
}

@inproceedings{kalske_challenges_2017,
  title = {Challenges When Moving from Monolith to Microservice Architecture},
  booktitle = {International {{Conference}} on {{Web Engineering}}},
  author = {Kalske, Miika and M{\"a}kitalo, Niko and Mikkonen, Tommi},
  year = {2017},
  pages = {32--47},
  publisher = {{Springer}}
}

@article{kaminski_framework_2018,
  title = {A Framework for Sensitivity Analysis of Decision Trees},
  author = {Kami{\'n}ski, Bogumi{\l} and Jakubczyk, Micha{\l} and Szufel, Przemys{\l}aw},
  year = {2018},
  journal = {Central European Journal of Operations Research},
  volume = {26},
  number = {1},
  pages = {135--159},
  issn = {1435-246X},
  doi = {10.1007/s10100-017-0479-6},
  abstract = {In the paper, we consider sequential decision problems with uncertainty, represented as decision trees. Sensitivity analysis is always a crucial element of decision making and in decision trees it often focuses on probabilities. In the stochastic model considered, the user often has only limited information about the true values of probabilities. We develop a framework for performing sensitivity analysis of optimal strategies accounting for this distributional uncertainty. We design this robust optimization approach in an intuitive and not overly technical way, to make it simple to apply in daily managerial practice. The proposed framework allows for (1) analysis of the stability of the expected-value-maximizing strategy and (2) identification of strategies which are robust with respect to pessimistic/optimistic/mode-favoring perturbations of probabilities. We verify the properties of our approach in two cases: (a) probabilities in a tree are the primitives of the model and can be modified independently; (b) probabilities in a tree reflect some underlying, structural probabilities, and are interrelated. We provide a free software tool implementing the methods described.},
  pmcid = {PMC5767274},
  pmid = {29375266},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\IP9ZV3IL\\Kamiński e.a. - 2018 - A framework for sensitivity analysis of decision t.pdf}
}

@article{kashei_artificial_2019,
  title = {An Artificial Neural Network (p, d, q) Model for Timeseries Forecasting},
  author = {Kashei, Mehdi and Bijari, Mehdi},
  year = {2019},
  journal = {Expert Systems with Applications},
  volume = {37},
  number = {2010},
  pages = {480--489},
  abstract = {Artificial neural networks (ANNs) are flexible computing frameworks and universal approximators that can be applied to a wide range of time series forecasting problems with a high degree of accuracy. However, despite all advantages cited for artificial neural networks, their performance for some real time series is not satisfactory. Improving forecasting especially time series forecasting accuracy is an important yet often difficult task facing forecasters. Both theoretical and empirical findings have indicated that integration of different models can be an effective way of improving upon their predictive performance, especially when the models in the ensemble are quite different. In this paper, a novel hybrid model of artificial neural networks is proposed using auto-regressive integrated moving average (ARIMA) models in order to yield a more accurate forecasting model than artificial neural networks. The empirical results with three well-known real data sets indicate that the proposed model can be an effective way to improve forecasting accuracy achieved by artificial neural networks. Therefore, it can be used as an appropriate alternative model for forecasting task, especially when higher forecasting accuracy is needed.}
}

@article{kavakiotis_machine_2017,
  title = {Machine {{Learning}} and {{Data Mining Methods}} in {{Diabetes Research}}},
  author = {Kavakiotis, Ioannis and Tsave, Olga and Salifoglou, Athanasios and Maglaveras, Nicos and Vlahavas, Ioannis and Chouvarda, Ioanna},
  year = {2017},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {15},
  pages = {104--116},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2016.12.005},
  abstract = {The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85\% of those used were characterized by supervised learning approaches and 15\% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM.},
  langid = {english},
  keywords = {Biomarker(s) identification,Data mining,Diabetes mellitus,Diabetic complications,Disease prediction models,Machine learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XAE5Z3Q6\\Kavakiotis e.a. - 2017 - Machine Learning and Data Mining Methods in Diabet.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\Y2MZIG6H\\S2001037016300733.html}
}

@inproceedings{kazanavicius_migrating_2019,
  title = {Migrating Legacy Software to Microservices Architecture},
  booktitle = {2019 {{Open Conference}} of {{Electrical}}, {{Electronic}} and {{Information Sciences}} ({{eStream}})},
  author = {Kazanavi{\v c}ius, Justas and Ma{\v z}eika, Dalius},
  year = {2019},
  pages = {1--5},
  publisher = {{IEEE}},
  isbn = {1-72812-499-9}
}

@incollection{khanna_r._yusuf_s._&_h._phan_introducing_2017,
  title = {Introducing a {{Hybrid Mobile Application}}},
  booktitle = {Ionic: {{Hybrid}} Mobile App Development},
  author = {{Khanna, R. Yusuf, S. \& H. Phan}},
  year = {2017},
  month = apr,
  publisher = {{Packt Publishing}}
}

@misc{killourhy_comparing_nodate,
  title = {Comparing {{Anomaly-Detection Algorithms}} for {{Keystroke Dynamics}}},
  author = {Killourhy, Kevin and Maxion, Roy},
  abstract = {Store the projects. Contribute to Neurocast/data-science-project development by creating an account on GitHub.},
  howpublished = {https://www.cs.cmu.edu/\textasciitilde maxion/pubs/KillourhyMaxion09.pdf},
  langid = {english}
}

@book{kimball_data_2011,
  title = {The {{Data Warehouse Toolkit}}: {{The Complete Guide}} to {{Dimensional Modeling}}},
  shorttitle = {The {{Data Warehouse Toolkit}}},
  author = {Kimball, Ralph and Ross, Margy},
  year = {2011},
  month = aug,
  publisher = {{John Wiley \& Sons}},
  abstract = {The latest edition of the single most authoritative guide on dimensional modeling for data warehousing! Dimensional modeling has become the most widely accepted approach for data warehouse design. Here is a complete library of dimensional modeling techniques-- the most comprehensive collection ever written. Greatly expanded to cover both basic and advanced techniques for optimizing data warehouse design, this second edition to Ralph Kimball's classic guide is more than sixty percent updated. The authors begin with fundamental design recommendations and gradually progress step-by-step through increasingly complex scenarios. Clear-cut guidelines for designing dimensional models are illustrated using real-world data warehouse case studies drawn from a variety of business application areas and industries, including: * Retail sales and e-commerce * Inventory management * Procurement * Order management * Customer relationship management (CRM) * Human resources management * Accounting * Financial services * Telecommunications and utilities * Education * Transportation * Health care and insurance By the end of the book, you will have mastered the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query response. You will also learn how to create an architected framework that integrates the distributed data warehouse using standardized dimensions and facts. This book is also available as part of the Kimball's Data Warehouse Toolkit Classics Box Set (ISBN: 9780470479575) with the following 3 books: The Data Warehouse Toolkit, 2nd Edition (9780471200246) The Data Warehouse Lifecycle Toolkit, 2nd Edition (9780470149775) The Data Warehouse ETL Toolkit (9780764567575)},
  googlebooks = {XoS2oy1IcB4C},
  isbn = {978-1-118-08214-0},
  langid = {english},
  keywords = {Computers / Databases / Data Warehousing,Computers / Information Technology}
}

@misc{kixs/jivc_-_defensie_tomla_2018,
  title = {{TOMLA manual}},
  author = {{KIXS/JIVC - defensie}},
  year = {2018},
  month = jul,
  publisher = {{Defensie}},
  langid = {Engels}
}

@article{kizilkaya_arma_2006,
  title = {{{ARMA}} Model Parameter Estimation Based on the Equivalent {{MA}} Approach},
  author = {Kizilkaya, Aydin and Kayran, Ahmet H.},
  year = {2006},
  month = nov,
  journal = {Digital Signal Processing},
  volume = {16},
  number = {6},
  pages = {670--681},
  issn = {10512004},
  doi = {10.1016/j.dsp.2006.08.010},
  abstract = {The paper investigates the relation between the parameters of an autoregressive moving average (ARMA) model and its equivalent moving average (EMA) model. On the basis of this relation, a new method is proposed for determining the ARMA model parameters from the coefficients of a finite-order EMA model. This method is a three-step approach: in the first step, a simple recursion relating the EMA model parameters and the cepstral coefficients of an ARMA process is derived to estimate the EMA model parameters; in the second step, the AR parameters are estimated by solving the linear equation set composed of EMA parameters; then, the MA parameters are obtained via simple computations using the estimated EMA and AR parameters. Simulations including both low- and high-order ARMA processes are given to demonstrate the performance of the new method. The end results are compared with the existing method in the literature over some performance criteria. It is observed from the simulations that our new algorithm produces the satisfactory and acceptable results.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\EFJJDIVY\\Kizilkaya en Kayran - 2006 - ARMA model parameter estimation based on the equiv.pdf}
}

@article{kohavi_study_nodate,
  title = {A {{Study}} of {{CrossValidation}} and {{Bootstrap}} for {{Accuracy Estimation}} and {{Model Selection}}},
  author = {Kohavi, Ron},
  pages = {7},
  abstract = {We review accuracy estimation methods and compare the two most common methods: crossvalidation and bootstrap. Recent experimental results on arti cial data and theoretical results in restricted settings have shown that for selecting a good classi er from a set of classiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the e ects of di erent parameters on these algorithms on real-world datasets. For crossvalidation, we vary the number of folds and whether the folds are strati ed or not for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold strati ed cross validation, even if computation power allows using more folds.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JVQPSZPU\\Kohavi - foAr AStcucduyraocfyCErsotsims-VatailoidnaatinodnM.pdf}
}

@misc{koninkrijksrelaties_auteurswet_nodate,
  type = {{wet}},
  title = {{Auteurswet}},
  author = {en Koninkrijksrelaties, Ministerie van Binnenlandse Zaken},
  abstract = {Artikel 2 Auteurswet},
  howpublished = {http://wetten.overheid.nl/jci1.3:c:BWBR0001886\&hoofdstuk=I\&paragraaf=1\&artikel=2\&z=2018-10-11\&g=2018-10-11},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PRXXV5A4\\2018-10-11.html}
}

@inproceedings{koschel_making_2017,
  title = {Making the Move to Microservice Architecture},
  booktitle = {2017 {{International Conference}} on {{Information Society}} (i-{{Society}})},
  author = {Koschel, Arne and Astrova, Irina and D{\"o}tterl, Jeremias},
  year = {2017},
  pages = {74--79},
  publisher = {{IEEE}},
  isbn = {1-908320-80-X}
}

@article{kotsiantis_data_2007,
  title = {Data {{Preprocessing}} for {{Supervised Leaning}}},
  author = {Kotsiantis, S B and Kanellopoulos, D and Pintelas, P E},
  year = {2007},
  volume = {1},
  number = {12},
  pages = {6},
  abstract = {Many factors affect the success of Machine Learning (ML) on a given task. The representation and quality of the instance data is first and foremost. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. It is well known that data preparation and filtering steps take considerable amount of processing time in ML problems. Data pre-processing includes data cleaning, normalization, transformation, feature extraction and selection, etc. The product of data pre-processing is the final training set. It would be nice if a single sequence of data pre-processing algorithms had the best performance for each data set but this is not happened. Thus, we present the most well know algorithms for each step of data pre-processing so that one achieves the best performance for their data set.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\X3CHCX5N\\Kotsiantis e.a. - 2007 - Data Preprocessing for Supervised Leaning.pdf}
}

@article{kovacevic_application_2019,
  title = {Application of the Matrix Approach in Risk Assessment},
  author = {Kova{\v c}evi{\'c}, Nenad and Stojiljkovi{\'c}, Aleksandra and Kova{\v c}, Mitar and "Educons" University, Belgrade, Faculty for Project {and} Innovation Management and {University of Defense}, Belgrade, Military Aacademy and {University of Novi Sad}, Subotica, Faculty of Economics},
  year = {2019},
  journal = {Operational Research in Engineering Sciences: Theory and Applications},
  volume = {2},
  number = {3},
  pages = {55--64},
  publisher = {{Regional Association for Security and crisis management, Belgrade, Serbia}},
  issn = {2620-1607},
  doi = {10.31181/oresta1903055k},
  abstract = {The risk assessment process is based on risk management.~Risk assessment is, in principle, an entirely empirical decision-making process, based on risk assessors' knowledge and experience, necessary to identify (a) hazard(s) as the cause for risk by using specific and well-known and recognized methods so far.~Currently, there are a large number of methods recognized for risk assessment, which are mostly formed by various organizations and associations of engineers, usually in insurance companies.~The paper presents the most pragmatic matrix (qualitative) risk assessment methods, such as: a 3x3 matrix (OHSAS), a 4x4 matrix (AS/NZS 4360) and a 5x5 matrix (MIL-STD-882B). The paper is significant in that the matrix approach in risk assessment is the basis for the development of risk assessment methods, regardless of the method of the group which they belong to.},
  langid = {english},
  keywords = {decision-making,matrix approach,risk assessment}
}

@article{kovacevic_application_2019-1,
  title = {Application of the Matrix Approach in Risk Assessment},
  author = {Kova{\v c}evi{\'c}, Nenad and Stojiljkovi{\'c}, Aleksandra and Kova{\v c}, Mitar and "Educons" University, Belgrade, Faculty for Project {and} Innovation Management and {University of Defense}, Belgrade, Military Aacademy and {University of Novi Sad}, Subotica, Faculty of Economics},
  year = {2019},
  journal = {Operational Research in Engineering Sciences: Theory and Applications},
  volume = {2},
  number = {3},
  pages = {55--64},
  publisher = {{Regional Association for Security and crisis management, Belgrade, Serbia}},
  issn = {2620-1607},
  doi = {10.31181/oresta1903055k},
  abstract = {The risk assessment process is based on risk management.~Risk assessment is, in principle, an entirely empirical decision-making process, based on risk assessors' knowledge and experience, necessary to identify (a) hazard(s) as the cause for risk by using specific and well-known and recognized methods so far.~Currently, there are a large number of methods recognized for risk assessment, which are mostly formed by various organizations and associations of engineers, usually in insurance companies.~The paper presents the most pragmatic matrix (qualitative) risk assessment methods, such as: a 3x3 matrix (OHSAS), a 4x4 matrix (AS/NZS 4360) and a 5x5 matrix (MIL-STD-882B). The paper is significant in that the matrix approach in risk assessment is the basis for the development of risk assessment methods, regardless of the method of the group which they belong to.},
  langid = {english},
  keywords = {decision-making,matrix approach,risk assessment}
}

@article{kowolenko_big_2018,
  title = {Big Data: Developing an Open Source "Big Data" Cognitive Computing Platform},
  shorttitle = {Big Data},
  author = {Kowolenko, Michael and Vouk, Mladen A.},
  year = {2018},
  month = mar,
  journal = {Ubiquity},
  volume = {2018},
  number = {March},
  pages = {1--15},
  issn = {15302180},
  doi = {10.1145/3158344},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4UXDIJKT\\Kowolenko en Vouk - 2018 - Big data developing an open source big data cog.pdf}
}

@article{krinninger_one-shot_nodate,
  title = {One-{{Shot 3D Body-Measurement}}},
  author = {Krinninger, Thomas},
  pages = {74},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PFZ8FJG6\\Krinninger - One-Shot 3D Body-Measurement.pdf}
}

@article{kruchten_focusguest_2006,
  title = {Focusguest Editors' Introduction},
  author = {Kruchten, Philippe and Obbink, Henk and Stafford, Judith},
  year = {2006},
  journal = {IEEE SOFTWARE},
  pages = {9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\U386GWAK\\Kruchten e.a. - 2006 - focusguest editors’ introduction.pdf}
}

@misc{kumar_machine_2019,
  title = {Machine {{Learning Quick Reference}} : {{Quick}} and {{Essential Machine Learning Hacks}} for {{Training Smart Data Models}}},
  author = {Kumar, Rahul},
  year = {2019},
  howpublished = {http://web.b.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzIwMTg5NzVfX0FO0?sid=d5518d51-fc37-428d-b8e0-d3d62aa63b53@pdc-v-sessmgr04\&vid=0\&format=EB\&lpid=lp\_10\&rid=0},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NM224AZH\\bmxlYmtfXzIwMTg5NzVfX0FO0.html}
}

@incollection{kutbay_partitional_2018,
  title = {Partitional {{Clustering}}},
  author = {Kutbay, U{\u g}urhan},
  year = {2018},
  month = aug,
  doi = {10.5772/intechopen.75836},
  isbn = {978-1-78923-526-5}
}

@inproceedings{kwatra_k-anonymised_2022,
  title = {A K-{{Anonymised Federated Learning Framework}} with {{Decision Trees}}},
  booktitle = {Data {{Privacy Management}}, {{Cryptocurrencies}} and {{Blockchain Technology}}},
  author = {Kwatra, Saloni and Torra, Vicen{\c c}},
  editor = {{Garcia-Alfaro}, Joaquin and {Mu{\~n}oz-Tapia}, Jose Luis and {Navarro-Arribas}, Guillermo and Soriano, Miguel},
  year = {2022},
  pages = {106--120},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {We propose a privacy-preserving framework using Mondrian k-anonymity with decision trees in a Federated Learning (FL) setting for the horizontally partitioned data. Data heterogeneity in FL makes the data non-IID (Non-Independent and Identically Distributed). We use a novel approach to create non-IID partitions of data by solving an optimization problem. In this work, each device trains a decision tree classifier. Devices share the root node of their trees with the aggregator. The aggregator merges the trees by choosing the most common split attribute and grows the branches based on the split values of the chosen split attribute. This recursive process stops when all the nodes to be merged are leaf nodes. After the merging operation, the aggregator sends the merged decision tree to the distributed devices. Therefore, we aim to build a joint machine learning model based on the data from multiple devices while offering k-anonymity to the participants.},
  isbn = {978-3-030-93944-1}
}

@misc{lange_ntru_nodate,
  title = {{{NTRU Prime}}: Reducing Attack Surface at Low Cost},
  shorttitle = {{{NTRU Prime}}},
  author = {Lange, Chitchanok Chuengsatiansup, Tanja, Daniel J. Bernstein and van Vredendaal, Christine},
  abstract = {Several ideal-lattice-based cryptosystems have been broken by recent attacks that exploit special structures of the rings used in those cryptosystems. The same structures are also used in the leading proposals for post-quantum lattice-based cryptography, including the classic NTRU cryptosystem and typical Ring-LWE-based cryptosystems. This paper (1) proposes NTRU Prime, which tweaks NTRU to use rings without these structures; (2) proposes Streamlined NTRU Prime, a public-key cryptosystem optimized from an implementation perspective, subject to the standard design goal of IND-CCA2 security; (3) finds high-security post-quantum parameters for Streamlined NTRU Prime; and (4) optimizes a constant-time implementation of those parameters. The resulting sizes and speeds show that reducing the attack surface has very low cost.},
  keywords = {fast sorting,ideal lattices,Karatsuba,lattice-based cryptography,NTRU,post-quantum cryptography,public-key encryption,Ring-LWE,security,software implementation,Soliloquy,vectorization},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\AWY8SFS2\\Lange en Vredendaal - NTRU Prime reducing attack surface at low cost.pdf}
}

@article{langfelder_defining_2008,
  title = {Defining Clusters from a Hierarchical Cluster Tree: The {{Dynamic Tree Cut}} Package for {{R}}},
  shorttitle = {Defining Clusters from a Hierarchical Cluster Tree},
  author = {Langfelder, Peter and Zhang, Bin and Horvath, Steve},
  year = {2008},
  month = mar,
  journal = {Bioinformatics},
  volume = {24},
  number = {5},
  pages = {719--720},
  publisher = {{Oxford Academic}},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btm563},
  abstract = {Abstract.  Summary: Hierarchical clustering is a widely used method for detecting clusters in genomic data. Clusters are defined by cutting branches off the den},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\DV8DC2UW\\Langfelder e.a. - 2008 - Defining clusters from a hierarchical cluster tree.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\8BGRREGJ\\200751.html}
}

@article{laskey_service_2009,
  title = {Service Oriented Architecture},
  author = {Laskey, Kathryn B. and Laskey, Kenneth},
  year = {2009},
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume = {1},
  number = {1},
  pages = {101--105},
  publisher = {{Wiley Online Library}},
  isbn = {1939-5108}
}

@article{laskey_service_2009-1,
  title = {Service Oriented Architecture},
  author = {Laskey, Kathryn B and Laskey, Kenneth},
  year = {2009},
  journal = {Vo lu me},
  pages = {5},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UU7UBHNK\\Laskey en Laskey - 2009 - Service oriented architecture.pdf}
}

@article{ledesma_scree_2015,
  title = {The {{Scree Test}} and the {{Number}} of {{Factors}}: A {{Dynamic Graphics Approach}}},
  shorttitle = {The {{Scree Test}} and the {{Number}} of {{Factors}}},
  author = {Ledesma, Rub{\'e}n and {Valero-Mora}, Pedro and Macbeth, Guillermo},
  year = {2015},
  month = jun,
  journal = {The Spanish Journal of Psychology},
  volume = {18},
  doi = {10.1017/sjp.2015.13},
  abstract = {Exploratory Factor Analysis and Principal Component Analysis are two data analysis methods that are commonly used in psychological research. When applying these techniques, it is important to determine how many factors to retain. This decision is sometimes based on a visual inspection of the Scree plot. However, the Scree plot may at times be ambiguous and open to interpretation. This paper aims to explore a number of graphical and computational improvements to the Scree plot in order to make it more valid and informative. These enhancements are based on dynamic and interactive data visualization tools, and range from adding Parallel Analysis results to "linking" the Scree plot with other graphics, such as factor-loadings plots. To illustrate our proposed improvements, we introduce and describe an example based on real data on which a principal component analysis is appropriate. We hope to provide better graphical tools to help researchers determine the number of factors to retain.}
}

@article{leevy_survey_2018,
  title = {A Survey on Addressing High-Class Imbalance in Big Data},
  author = {Leevy, Joffrey L. and Khoshgoftaar, Taghi M. and Bauder, Richard A. and Seliya, Naeem},
  year = {2018},
  month = nov,
  journal = {Journal of Big Data},
  volume = {5},
  number = {1},
  pages = {42},
  issn = {2196-1115},
  doi = {10.1186/s40537-018-0151-6},
  abstract = {In a majority\textendash minority classification problem, class imbalance in the dataset(s) can dramatically skew the performance of classifiers, introducing a prediction bias for the majority class. Assuming the positive (minority) class is the group of interest and the given application domain dictates that a false negative is much costlier than a false positive, a negative (majority) class prediction bias could have adverse consequences. With big data, the mitigation of class imbalance poses an even greater challenge because of the varied and complex structure of the relatively much larger datasets. This paper provides a large survey of published studies within the last 8~years, focusing on high-class imbalance (i.e., a majority-to-minority class ratio between 100:1 and 10,000:1) in big data in order to assess the state-of-the-art in addressing adverse effects due to class imbalance. In this paper, two techniques are covered which include Data-Level (e.g., data sampling) and Algorithm-Level (e.g., cost-sensitive and hybrid/ensemble) Methods. Data sampling methods are popular in addressing class imbalance, with Random Over-Sampling methods generally showing better overall results. At the Algorithm-Level, there are some outstanding performers. Yet, in the published studies, there are inconsistent and conflicting results, coupled with a limited scope in evaluated techniques, indicating the need for more comprehensive, comparative studies.}
}

@inproceedings{lefyer_method_2017,
  title = {Method of Person Identification Based on Biometric Characteristics of Touch Screen Gestures},
  booktitle = {2017 20th {{Conference}} of {{Open Innovations Association}} ({{FRUCT}})},
  author = {Lefyer, Kirill and Spivak, Anton},
  year = {2017},
  month = apr,
  pages = {222--227},
  publisher = {{IEEE}},
  address = {{St-Petersburg, Russia}},
  doi = {10.23919/FRUCT.2017.8071315},
  abstract = {Vast majority of modern smartphones is equipped with touch-sensitive screens. Being precise and accurate input devices, those screens can actually provide a lot of data from user input gestures to analyze. Having such a data, it is possible to find unique characteristics of user's input to identify smartpohe's user just by analyzing their input gestures. This article presents a study on possible approaches to identification a person by parameters of touch gestures he or she inputs on the screen of smartphone.},
  isbn = {978-952-68653-0-0},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NM5BHN73\\Lefyer en Spivak - 2017 - Method of person identification based on biometric.pdf}
}

@article{leite_survey_2020,
  title = {A {{Survey}} of {{DevOps Concepts}} and {{Challenges}}},
  author = {Leite, Leonardo and Rocha, Carla and Kon, Fabio and Milojicic, Dejan and Meirelles, Paulo},
  year = {2020},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {6},
  eprint = {1909.05409},
  eprinttype = {arxiv},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3359981},
  abstract = {DevOps is a collaborative and multidisciplinary organizational effort to automate continuous delivery of new software updates while guaranteeing their correctness and reliability. The present survey investigates and discusses DevOps challenges from the perspective of engineers, managers, and researchers. We review the literature and develop a DevOps conceptual map, correlating the DevOps automation tools with these concepts. We then discuss their practical implications for engineers, managers, and researchers. Finally, we critically explore some of the most relevant DevOps challenges reported by the literature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering,D.2},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XIHAP3K7\\Leite e.a. - 2020 - A Survey of DevOps Concepts and Challenges.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\EFP46AKF\\1909.html}
}

@incollection{leo_choice_nodate,
  title = {{Choice of the best imputation method}},
  booktitle = {{Secondary Analysis of Electronic Health Records}},
  author = {Leo, Anthony Celi and Charlton, Peter and Mahdi Ghassemi, Mohammed and Johnson, Alistair and Komorowski, Matthieu and Marshall, Dominic and Neummann, Tristan and Paik, Kenneth and Pollard Joseph, Tom and Raffa, Jesse and Salciccioli, Justin},
  pages = {152},
  isbn = {978-3-319-43742-2},
  langid = {Engels}
}

@incollection{leo_noise_nodate,
  title = {{Noise versus Outliers}},
  booktitle = {{Secondary Analysis of Electronic Health Records}},
  author = {Leo, Anthony Celi and Charlton, Peter and Mahdi Ghassemi, Mohammed and Johnson, Alistair and Komorowski, Matthieu and Marshall, Dominic and Neummann, Tristan and Paik, Kenneth and Pollard Joseph, Tom and Raffa, Jesse and Salciccioli, Justin},
  pages = {166--167},
  isbn = {978-3-319-43742-2},
  langid = {Engels}
}

@incollection{leo_types_nodate,
  title = {{Types of missingness}},
  booktitle = {{Secondary Analysis of Electronic Health Records}},
  author = {Leo, Anthony Celi and Charlton, Peter and Mahdi Ghassemi, Mohammed and Johnson, Alistair and Komorowski, Matthieu and Marshall, Dominic and Neummann, Tristan and Paik, Kenneth and Pollard Joseph, Tom and Raffa, Jesse and Salciccioli, Justin},
  pages = {145},
  isbn = {978-3-319-43742-2},
  langid = {Engels}
}

@article{li_feature_2017,
  title = {Feature {{Selection}}: {{A Data Perspective}}},
  shorttitle = {Feature {{Selection}}},
  author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
  year = {2017},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {6},
  pages = {1--45},
  issn = {03600300},
  doi = {10.1145/3136625},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GZ8YS3YQ\\Li e.a. - 2017 - Feature Selection A Data Perspective.pdf}
}

@article{li_quantum_2021,
  title = {Quantum Computing and the Security of Public Key Cryptography},
  author = {Li, Kai and Yan, Pei-Gen and Cai, Qing-Yu},
  year = {2021},
  journal = {Fundamental Research},
  volume = {1},
  number = {1},
  pages = {85--87},
  publisher = {{Elsevier}},
  issn = {2667-3258}
}

@article{li_towards_2017,
  title = {Towards a Full-Stack Devops Environment (Platform-as-a-Service) for Cloud-Hosted Applications},
  author = {Li, Zhenhua and Zhang, Yun and Liu, Yunhao},
  year = {2017},
  month = feb,
  journal = {Tsinghua Science and Technology},
  volume = {22},
  number = {01},
  pages = {1--9},
  issn = {1007-0214},
  doi = {10.1109/TST.2017.7830891},
  abstract = {If every programmer of cloud-hosted apps possessed exceptional technical capability and endless patience, the DevOps environment (also known as Platform-as-a-Service, or PaaS) would perhaps become irrelevant. However, the reality is almost always the opposite case. Hence, IT engineers dream of a reliable and usable DevOps environment that can substantially facilitate their developments and simplify their operations. Current DevOps environments include Google App Engine, Docker, Kubernetes, Mesos, and so forth. In other words, PaaS bridges the gap between vivid IT engineers and stiff cloud systems. In this paper, we comprehensively examine state-of-the-art PaaS solutions across various tiers of the cloud-computing DevOps stack. On this basis, we identify areas of consensus and diversity in their philosophies and methodologies. In addition, we explore cutting-edge solutions towards realizing a more fine-grained, full-stack DevOps environment. From this paper, readers are expected to quickly grasp the essence, current status, and future prospects of PaaS.},
  keywords = {cloud computing,Cloud computing,Containers,development,DevOps,environment,Google,operation,Platform-as-a-Service (PaaS),Resource management,Servers,Software as a service,Yarn},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RE6JQ5XX\\Li e.a. - 2017 - Towards a full-stack devops environment (platform-.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\X6KT7GSW\\7830891.html}
}

@article{lindsay_surviving_nodate,
  title = {Surviving the {{Quantum Cryptocalypse}}},
  author = {Lindsay, Jon R},
  pages = {25},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\DDR8JGBU\\Lindsay - Surviving the Quantum Cryptocalypse.pdf}
}

@article{liou_autoencoder_2014,
  title = {Autoencoder for Words},
  author = {Liou, Cheng-Yuan and Cheng, Wei-Chen and Liou, Jiun-Wei and Liou, Daw-Ran},
  year = {2014},
  month = sep,
  journal = {Neurocomputing},
  volume = {139},
  pages = {84--96},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2013.09.055},
  abstract = {This paper presents a training method that encodes each word into a different vector in semantic space and its relation to low entropy coding. Elman network is employed in the method to process word sequences from literary works. The trained codes possess reduced entropy and are used in ranking, indexing, and categorizing literary works. A modification of the method to train the multi-vector for each polysemous word is also presented where each vector represents a different meaning of its word. These multiple vectors can accommodate several different meanings of their word. This method is applied to the stylish analyses of two Chinese novels, Dream of the Red Chamber and Romance of the Three Kingdoms.},
  langid = {english},
  keywords = {Elman network,Minimum entropy coding,Polysemous word,Semantic indexing,Writing style similarity},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LSN5RSBG\\Liou e.a. - 2014 - Autoencoder for words.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\T8TXTUAI\\S0925231214003658.html}
}

@inproceedings{liu_isolation_2008,
  title = {Isolation {{Forest}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  year = {2008},
  month = dec,
  pages = {413--422},
  publisher = {{IEEE}},
  address = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.17},
  abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and Random Forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
  isbn = {978-0-7695-3502-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JV89ECQJ\\Liu e.a. - 2008 - Isolation Forest.pdf}
}

@article{loshin_relational_2001,
  title = {Relational {{Database}}},
  author = {Loshin, Pete},
  year = {2001},
  month = jan,
  journal = {Computerworld},
  volume = {35},
  number = {2},
  pages = {60},
  issn = {00104841},
  abstract = {Provides information on relational databases, a storage technology that allow data to be stored in multiple flat-file tables that are related to one another by shared data fields called keys.  Description of a database; Problems of databases; Creation of the relational database model by researcher E.F. Codd.},
  keywords = {COMPUTER storage devices,INFORMATION retrieval,RELATIONAL databases}
}

@article{lwakatare_devops_2019,
  title = {{{DevOps}} in Practice: {{A}} Multiple Case Study of Five Companies},
  author = {Lwakatare, Lucy Ellen and Kilamo, Terhi and Karvonen, Teemu and Sauvola, Tanja and Heikkil{\"a}, Ville and Itkonen, Juha and Kuvaja, Pasi and Mikkonen, Tommi and Oivo, Markku and Lassenius, Casper},
  year = {2019},
  journal = {Information and software technology},
  volume = {114},
  number = {Journal Article},
  pages = {217--230},
  publisher = {{Elsevier B.V}},
  address = {{AMSTERDAM}},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2019.06.010},
  abstract = {Context: DevOps is considered important in the ability to frequently and reliably update a system in operational state. DevOps presumes cross-functional collaboration and automation between software development and operations. DevOps adoption and implementation in companies is non-trivial due to required changes in technical, organisational and cultural aspects. Objectives: This exploratory study presents detailed descriptions of how DevOps is implemented in practice. The context of our empirical investigation is web application and service development in small and medium sized companies. Method: A multiple-case study was conducted in five different development contexts with successful DevOps implementations since its benefits, such as quick releases and minimum deployment errors, were achieved. Data was mainly collected through interviews with 26 practitioners and observations made at the companies. Data was analysed by first coding each case individually using a set of predefined themes and thereafter perform a cross-case synthesis. Results: Our analysis yielded some of the following results: (i) software development team attaining ownership and responsibility to deploy software changes in production is crucial in DevOps. (ii) toolchain usage and support in deployment pipeline activities accelerates the delivery of software changes, bug fixes and handling of production incidents. (ii) the delivery speed to production is affected by context factors, such as manual approvals by the product owner (iii) steep learning curve for new skills is experienced by both software developers and operations staff, who also have to cope with working under pressure. Conclusion: Our findings contributes to the overall understanding of DevOps concept, practices and its perceived impacts, particularly in small and medium sized companies. We discuss two practical implications of the results.;Keywords DevOps; Continuous deployment; Agile; Operations; Development Context: DevOps is considered important in the ability to frequently and reliably update a system in operational state. DevOps presumes cross-functional collaboration and automation between software development and operations. DevOps adoption and implementation in companies is non-trivial due to required changes in technical, organisational and cultural aspects. Objectives: This exploratory study presents detailed descriptions of how DevOps is implemented in practice. The context of our empirical investigation is web application and service development in small and medium sized companies. Method: A multiple-case study was conducted in five different development contexts with successful DevOps implementations since its benefits, such as quick releases and minimum deployment errors, were achieved. Data was mainly collected through interviews with 26 practitioners and observations made at the companies. Data was analysed by first coding each case individually using a set of predefined themes and thereafter perform a cross-case synthesis. Results: Our analysis yielded some of the following results: (i) software development team attaining ownership and responsibility to deploy software changes in production is crucial in DevOps. (ii) toolchain usage and support in deployment pipeline activities accelerates the delivery of software changes, bug fixes and handling of production incidents. (ii) the delivery speed to production is affected by context factors, such as manual approvals by the product owner (iii) steep learning curve for new skills is experienced by both software developers and operations staff, who also have to cope with working under pressure. Conclusion: Our findings contributes to the overall understanding of DevOps concept, practices and its perceived impacts, particularly in small and medium sized companies. We discuss two practical implications of the results. Author Affiliation: (a) M3S, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, Finland (b) Faculty of Information Technology and Communication Sciences, Tampere University, Tampere, Finland (c) Department of Computer Science, Aalto University, Helsinki, Finland (d) Department of Computer Science, University of Helsinki, Helsinki, Finland * Corresponding author. Article History: Received 31 March 2017; Revised 5 April 2019; Accepted 23 June 2019 Byline: Lucy Ellen Lwakatare [lucylwakatare@yahoo.com] (*,a), Terhi Kilamo (b), Teemu Karvonen (a), Tanja Sauvola (a), Ville Heikkila (c), Juha Itkonen (c), Pasi Kuvaja (a), Tommi Mikkonen (d), Markku Oivo (a), Casper Lassenius (c);},
  langid = {english},
  keywords = {Agile,Career development,Case studies,Computer Science,Computer Science; Information Systems,Computer Science; Software Engineering,Computer software industry,Continuous deployment,Development,DevOps,Electrical engineering,Operations,Science \& Technology,Technology}
}

@article{lwakatare_devops_2019-1,
  title = {{{DevOps}} in Practice: {{A}} Multiple Case Study of Five Companies},
  shorttitle = {{{DevOps}} in Practice},
  author = {Lwakatare, Lucy Ellen and Kilamo, Terhi and Karvonen, Teemu and Sauvola, Tanja and Heikkil{\"a}, Ville and Itkonen, Juha and Kuvaja, Pasi and Mikkonen, Tommi and Oivo, Markku and Lassenius, Casper},
  year = {2019},
  month = oct,
  journal = {Information and Software Technology},
  volume = {114},
  pages = {217--230},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2019.06.010},
  abstract = {Context: DevOps is considered important in the ability to frequently and reliably update a system in operational state. DevOps presumes cross-functional collaboration and automation between software development and operations. DevOps adoption and implementation in companies is non-trivial due to required changes in technical, organisational and cultural aspects. Objectives: This exploratory study presents detailed descriptions of how DevOps is implemented in practice. The context of our empirical investigation is web application and service development in small and medium sized companies. Method: A multiple-case study was conducted in five different development contexts with successful DevOps implementations since its benefits, such as quick releases and minimum deployment errors, were achieved. Data was mainly collected through interviews with 26 practitioners and observations made at the companies. Data was analysed by first coding each case individually using a set of predefined themes and thereafter perform a cross-case synthesis. Results: Our analysis yielded some of the following results: (i) software development team attaining ownership and responsibility to deploy software changes in production is crucial in DevOps. (ii) toolchain usage and support in deployment pipeline activities accelerates the delivery of software changes, bug fixes and handling of production incidents. (ii) the delivery speed to production is affected by context factors, such as manual approvals by the product owner (iii) steep learning curve for new skills is experienced by both software developers and operations staff, who also have to cope with working under pressure. Conclusion: Our findings contributes to the overall understanding of DevOps concept, practices and its perceived impacts, particularly in small and medium sized companies. We discuss two practical implications of the results.},
  langid = {english},
  keywords = {Agile,Continuous deployment,Development,DevOps,Operations},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CXPCPNWD\\Lwakatare e.a. - 2019 - DevOps in practice A multiple case study of five .pdf}
}

@article{maeda_visualization_2011,
  title = {Visualization of {{Code Clone Detection Results}} and the {{Implementation}} with {{Structured Data}}},
  author = {Maeda, Kazuaki},
  year = {2011},
  month = mar,
  journal = {International Journal of Mathematical and Computational Sciences},
  volume = {5},
  number = {3},
  pages = {441--446},
  abstract = {Visualization of Code Clone Detection Results and the Implementation with Structured Data},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RNMWMV4Q\\Maeda - 2011 - Visualization of Code Clone Detection Results and .pdf;C\:\\Users\\tjvan\\Zotero\\storage\\JYQN4EDC\\visualization-of-code-clone-detection-results-and-the-implementation-with-structured-data.html}
}

@inproceedings{mahmood_service_2007,
  title = {Service Oriented Architecture: Potential Benefits and Challenges},
  booktitle = {Proceedings of the 11th {{WSEAS International Conference}} on {{COMPUTERS}}},
  author = {Mahmood, Zaigham},
  year = {2007},
  pages = {497--501},
  publisher = {{Citeseer}}
}

@article{mahmood_service_nodate,
  title = {Service {{Oriented Architecture}}: {{Potential Benefits}} and {{Challenges}}},
  author = {Mahmood, Zaigham},
  pages = {5},
  abstract = {Globalisation, tighter economies, business process outsourcing, ever increasing regulatory environments and knowledgeable consumers are forcing the large enterprises to transform the way they provide their business and services. Businesses are required to be agile and flexible and IT managers are being asked to deliver improved functionality while leveraging existing IT investment. In this climate, Service Oriented Architecture (SOA) is proving to be an attractive approach to Enterprise Application Integration and other solutions that they seek. SOA promises better alignment of IT with business, effective reuse, interoperability and reduced costs of development. However, like any approach, it has its limitations and therefore posses a number of challenges. In this paper, we introduce the SOA approach, present the benefits it offers and discuss the inherent issues and challenges. The objective is to provide enough background information that enterprises wishing to embark on the road to SOA have a better understanding of this approach.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UN48RB87\\Mahmood - Service Oriented Architecture Potential Benefits .pdf}
}

@article{marijn_kruisselbrink_arun_ranganathan_file_2018,
  title = {{File API}},
  author = {{Marijn Kruisselbrink, Arun Ranganathan}},
  year = {2018},
  month = nov,
  journal = {File API},
  abstract = {This specification provides an API for representing file objects in web applications, as well as programmatically selecting them and accessing their data. This includes: A FileList interface, which represents an array of individually selected files from the underlying system. The user interface for selection can be invoked via {$<$}input type="file"{$>$}, i.e. when the input element is in the File Upload state [HTML]. A Blob interface, which represents immutable raw binary data, and allows access to ranges of bytes within the Blob object as a separate Blob. A File interface, which includes readonly informational attributes about a file such as its name and the date of the last modification (on disk) of the file. A FileReader interface, which provides methods to read a File or a Blob, and an event model to obtain the results of these reads. A URL scheme for use with binary data such as files, so that they can be referenced within web applications. Additionally, this specification defines objects to be used within threaded web applications for the synchronous reading of files. \textsection 10 Requirements and Use Cases covers the motivation behind this specification. This API is designed to be used in conjunction with other APIs and elements on the web platform, notably: XMLHttpRequest (e.g. with an overloaded send() method for File or Blob arguments), postMessage(), DataTransfer (part of the drag and drop API defined in [HTML]) and Web Workers. Additionally, it should be possible to programmatically obtain a list of files from the input element when it is in the File Upload state  [HTML]. These kinds of behaviors are defined in the appropriate affiliated specifications.},
  langid = {Engels}
}

@article{marszalek_accurate_2012,
  title = {Accurate {{Object Recognition}} with {{Shape Masks}}},
  author = {Marsza{\l}ek, Marcin and Schmid, Cordelia},
  year = {2012},
  month = apr,
  journal = {International Journal of Computer Vision},
  volume = {97},
  number = {2},
  pages = {191--209},
  issn = {09205691},
  doi = {10.1007/s11263-011-0479-2},
  abstract = {In this paper we propose an object recognition approach that is based on shape masks-generalizations of segmentation masks. As shape masks carry information about the extent (outline) of objects, they provide a convenient tool to exploit the geometry of objects. We apply our ideas to two common object class recognition tasks-classification and localization. For classification, we extend the orderless bag-of-features image representation. In the proposed setup shape masks can be seen as weak geometrical constraints over bag-of-features. Those constraints can be used to reduce background clutter and help recognition. For localization, we propose a new recognition scheme based on high-dimensional hypothesis clustering. Shape masks allow to go beyond bounding boxes and determine the outline (approximate segmentation) of the object during localization. Furthermore, the method easily learns and detects possible object viewpoints and articulations, which are often well characterized by the object outline. Our experiments reveal that shape masks can improve recognition accuracy of state-of-the-art methods while returning richer recognition answers at the same time. We evaluate the proposed approach on the challenging natural-scene Graz-02 object classes dataset.},
  keywords = {Bag-of-features,COMPUTER vision,GEOMETRY,Graz-02,IMAGE segmentation,Local features,LOCALIZATION theory,Object recognition,Object segmentation,PIXELS,Shape masks},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9MYPILYC\\Marszałek en Schmid - 2012 - Accurate Object Recognition with Shape Masks.pdf}
}

@article{martinez-alvarez_survey_2015,
  title = {A Survey on Data Mining Techniques Applied to Electricity-Related Time Series Forecasting},
  author = {{Mart{\'i}nez-{\'A}lvarez}, Francisco and Troncoso, Alicia and {Asencio-Cort{\'e}s}, Gualberto and Riquelme, Jos{\'e} C},
  year = {2015},
  journal = {Energies},
  volume = {8},
  number = {11},
  pages = {13162--13193},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@article{martins_intelligent_2019,
  title = {Intelligent Beacon Location and Fingerprinting},
  author = {Martins, Pedro and Abbasi, Maryam and Sa, Filipe and Celiclio, Jose and Morgado, Francisco and Caldeira, Filipe},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {The 10th {{International Conference}} on {{Ambient Systems}}, {{Networks}} and {{Technologies}} ({{ANT}} 2019) / {{The}} 2nd {{International Conference}} on {{Emerging Data}} and {{Industry}} 4.0 ({{EDI40}} 2019) / {{Affiliated Workshops}}},
  volume = {151},
  pages = {9--16},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.04.005},
  abstract = {The complex way radio waves propagate indoors, leads to the derivation of location using fingerprinting techniques. In this cases, location is computed relying on WiFi signals strength mapping. Recent Bluetooth Low Energy (BLE) provides new opportunities to explore positioning. Indoor location identification plays a fundamental role as a business and personal level. At a business level, indoor location pinpointing where GPS signal is nonexistent is used to advise users and send push notifications (e.g., stores publicity, guide persons with special needs, or even for emergency evacuation). In this work is studied how BLE beacons radio signals can be used for indoor location scenarios, as well as their precision. The proposed study is performed inside the campus of Viseu Polytechnic Institute, using hundreds of students, each with his smart-phone, as proof of concept. Experimental results show that BLE allows having less than 1.5 meters error approximately 90\% of the times.},
  keywords = {beacons,BLE,blockchain,Bluetooth,crowd learning,fingerprinting,GPS,indoor location,WiFi,wireless},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\H8EMITV6\\Martins e.a. - 2019 - Intelligent beacon location and fingerprinting.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\BD58VNTP\\S1877050919304661.html}
}

@misc{matei_spark_nodate,
  title = {{Spark: Cluster Computing with Working Sets}},
  author = {Matei, Zaharia and Mosharaf, Chowdhury and Michael J., Franklin and Shenker, Scott and Stoica, Ion},
  publisher = {{University of California, Berkeley}},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
  langid = {Engels}
}

@article{mavroeidis_impact_2018,
  title = {The {{Impact}} of {{Quantum Computing}} on {{Present Cryptography}}},
  author = {Mavroeidis, Vasileios and Vishi, Kamer and Zych, Mateusz D. and J{\o}sang, Audun},
  year = {2018},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {9},
  number = {3},
  eprint = {1804.00200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  issn = {21565570, 2158107X},
  doi = {10.14569/IJACSA.2018.090354},
  abstract = {The aim of this paper is to elucidate the implications of quantum computing in present cryptography and to introduce the reader to basic post-quantum algorithms. In particular the reader can delve into the following subjects: present cryptographic schemes (symmetric and asymmetric), differences between quantum and classical computing, challenges in quantum computing, quantum algorithms (Shor's and Grover's), public key encryption schemes affected, symmetric schemes affected, the impact on hash functions, and post quantum cryptography. Specifically, the section of Post-Quantum Cryptography deals with different quantum key distribution methods and mathematicalbased solutions, such as the BB84 protocol, lattice-based cryptography, multivariate-based cryptography, hash-based signatures and code-based cryptography.},
  archiveprefix = {arXiv},
  keywords = {81P94; 11T71; 94A60; 14G50,Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\Y57D26NK\\Mavroeidis e.a. - 2018 - The Impact of Quantum Computing on Present Cryptog.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\VKHME86L\\1804.html}
}

@article{mayernik_open_2017,
  title = {Open Data: {{Accountability}} and Transparency},
  shorttitle = {Open Data},
  author = {Mayernik, Matthew S},
  year = {2017},
  month = dec,
  journal = {Big Data \& Society},
  volume = {4},
  number = {2},
  pages = {2053951717718853},
  issn = {2053-9517},
  doi = {10.1177/2053951717718853},
  abstract = {The movements by national governments, funding agencies, universities, and research communities toward ``open data'' face many difficult challenges. In high-level visions of open data, researchers' data and metadata practices are expected to be robust and structured. The integration of the internet into scientific institutions amplifies these expectations. When examined critically, however, the data and metadata practices of scholarly researchers often appear incomplete or deficient. The concepts of ``accountability'' and ``transparency'' provide insight in understanding these perceived gaps. Researchers' primary accountabilities are related to meeting the expectations of research competency, not to external standards of data deposition or metadata creation. Likewise, making data open in a transparent way can involve a significant investment of time and resources with no obvious benefits. This paper uses differing notions of accountability and transparency to conceptualize ``open data'' as the result of ongoing achievements, not one-time acts.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NAVA5SSC\\Mayernik - 2017 - Open data Accountability and transparency.pdf}
}

@book{mckinney_python_2012,
  title = {Python for {{Data Analysis}}: {{Data Wrangling}} with {{Pandas}}, {{NumPy}}, and {{IPython}}},
  shorttitle = {Python for {{Data Analysis}}},
  author = {McKinney, Wes},
  year = {2012},
  month = oct,
  publisher = {{"O'Reilly Media, Inc."}},
  abstract = {Python for Data Analysis is concerned with the nuts and bolts of manipulating, processing, cleaning, and crunching data in Python. It is also a practical, modern introduction to scientific computing in Python, tailored for data-intensive applications. This is a book about the parts of the Python language and libraries you'll need to effectively solve a broad set of data analysis problems. This book is not an exposition on analytical methods using Python as the implementation language.Written by Wes McKinney, the main author of the pandas library, this hands-on book is packed with practical cases studies. It's ideal for analysts new to Python and for Python programmers new to scientific computing.Use the IPython interactive shell as your primary development environmentLearn basic and advanced NumPy (Numerical Python) featuresGet started with data analysis tools in the pandas libraryUse high-performance tools to load, clean, transform, merge, and reshape dataCreate scatter plots and static or interactive visualizations with matplotlibApply the pandas groupby facility to slice, dice, and summarize datasetsMeasure data by points in time, whether it's specific instances, fixed periods, or intervalsLearn how to solve problems in web analytics, social sciences, finance, and economics, through detailed examples},
  googlebooks = {v3n4\_AK8vu0C},
  isbn = {978-1-4493-2361-5},
  langid = {english},
  keywords = {Computers / Data Processing,Computers / General,Computers / Programming Languages / Python}
}

@misc{mcnamara_exploring_nodate,
  title = {Exploring {{Histograms}}},
  author = {McNamara, Aran Lunzer {and} Amelia},
  abstract = {An interactive essay on the joys and pitfalls of histograms},
  howpublished = {https://tinlizzie.org/histograms/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MK26S3LR\\histograms.html}
}

@article{micheli_lower_1986,
  title = {Lower Extremity Overuse Injuries},
  author = {Micheli, L. J.},
  year = {1986},
  journal = {Acta Medica Scandinavica. Supplementum},
  volume = {711},
  pages = {171--177},
  issn = {0365-463X},
  abstract = {Increased number of overuse injuries, particularly of the lower extremity, occurs in exercise. It is important for the physician who prescribes exercise to understand the mechanism of injury-repetitive microtrauma-and the risk factors for occurrence of these injuries. In the lower extremity, risk factors include inappropriate progression of the rate, intensity and duration of training; anatomic malalignment; muscle-tendon imbalances of strength, endurance, or flexibility; shoewear; surface; and pre-existent disease states. Specific recommendations for training progression, technique, and sports equipment may have to come from the physician, since the recreational athlete often does not have access to coach or athletic trainer. These are described.},
  langid = {english},
  pmid = {3535408},
  keywords = {Adult,Athletic Injuries,Child,Humans,Leg Injuries,Male,Risk}
}

@misc{mosca_cybersecurity_nodate,
  title = {Cybersecurity in an Era with Quantum Computers: Will We Be Ready?},
  shorttitle = {Cybersecurity in an Era with Quantum Computers},
  author = {Mosca, Michele},
  abstract = {Quantum computers will break currently deployed public-key cryptography, and significantly weaken symmetric key cryptography, which are pillars of modern-day cybersecurity. Thus, before large-scale quantum computers are built, we need to migrate our systems and practices to ones that cannot be broken by quantum computers. For systems that aim to provide long-term confidentiality, this migration should happen even sooner. There are viable options for quantum-proofing our cryptographic infrastructure, but the road ahead is neither easy nor fast. Impressive progress in developing the building blocks of a fault-tolerant scalable quantum computer indicates that the prospect of a large-scale quantum computer is a medium-term threat. For example, I estimate a \$1/2\$ chance of breaking RSA-2048 by \$2031\$. In this note, I briefly overview the problem, the solutions and some of the next steps.This note is based on the abstract for a talk I gave at QCRYPT 2015 in Tokyo. http://2015.qcrypt.net/scientific-program/},
  keywords = {cryptanalysis,post-quantum cryptography,quantum computing,quantum cryptanalysis,quantum cryptography,quantum-resistant cryptography,quantum-safe cryptography}
}

@article{mustonen_ways_nodate,
  title = {Ways to Improve {{Continuous Deployment}} Processes},
  author = {Mustonen, Aleksi},
  pages = {86},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LQZQHBSH\\Mustonen - Ways to improve Continuous Deployment processes.pdf}
}

@article{najm_measuring_2014,
  title = {Measuring {{Maintainability Index}} of a {{Software Depending}} on {{Line}} of {{Code Only}}},
  author = {Najm, NMAM},
  year = {2014},
  journal = {vol},
  volume = {16},
  pages = {64--69}
}

@article{nakamoto_bitcoin:_nodate,
  title = {Bitcoin: {{A Peer-to-Peer Electronic Cash System}}},
  author = {Nakamoto, Satoshi},
  pages = {9},
  abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HGWDVHWC\\Nakamoto - Bitcoin A Peer-to-Peer Electronic Cash System.pdf}
}

@book{navidi_statistics_2011,
  title = {{Statistics for engineers and scientists}},
  author = {Navidi, William},
  year = {2011},
  number = {3rde},
  publisher = {{McGraw-Hill}},
  isbn = {978-0-07-337633-2},
  langid = {Engels}
}

@misc{nbs-system_naxsi_nodate,
  title = {{{NAXSI}}},
  author = {{nbs-system}},
  abstract = {NAXSI is an open-source, high performance, low rules maintenance WAF for NGINX},
  copyright = {Open source}
}

@misc{neera_private_2021,
  title = {Private and {{Utility Enhanced Recommendations}} with {{Local Differential Privacy}} and {{Gaussian Mixture Model}}},
  author = {Neera, Jeyamohan and Chen, Xiaomin and Aslam, Nauman and Wang, Kezhi and Shu, Zhan},
  year = {2021},
  month = mar,
  number = {arXiv:2102.13453},
  eprint = {2102.13453},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recommendation systems rely heavily on users behavioural and preferential data (e.g. ratings, likes) to produce accurate recommendations. However, users experience privacy concerns due to unethical data aggregation and analytical practices carried out by the Service Providers (SP). Local differential privacy (LDP) based perturbation mechanisms add noise to users data at user side before sending it to the SP. The SP then uses the perturbed data to perform recommendations. Although LDP protects the privacy of users from SP, it causes a substantial decline in predictive accuracy. To address this issue, we propose an LDP-based Matrix Factorization (MF) with a Gaussian Mixture Model (MoG). The LDP perturbation mechanism, Bounded Laplace (BLP), regulates the effect of noise by confining the perturbed ratings to a predetermined domain. We derive a sufficient condition of the scale parameter for BLP to satisfy \$\textbackslash epsilon\$ LDP. At the SP, The MoG model estimates the noise added to perturbed ratings and the MF algorithm predicts missing ratings. Our proposed LDP based recommendation system improves the recommendation accuracy without violating LDP principles. The empirical evaluations carried out on three real world datasets, i.e., Movielens, Libimseti and Jester, demonstrate that our method offers a substantial increase in predictive accuracy under strong privacy guarantee.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\V7A4IS7T\\Neera e.a. - 2021 - Private and Utility Enhanced Recommendations with .pdf;C\:\\Users\\tjvan\\Zotero\\storage\\5ERLZYHX\\2102.html}
}

@article{nessabramof_waist_2008,
  title = {Waist {{Circumference Measurement}} in {{Clinical Practice}}},
  author = {Ness-Abramof, Rosane and Apovian, Caroline M.},
  year = {2008},
  journal = {Nutrition in Clinical Practice},
  volume = {23},
  number = {4},
  pages = {397--404},
  issn = {1941-2452},
  doi = {10.1177/0884533608321700},
  abstract = {The obesity epidemic is a major public health problem worldwide. Adult obesity is associated with increased morbidity and mortality. Measurement of abdominal obesity is strongly associated with increased cardiometabolic risk, cardiovascular events, and mortality. Although waist circumference is a crude measurement, it correlates with obesity and visceral fat amount, and is a surrogate marker for insulin resistance. A normal waist circumference differs for specific ethnic groups due to different cardiometabolic risk. For example, Asians have increased cardiometabolic risk at lower body mass indexes and with lower waist circumferences than other populations. One criterion for the diagnosis of the metabolic syndrome, according to different study groups, includes measurement of abdominal obesity (waist circumference or waist-to-hip ratio) because visceral adipose tissue is a key component of the syndrome. The waist circumference measurement is a simple tool that should be widely implemented in clinical practice to improve cardiometabolic risk stratification.},
  copyright = {\textcopyright{} 2008 by The American Society for Parenteral and Enteral Nutrition},
  langid = {english},
  keywords = {body weight,body weight changes,cardiovascular diseases,diabetes mellitus,metabolic syndrome X,obesity,waist-hip ratio},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZKBDPQC2\\Ness‐Abramof en Apovian - 2008 - Waist Circumference Measurement in Clinical Practi.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\VC4J5P9J\\0884533608321700.html}
}

@misc{newman_demystifying_2014,
  title = {Demystifying {{Conway}}'s {{Law}}},
  author = {Newman, Sam},
  year = {2014},
  month = jun,
  journal = {Thoughtworks}
}

@article{newman_missing_2014,
  title = {Missing {{Data}}: {{Five Practical Guidelines}}},
  shorttitle = {Missing {{Data}}},
  author = {Newman, Daniel A.},
  year = {2014},
  month = oct,
  journal = {Organizational Research Methods},
  volume = {17},
  number = {4},
  pages = {372--411},
  issn = {1094-4281},
  doi = {10.1177/1094428114548590},
  abstract = {Missing data (a) reside at three missing data levels of analysis (item-, construct-, and person-level), (b) arise from three missing data mechanisms (missing completely at random, missing at random, and missing not at random) that range from completely random to systematic missingness, (c) can engender two missing data problems (biased parameter estimates and inaccurate hypothesis tests/inaccurate standard errors/low power), and (d) mandate a choice from among several missing data treatments (listwise deletion, pairwise deletion, single imputation, maximum likelihood, and multiple imputation). Whereas all missing data treatments are imperfect and are rooted in particular statistical assumptions, some missing data treatments are worse than others, on average (i.e., they lead to more bias in parameter estimates and less accurate hypothesis tests). Social scientists still routinely choose the more biased and error-prone techniques (listwise and pairwise deletion), likely due to poor familiarity with and misconceptions about the less biased/less error-prone techniques (maximum likelihood and multiple imputation). The current user-friendly review provides five easy-to-understand practical guidelines, with the goal of reducing missing data bias and error in the reporting of research results. Syntax is provided for correlation, multiple regression, and structural equation modeling with missing data.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YUR2FS58\\Newman - 2014 - Missing Data Five Practical Guidelines.pdf}
}

@misc{noauthor_11_nodate,
  title = {1.1. {{Linear Models}} \textemdash{} Scikit-Learn 0.22.1 Documentation},
  howpublished = {https://scikit-learn.org/stable/modules/linear\_model.html\#ridge-regression},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MNLS9I9J\\linear_model.html}
}

@misc{noauthor_abdominal_nodate,
  title = {Abdominal {{Circumference Is Superior}} to {{Body Mass Index}} in {{Es}}... : {{Medicine}} \& {{Science}} in {{Sports}} \& {{Exercise}}},
  howpublished = {https://journals-lww-com.hu.idm.oclc.org/acsm-msse/fulltext/2014/10000/Abdominal\_Circumference\_Is\_Superior\_to\_Body\_Mass.10.aspx}
}

@misc{noauthor_algemene_2018,
  title = {{Algemene informatie AVG}},
  year = {2018},
  month = sep,
  publisher = {{Autoriteit persoonsgegevens}},
  langid = {dutch}
}

@misc{noauthor_algemene_nodate,
  title = {{Algemene verordening gegevensbescherming (AVG)}},
  howpublished = {https://www.autoriteitpersoonsgegevens.nl/nl/over-privacy/wetten/algemene-verordening-gegevensbescherming-avg},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VC4TGGB2\\algemene-verordening-gegevensbescherming-avg.html}
}

@misc{noauthor_alles_nodate,
  title = {Alles Wat u Moet Weten over Blockchain Technologie.},
  howpublished = {http://www.watisblockchain.nl/wat\_is\_blockchain.php},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GG8KYLGG\\wat_is_blockchain.html}
}

@misc{noauthor_amazon_nodate,
  title = {Amazon {{Web Services}} ({{AWS}}) - {{Cloud Computing Services}}},
  journal = {Amazon Web Services, Inc.},
  abstract = {Amazon Web Services offers reliable, scalable, and inexpensive cloud computing services. Free to join, pay only for what you use.},
  howpublished = {https://aws.amazon.com/},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\S6ZG52XF\\aws.amazon.com.html}
}

@misc{noauthor_amazon_nodate-1,
  title = {Amazon {{Web Services}} ({{AWS}}) - {{Cloud Computing Services}}},
  howpublished = {https://aws.amazon.com/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RKQVNN62\\aws.amazon.com.html}
}

@misc{noauthor_angular_nodate,
  title = {Angular - {{The Ahead-of-Time}} ({{AOT}}) Compiler},
  howpublished = {https://angular.io/guide/aot-compiler},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WDH6NQL7\\aot-compiler.html}
}

@misc{noauthor_apache_nodate,
  title = {Apache {{Cassandra}}},
  howpublished = {http://cassandra.apache.org/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\Y2YKNZ25\\cassandra.apache.org.html}
}

@article{noauthor_asymmetrische_2018,
  title = {{Asymmetrische cryptografie}},
  year = {2018},
  month = aug,
  journal = {Wikipedia},
  abstract = {Bij asymmetrische cryptografie, ook wel bekend als publieke-sleutelcryptografie, (zoals RSA) wordt gebruikgemaakt van twee aparte sleutels: \'e\'en sleutel wordt gebruikt om de informatie te coderen (vercijferen) of te ondertekenen, en de tweede sleutel om de informatie weer te decoderen (ontcijferen) of de identiteit van de afzender te verifi\"eren. Bij de andere  methode, de symmetrische cryptografie, wordt dezelfde sleutel gebruikt voor zowel coderen als decoderen; de sleutel is dan niet geschikt voor ondertekenen/verifi\"eren. Het voordeel van de asymmetrische cryptografie is dat men door het verstrekken van de ene dan wel de andere sleutel kan kiezen wie de versleutelde informatie kan lezen en ook wie allemaal informatie kan versleutelen.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 52142182},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8VR8NDIJ\\index.html}
}

@misc{noauthor_at_nodate,
  title = {At the {{Forge}}},
  howpublished = {http://delivery.acm.org.hu.idm.oclc.org/10.1145/3030000/3024959/12095.html?ip=154.59.124.147\&id=3024959\&acc=ACTIVE\%20SERVICE\&key=6949478F00778FCB\%2E6949478F00778FCB\%2E4D4702B0C3E38B35\%2E4D4702B0C3E38B35\&\_\_acm\_\_=1576078831\_2e74c701a95b7465422c07d873640a33},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RTMN3DRB\\12095.html}
}

@article{noauthor_backlog_2018,
  title = {Backlog},
  year = {2018},
  month = nov,
  journal = {Wikipedia},
  abstract = {Backlog may refer to: Product backlog, a list of requirements for a software product in development Backlog of unexamined patent applications Backlog (album), a compilation by Electronica An argument to Berkeley sockets "listen" function},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 870158802},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6Z3ASAR7\\index.html}
}

@misc{noauthor_beleggen_nodate,
  title = {Beleggen Met Je Wisselgeld - {{Peaks}}},
  howpublished = {https://www.peaks.nl/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2S5CX3BX\\www.peaks.nl.html}
}

@misc{noauthor_betaalapp_nodate,
  title = {Betaalapp {{Payconiq}} | {{Consumentenbond}}},
  howpublished = {https://www.consumentenbond.nl/betaalrekening/grote-banken-introduceren-gezamenlijke-betaaldienst},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VNDW3KJB\\grote-banken-introduceren-gezamenlijke-betaaldienst.html}
}

@misc{noauthor_bir2017_2017,
  title = {{BIR2017 - Baseline Informatiebeveiliging Rijksdienst}},
  year = {2017},
  month = oct,
  publisher = {{Ministerie van Binnenlandse Zaken en Koninkrijksrelaties}},
  langid = {dutch}
}

@misc{noauthor_blockchain_nodate,
  title = {Blockchain - {{Wikipedia}}},
  howpublished = {https://nl.wikipedia.org/wiki/Blockchain},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2AHBPREC\\Blockchain.html}
}

@book{noauthor_data_nodate,
  title = {Data {{Preprocessing}} for {{Supervised Leaning Abstract}} \textemdash{} {{Many}} Factors Affect the Success Of},
  abstract = {Machine Learning (ML) on a given task. The representation and quality of the instance data is first and foremost. If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. It is well known that data preparation and filtering steps take considerable amount of processing time in ML problems. Data pre-processing includes data cleaning, normalization, transformation, feature extraction and selection, etc. The product of data pre-processing is the final training set. It would be nice if a single sequence of data pre-processing algorithms had the best performance for each data set but this is not happened. Thus, we present the most well know algorithms for each step of data pre-processing so that one achieves the best performance for their data set. Keywords\textemdash data mining, feature selection, data cleaning I.}
}

@misc{noauthor_data_nodate-1,
  title = {Data {{Mining}}, {{Southeast Asia Edition}}},
  howpublished = {http://web.a.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzE5MzY0N19fQU41?sid=c99a076d-c030-4383-bfe6-3c70e485fd6c@sessionmgr4006\&vid=0\&format=EB\&rid=1},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZIQTUK2C\\bmxlYmtfXzE5MzY0N19fQU41.html}
}

@misc{noauthor_data_nodate-2,
  title = {Data {{Lakes}} and {{Analytics}} | {{AWS}}},
  journal = {Amazon Web Services, Inc.},
  abstract = {With AWS' portfolio of data lakes and analytics services, it has never been easier and more cost effective for customers to collect, store, analyze and share insights to meet their business needs. AWS provides the most secure, scalable, comprehensive, and cost-effective portfolio of services that enable customers to build their data lake in the cloud, analyze all their data, including data from IoT devices with a variety of analytical approaches including machine learning.},
  howpublished = {https://aws.amazon.com/big-data/datalakes-and-analytics/},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7YX2ANFB\\datalakes-and-analytics.html}
}

@article{noauthor_data-driven_2018,
  title = {Data-Driven Programming},
  year = {2018},
  month = sep,
  journal = {Wikipedia},
  abstract = {In computer programming, data-driven programming is a programming paradigm in which the program statements describe the data to be matched and the processing required rather than defining a sequence of steps to be taken. Standard examples of data-driven languages are the text-processing languages sed and AWK, where the data is a sequence of lines in an input stream \textendash{} these are thus also known as line-oriented languages \textendash{} and pattern matching is primarily done via regular expressions or line numbers.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 860110995},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HEM3DT53\\index.html}
}

@misc{noauthor_db-engines_nodate,
  title = {{{DB-Engines Ranking}}},
  journal = {DB-Engines},
  abstract = {Popularity ranking of key-value stores.},
  howpublished = {https://db-engines.com/en/ranking/key-value+store},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TESKPAQL\\key-value+store.html}
}

@misc{noauthor_deze_2013,
  title = {{Deze onderdelen mogen niet ontbreken in je onderzoeksvoorstel of research proposal}},
  year = {2013},
  month = nov,
  journal = {Scribbr},
  abstract = {Een proposal is je rode draad tijdens het schrijven. Het is dus belangrijk dat je goed nadenkt over je plan van aanpak: een goed begin is het halve werk!},
  howpublished = {https://www.scribbr.nl/scriptie-structuur/onderzoeksvoorstel-research-proposal-scriptie/},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TK2S8H9E\\onderzoeksvoorstel-research-proposal-scriptie.html}
}

@article{noauthor_disclaimer_2017,
  title = {{Disclaimer}},
  year = {2017},
  month = feb,
  journal = {Wikipedia},
  abstract = {Een disclaimer, ook wel een oordeelonthouding, voorwaardelijkheidsverklaring of vrijtekening, is een korte tekst waarin iemand zijn of haar aansprakelijkheid in een bepaalde risicohoudende aangelegenheid of situatie afwijst of beperkt. Het woord disclaimer is afkomstig van het Engelse werkwoord to disclaim, wat "afwijzen" of "verwerpen" betekent.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 48546522},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8AMX6TL5\\index.html}
}

@article{noauthor_distributed_2018,
  title = {{Distributed denial of service}},
  year = {2018},
  month = dec,
  journal = {Wikipedia},
  abstract = {Denial-of-service-aanvallen (DoS-aanvallen)  en distributed-denial-of-service-aanvallen (DDoS-aanvallen) zijn pogingen om een computer, computernetwerk of dienst niet of moeilijker bereikbaar te maken voor de bedoelde klanten. Het verschil tussen een 'gewone' DoS-aanval en een distributed-DoS-aanval is dat in het laatste geval meerdere computers tegelijk de aanval op hun doelwit uitvoeren.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 52863739},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PX4U43NN\\index.html}
}

@misc{noauthor_docker_2018,
  title = {Docker},
  year = {2018},
  month = sep
}

@misc{noauthor_docker_2018-1,
  title = {Docker {{Swarm}}: {{Services}} and {{Stacks}} in the {{Cluster}}},
  shorttitle = {Docker {{Swarm}}},
  year = {2018},
  month = apr,
  journal = {JAXenter},
  abstract = {With Docker Swarm, the single container steps into the back and the interaction of several instances across different hosts becomes the focus of attention.},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\84FFVMJD\\services-and-stacks-in-the-cluster-141399.html}
}

@misc{noauthor_docusign_nodate,
  title = {{{DocuSign Login}} - {{Enter}} Your Password to Sign In},
  howpublished = {https://account.docusign.com/password},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\U8QHIJ2I\\password.html}
}

@misc{noauthor_esb_2021,
  title = {{{ESB}} ({{Enterprise Service Bus}})},
  year = {2021},
  month = apr,
  journal = {What is an ESB (Enterprise Service Bus)?}
}

@misc{noauthor_ethereum_nodate,
  title = {Ethereum},
  journal = {ethereum.org},
  abstract = {Ethereum is a global, decentralized platform for money and new kinds of applications. On Ethereum, you can write code that controls money, and build applications accessible anywhere in the world.},
  howpublished = {https://ethereum.org},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KNM4N5WS\\www.ethereum.org.html}
}

@misc{noauthor_ethereum_nodate-1,
  title = {Ethereum {{Project}}},
  abstract = {Ethereum is a decentralized platform for applications that run exactly as programmed without any chance of fraud, censorship or third-party interference.},
  howpublished = {https://www.ethereum.org/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\H34IIKXF\\www.ethereum.org.html}
}

@misc{noauthor_fielding_nodate,
  title = {Fielding {{Dissertation}}: {{CHAPTER}} 5: {{Representational State Transfer}} ({{REST}})},
  howpublished = {https://www.ics.uci.edu/\textasciitilde fielding/pubs/dissertation/rest\_arch\_style.htm\#sec\_5\_1\_3},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\52PCHKUY\\rest_arch_style.html}
}

@misc{noauthor_figure_nodate,
  title = {Figure 1. {{Scatter}} Plot of Waist Circumference and {{BMI}} Showing Linear...},
  journal = {ResearchGate},
  abstract = {Download scientific diagram | Scatter plot of waist circumference and BMI showing linear regression line (mean and 95\% confidence interval) for women and men. Horizontal reference line (dotted) is set at 88cm for female waist circumference and 102cm for men, the accepted thresholds for central obesity; filled markers indicate the presence of central obesity, unfilled markers indicate its absence. The vertical reference~ from publication: Relationships between BMI, waist circumference, hypertension and fasting glucose: Rethinking risk factors in Indigenous diabetes | Objective: To determine whether the body mass index (BMI) threshold defined for obesity (30kg/m2) adequately reflects risk in an Aboriginal community with a high rate of Type 2 diabetes. Methods: Data about five diabetes risk factors (age, BMI, waist circumference (WC),... | Body Mass Index, Waist Circumference and Diabetes | ResearchGate, the professional network for scientists.},
  howpublished = {https://www.researchgate.net/figure/Scatter-plot-of-waist-circumference-and-BMI-showing-linear-regression-line-mean-and-95\_fig1\_241225018},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JFQSKFGQ\\Scatter-plot-of-waist-circumference-and-BMI-showing-linear-regression-line-mean-and-95_fig1_241.html}
}

@article{noauthor_fips_nodate,
  title = {{{FIPS}} 197, {{Advanced Encryption Standard}} ({{AES}})},
  pages = {51},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\3LSJJ6BP\\FIPS 197, Advanced Encryption Standard (AES).pdf}
}

@article{noauthor_gantt-grafiek_2015,
  title = {{Gantt-grafiek}},
  year = {2015},
  month = feb,
  journal = {Gantt-grafiek},
  langid = {dutch}
}

@misc{noauthor_get_2018,
  title = {Get {{Started}}, {{Part}} 1: {{Orientation}} and Setup},
  shorttitle = {Get {{Started}}, {{Part}} 1},
  year = {2018},
  month = nov,
  journal = {Docker Documentation},
  abstract = {Get oriented on some basics of Docker before diving into the walkthrough.},
  howpublished = {https://docs.docker.com/get-started/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HMPJGE32\\get-started.html}
}

@misc{noauthor_grafana:_nodate,
  title = {Grafana: {{The}} Open Observability Platform},
  shorttitle = {Grafana},
  journal = {Grafana Labs},
  abstract = {The open observability platform  Grafana is the open source analytics \& monitoring solution for every database  Get Grafana Learn more   Used by thousands of companies to monitor everything from infrastructure, applications, power plants to beehives.                       What is Grafana? Download Live Demo},
  howpublished = {https://grafana.com/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MAMSFFEB\\grafana.com.html}
}

@misc{noauthor_gratis_nodate,
  type = {{Text}},
  title = {{Gratis woordenboek}},
  journal = {Van Dale},
  howpublished = {https://www.vandale.nl/gratis-woordenboek/nederlands/betekenis/geschiedenis},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\57V7N4AW\\geschiedenis.html}
}

@misc{noauthor_het_nodate,
  title = {Het Boekhoudpakket Voor Zzp'ers: {{Tellow}}},
  howpublished = {https://www.tellow.nl/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RNJG9D4P\\www.tellow.nl.html}
}

@misc{noauthor_ibm_2021,
  title = {{{IBM Quantum}} Roadmap to Build Quantum-Centric Supercomputers},
  year = {2021},
  month = feb,
  journal = {IBM Research Blog},
  abstract = {The updated IBM Quantum roadmap: weaving quantum processors, CPUs, and GPUs into a compute fabric to solve problems beyond the scope of classical resources.},
  copyright = {\textcopyright{} Copyright IBM Corp. 2021},
  howpublished = {https://research.ibm.com/blog/ibm-quantum-roadmap-2025},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LKGAGVSL\\ibm-quantum-roadmap-2025.html}
}

@misc{noauthor_ibms_2021,
  title = {{{IBM}}'s Roadmap for Scaling Quantum Technology},
  year = {2021},
  month = feb,
  journal = {IBM Research Blog},
  abstract = {Our quantum roadmap is leading to increasingly larger and better chips, with a 1,000-qubit chip, IBM Quantum Condor, targeted for the end of 2023.},
  copyright = {\textcopyright{} Copyright IBM Corp. 2021},
  howpublished = {https://research.ibm.com/blog/ibm-quantum-roadmap},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\V4F8FDLW\\ibm-quantum-roadmap.html}
}

@misc{noauthor_imputing_nodate,
  title = {Imputing Missing Values with Variants of {{IterativeImputer}} \textemdash{} Scikit-Learn 0.22.1 Documentation},
  howpublished = {https://scikit-learn.org/stable/auto\_examples/impute/plot\_iterative\_imputer\_variants\_comparison.html},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9BBV9TN2\\plot_iterative_imputer_variants_comparison.html}
}

@misc{noauthor_ionic-native/http_2018,
  type = {{Question}},
  title = {{Ionic-native/http debug}},
  year = {2018},
  month = mar,
  abstract = {You have to debug the app on the native level then. A simple way is to define a proxy, e.g. mitmproxy or Charles Web Proxy, in the device you are using to test to see all the traffic going out and in of your device.},
  langid = {Engels}
}

@article{noauthor_iso_2019,
  title = {{ISO 25010}},
  year = {2019},
  month = apr,
  journal = {Wikipedia},
  abstract = {De ISO-norm 25010 vervangt sinds 2011 de ISO-norm 9126 en beschrijft de onderstaande kwaliteitskenmerken van software. Het model voor productkwaliteit onderscheidt acht hoofdcategorie\"en die elk zijn onderverdeeld in kwaliteitseigenschappen, 31 in totaal. Naast het model voor productkwaliteit beschrijft de norm ook een model voor kwaliteit tijdens gebruik. Dit model onderscheidt vijf hoofdcategorie\"en die zijn onderverdeeld in 11 kwaliteitseigenschappen.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 53552769},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QN6VFLIX\\index.html}
}

@misc{noauthor_iso_nodate,
  title = {{{ISO}} 25010},
  howpublished = {https://iso25000.com/index.php/en/iso-25000-standards/iso-25010?limit=3\&start=6},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\J5DHIRKZ\\iso-25010.html}
}

@book{noauthor_journal_nodate,
  title = {Journal of {{Supercomputing}}}
}

@misc{noauthor_json_2017,
  title = {{The JSON Data Interchange Syntax}},
  year = {2017},
  month = dec,
  publisher = {{ecma international}},
  langid = {Engels}
}

@article{noauthor_key-value_2019,
  title = {Key-Value Database},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  abstract = {A key-value database, or key-value store, is a data storage paradigm designed for storing, retrieving, and managing associative arrays, a data structure more commonly known today as a dictionary or hash table. Dictionaries contain a collection of objects, or records, which in turn have many different fields within them, each containing data. These records are stored and retrieved using a key that uniquely identifies the record, and is used to quickly find the data within the database.  Key-value databases work in a very different fashion from the better known relational databases (RDB). RDBs pre-define the data structure in the database as a series of tables containing fields with well defined data types. Exposing the data types to the database program allows it to apply a number of optimizations. In contrast, key-value systems treat the data as a single opaque collection, which may have different fields for every record. This offers considerable flexibility and more closely follows modern concepts like object-oriented programming. Because optional values are not represented by placeholders or input parameters, as in most RDBs, key-value databases often use far less memory to store the same database, which can lead to large performance gains in certain workloads.Performance, a lack of standardization and other issues limited key-value systems to niche uses for many years, but the rapid move to cloud computing after 2010 has led to a renaissance as part of the broader NoSQL movement. Some graph databases are also key-value databases internally, adding the concept of the relationships (pointers) between records as a first class data type.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 881114299},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MB3XFEKR\\index.html}
}

@misc{noauthor_key-value_nodate,
  title = {Key-{{Value Databases}}},
  journal = {Basho},
  abstract = {Key-Value Databases},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JR9SUALK\\key-value-databases.html}
}

@misc{noauthor_lazy_nodate,
  title = {Lazy Loading - {{Wikipedia}}},
  howpublished = {https://en.wikipedia.org/wiki/Lazy\_loading},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\DWN5FSFK\\Lazy_loading.html}
}

@misc{noauthor_lineaire_nodate,
  title = {Lineaire Regressie - {{WikiStatistiek}}},
  howpublished = {https://wikistatistiek.amc.nl/index.php/Lineaire\_regressie},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8JEDZ2KD\\Lineaire_regressie.html}
}

@misc{noauthor_machine_nodate,
  title = {Machine {{Learning Quick Reference}} : {{Quick}} and {{Essential Machine Learning Hacks}} for {{Training Smart Data Models}}},
  howpublished = {http://web.a.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzIwMTg5NzVfX0FO0?sid=f0c5838c-b90a-4669-b395-6b6b8993fc85@sdc-v-sessmgr02\&vid=0\&format=EB\&rid=1},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6N494N2H\\bmxlYmtfXzIwMTg5NzVfX0FO0.html}
}

@misc{noauthor_machine_nodate-1,
  title = {Machine {{Learning Quick Reference}} : {{Quick}} and {{Essential Machine Learning Hacks}} for {{Training Smart Data Models}}},
  howpublished = {http://web.a.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzIwMTg5NzVfX0FO0?sid=4e045b3f-b898-432e-8254-595cae9518f4@sdc-v-sessmgr02\&vid=0\&format=EB\&lpid=lp\_7\&rid=0},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JJ6HKQPY\\bmxlYmtfXzIwMTg5NzVfX0FO0.html}
}

@misc{noauthor_machine_nodate-2,
  title = {Machine {{Learning}} with {{R}} : {{Expert Techniques}} for {{Predictive Modeling}}, 3rd {{Edition}}},
  howpublished = {http://web.b.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzIxMDYzMDRfX0FO0?sid=9548151f-0760-4fd2-848b-bcccb0da676e@pdc-v-sessmgr03\&vid=0\&format=EB\&lpid=lp\_1\&rid=0},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\FHKDAEEL\\bmxlYmtfXzIxMDYzMDRfX0FO0.html}
}

@misc{noauthor_machine_nodate-3,
  title = {Machine {{Learning Quick Reference}} : {{Quick}} and {{Essential Machine Learning Hacks}} for {{Training Smart Data Models}}},
  howpublished = {http://web.a.ebscohost.com.hu.idm.oclc.org/ehost/ebookviewer/ebook/bmxlYmtfXzIwMTg5NzVfX0FO0?sid=ad3c1ed2-e917-42fb-85f0-b5e4ef06f03f@sessionmgr4008\&vid=0\&format=EB\&rid=1},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YEGU3A3U\\bmxlYmtfXzIwMTg5NzVfX0FO0.html}
}

@misc{noauthor_mendeley_nodate,
  title = {Mendeley - {{Reference Management Software}} \& {{Researcher Network}}},
  howpublished = {https://www.mendeley.com/?interaction\_required=true},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NAY8JTPD\\www.mendeley.com.html}
}

@article{noauthor_moscow_2018,
  title = {{MoSCoW method}},
  year = {2018},
  month = may,
  journal = {MoSCoW method},
  pages = {1},
  langid = {Engels}
}

@misc{noauthor_nginx_2017,
  title = {Nginx {{Performance Tuning Guide}} for {{Beginners}} ({{Skyrocket Performance}})},
  year = {2017},
  month = oct,
  journal = {Official Power Up Hosting Blog},
  abstract = {Learn How to do Nginx performance tuning to get top notch performance from the Nginx server. Learn How to maintain the server up even when the load is heavy},
  howpublished = {https://poweruphosting.com/blog/nginx-performance-tunning/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\H28X96MA\\nginx-performance-tunning.html}
}

@article{noauthor_nginx_2018,
  title = {Nginx},
  year = {2018},
  month = sep,
  journal = {nginx},
  pages = {1},
  publisher = {{Wikipedia}}
}

@misc{noauthor_nginx_nodate,
  title = {Nginx {{HTTP Server}} - {{Fourth Edition}} {$>$} {{Basic Nginx Configuration}} {$>$} {{Autobench}} : {{Safari Books Online}}},
  howpublished = {https://proquest-safaribooksonline-com.hu.idm.oclc.org/book/web-development/9781788623551/testing-your-server/84a9b046\_bfc1\_4d61\_a378\_d22c8fd24f18\_xhtml\#X2ludGVybmFsX0h0bWxWaWV3P3htbGlkPTk3ODE3ODg2MjM1NTElMkY2NGZlNmUyMV9kNTFmXzRmYjNfOGFjZV8wOTIwNzQwMzQzODFfeGh0bWwmcXVlcnk9},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\67HIP7FT\\84a9b046_bfc1_4d61_a378_d22c8fd24f18_xhtml.html}
}

@misc{noauthor_no_nodate,
  title = {No {{License}}},
  journal = {Choose a License},
  abstract = {You're under no obligation to choose a license and it's your right not to include one with your code or project. But please note that opting out of open source licenses doesn't mean you're opting out of copyright law.},
  howpublished = {https://choosealicense.com/no-permission/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C2SGZET7\\no-permission.html}
}

@incollection{noauthor_notitle_nodate,
  type = {Incollection}
}

@book{noauthor_notitle_nodate-1,
  type = {Book}
}

@misc{noauthor_notitle_nodate-2,
  type = {Misc}
}

@book{noauthor_notitle_nodate-3,
  type = {Book}
}

@misc{noauthor_notitle_nodate-4,
  howpublished = {https://scholar.googleusercontent.com/scholar.bib?q=info:BSy7oOAqfiEJ:scholar.google.com/\&output=citation\&scisdr=CgXVFrawEJfD\_mJ2IuU:AAGBfm0AAAAAXoxzOuXPxdzgYvGm9ILeskmOP3FdaNFy\&scisig=AAGBfm0AAAAAXoxzOkksxfM4kXlUlOaoaN48bCtnmwdO\&scisf=4\&ct=citation\&cd=-1\&hl=nl},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2HURBKE7\\scholar.html}
}

@misc{noauthor_notitle_nodate-5,
  type = {Misc}
}

@article{noauthor_notitle_nodate-6,
  type = {Article}
}

@misc{noauthor_notitle_nodate-7,
  howpublished = {https://scholar.googleusercontent.com/scholar.bib?q=info:9EOC62dg0HAJ:scholar.google.com/\&output=citation\&scisdr=CgUbRRGTEPH9k6GWFLg:AAGBfm0AAAAAXlOTDLhZAdaQ3b2BK1csn6cf6R5gDhwQ\&scisig=AAGBfm0AAAAAXlOTDOiiJm2282pmp5QPDSWA1sp5Isfj\&scisf=4\&ct=citation\&cd=-1\&hl=en},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\L83KGYII\\scholar.html}
}

@misc{noauthor_notitle_nodate-8,
  type = {Misc}
}

@misc{noauthor_notitle_nodate-9,
  type = {Misc}
}

@misc{noauthor_online_nodate,
  title = {Online Huishoudboekje Voor al Je Inkomsten En Uitgaven},
  howpublished = {https://www.cashflow.nl/}
}

@misc{noauthor_opencv_nodate,
  title = {{{OpenCV}} Library},
  howpublished = {https://opencv.org/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LG8C22HW\\opencv.org.html}
}

@misc{noauthor_opencv:_nodate,
  title = {{{OpenCV}}: {{Interactive Foreground Extraction}} Using {{GrabCut Algorithm}}},
  howpublished = {https://docs.opencv.org/3.1.0/d8/d83/tutorial\_py\_grabcut.html},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8WXIVL7D\\tutorial_py_grabcut.html}
}

@article{noauthor_otap_2018,
  title = {{OTAP}},
  year = {2018},
  month = feb,
  journal = {Wikipedia},
  abstract = {Ontwikkeling, Test, Acceptatie en Productie, afgekort OTAP  is de naam van een methodiek die wordt gebruikt in de ICT. De hoofdwoorden in de naam geven de fases aan die onder andere in de softwareontwikkeling doorlopen worden. Het Nederlandse begrip is afgeleid van het Engelse DTAP: Development, Testing, Acceptance and Production. Het pad dat wordt doorlopen is als volgt: O: Een programma of component wordt eerst ontwikkeld in de ontwikkelomgeving, hierin bevindt zich veelal een of meerdere personen in een ontwikkelteam die ieder op een ontwikkelwerkplek werken aan 1 gezamenlijke versie, die aan het einde van elke dag wordt gekopieerd ofwel 'ingecheckt' in het versiebeheerprogramma op de ontwikkelserver. T: Deze gezamenlijke versie wordt dan 's nachts automatisch van programmacode naar een draaibaar programma omgezet, ofwel 'gebuild' en eventueel doorgezet naar de testserver. Op de testserver kan er met deeltest, ofwel \textasciiacute unittests\textasciiacute{} automatisch technisch en functioneel getest worden, waarvan de resultaten de volgende dag klaarliggen voor het ontwikkelteam. Als er na een ontwikkelperiode een functionele samengestelde versie ofwel een \textasciiacute release\textasciiacute{} wordt gestabiliseerd, kan deze release volledig doorgetest worden met alle voor de software gekozen testgevallen door alle betrokken partijen, personen en gebruikers. A: Na goedkeuring kan de versie worden ge\"installeerd in de acceptatieomgeving. Het installatieproces wordt gedocumenteerd in een productiegang draaiboek. De acceptatie-omgeving is qua software en hardware zo veel mogelijk gelijk aan de productieomgeving. De klant kan hier alvast zien hoe de release er functioneel en qua performance uit gaat zien in productie. Dit, zonder dat de dagelijkse productie onderbroken wordt. P: Indien de klant accepteert in de acceptatie-omgeving, wordt het programma ge\"installeerd op de productieomgeving, zoals gedocumenteerd toen de release naar de acceptatie-omgeving ging. Tevens moet er een terugdraaiplan zijn om bij verrassingen toch de productie-installatie ongedaan te maken en door te gaan in productie met de oude versie.E: Indien er veel gebruikersopleidingen moeten worden gegeven is het zinvol om een Educatie-omgeving neer te zetten. De educatie-omgeving is ook een productie-omgeving, maar dan voor opleidingen. Hier kan dan ongestoord opleidingen gegeven worden los van het OTAP release proces. Alle andere omgevingen zoals onderhoudsomgevingen en straattestomgevingen zouden goed binnen de OTAP omgevingen gefaciliteerd kunnen worden en daar zijn geen specifieke omgevingen voor benodigd. Met deze OTAP-werkwijze is de software volledig getest en geaccepteerd op een vergelijkbare omgeving als productie. Samen met de voorspelbare gedocumenteerde productiegang en het terugdraaiplan bij verrassingen, is de productie omgeving ultiem beschermd en is de continu\"iteit gewaarborgd.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 51022328},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JIPDBMPN\\index.html}
}

@misc{noauthor_overview_2018,
  title = {An Overview of {{HTTP}}},
  year = {2018},
  month = jun,
  journal = {MDN Web Docs},
  abstract = {HTTP is a protocol which allows the fetching of resources, such as HTML documents. It is the foundation of any data exchange on the Web and a client-server protocol, which means requests are initiated by the recipient, usually the Web browser. A complete document is reconstructed from the different sub-documents fetched, for instance text, layout description, images, videos, scripts, and more.},
  howpublished = {https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\J9F3RUJ5\\Overview.html}
}

@misc{noauthor_owasp_2017,
  title = {{OWASP Top-10 - 2017}},
  year = {2017},
  publisher = {{WhiteHats}},
  langid = {dutch}
}

@misc{noauthor_owasp_nodate,
  title = {{{OWASP}}},
  howpublished = {https://www.owasp.org/index.php/Main\_Page},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CP35E4X2\\Main_Page.html}
}

@misc{noauthor_owasp_nodate-1,
  title = {{{OWASP}}},
  howpublished = {https://www.owasp.org/index.php/Main\_Page},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YEAGWTGX\\Main_Page.html}
}

@misc{noauthor_owasp_nodate-2,
  title = {{{OWASP}}},
  howpublished = {https://www.owasp.org/index.php/Main\_Page},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2RFCKV9V\\Main_Page.html}
}

@misc{noauthor_pdf_nodate,
  title = {({{PDF}}) {{Relationships}} between {{BMI}}, Waist Circumference, Hypertension and Fasting Glucose: {{Rethinking}} Risk Factors in {{Indigenous}} Diabetes},
  shorttitle = {({{PDF}}) {{Relationships}} between {{BMI}}, Waist Circumference, Hypertension and Fasting Glucose},
  journal = {ResearchGate},
  abstract = {PDF | Objective: To determine whether the body mass index (BMI) threshold defined for obesity (30kg/m2) adequately reflects risk in an Aboriginal community with a high rate of Type 2 diabetes. Methods: Data about five diabetes risk factors (age, BMI, waist circumference (WC),...},
  howpublished = {https://www.researchgate.net/publication/241225018\_Relationships\_between\_BMI\_waist\_circumference\_hypertension\_and\_fasting\_glucose\_Rethinking\_risk\_factors\_in\_Indigenous\_diabetes},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9CZ7VGSK\\241225018_Relationships_between_BMI_waist_circumference_hypertension_and_fasting_glucose_Rethin.html}
}

@misc{noauthor_pdf_nodate-1,
  title = {({{PDF}}) {{The}} Best Criteria to Diagnose Metabolic Syndrome in Hypertensive {{Thai}} Patients},
  journal = {ResearchGate},
  abstract = {PDF | The metabolic syndrome (MS) is commonly found in clinical practice. There are many criteria to diagnose MS. The authors did a cross-sectional study to study the difference among the WHO criteria, the National Cholesterol Educational Program (NCEP) Adult Treatment Panel (ATP...},
  howpublished = {https://www.researchgate.net/publication/5299603\_The\_best\_criteria\_to\_diagnose\_metabolic\_syndrome\_in\_hypertensive\_Thai\_patients},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CZC623SH\\5299603_The_best_criteria_to_diagnose_metabolic_syndrome_in_hypertensive_Thai_patients.html}
}

@misc{noauthor_peaks_nodate,
  title = {{Peaks, beleggen met je wisselgeld}},
  journal = {Peaks, beleggen met je wisselgeld},
  abstract = {Met Peaks kan iedereen beleggen. Het enige dat je nodig hebt is een bankrekening. Ok\'e, en een beetje geduld. Al meer dan 160.000 mensen downloadden de app!},
  howpublished = {https://www.peaks.nl/},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2JQC7B5P\\www.peaks.nl.html}
}

@misc{noauthor_project_nodate,
  title = {Project {{Jupyter}}},
  abstract = {The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, visualizations, interactive dashboards and other media.},
  howpublished = {https://www.jupyter.org},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RCFTEW4E\\jupyter.org.html}
}

@misc{noauthor_psd2:_nodate,
  title = {{PSD2: walhalla voor fintechbedrijfjes?}},
  shorttitle = {{PSD2}},
  journal = {NRC},
  abstract = {Europese betaalrichtlijn: Sinds deze week worden banken gedwongen klantendata te delen met derden. De consument houdt zeggenschap.},
  howpublished = {https://www.nrc.nl/nieuws/2019/02/19/walhalla-voor-fintech-bedrijfjes-a3654612},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RNEYRNCC\\walhalla-voor-fintech-bedrijfjes-a3654612.html}
}

@article{noauthor_queteletindex_2019,
  title = {{Queteletindex}},
  year = {2019},
  month = feb,
  journal = {Wikipedia},
  abstract = {De queteletindex (afgekort QI) of body-mass index (BMI) is een index die de verhouding tussen lengte en gewicht bij een persoon weergeeft. De BMI wordt veel gebruikt om een indicatie te krijgen of er sprake is van overgewicht of ondergewicht. De BMI heeft een goede correlatie met de hoeveelheid lichaamsvet.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 53237707},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2GEDNZZN\\index.html}
}

@misc{noauthor_rabobank_nodate,
  title = {{Rabobank maakt in apps handig gebruik van PSD2}},
  abstract = {Een app voor al je bankrekeningen, een boekhoud-app die klanten van andere banken ook kunnen gebruiken. Rabobank bereidt zich voor op de aanstaande betaalrichtlijn PSD2 ...},
  howpublished = {https://www.computable.nl/artikel/nieuws/datamanagement/6550188/250449/rabobank-maakt-in-apps-handig-gebruik-van-psd2.html},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZY62PK3K\\rabobank-maakt-in-apps-handig-gebruik-van-psd2.html}
}

@misc{noauthor_redis_nodate,
  title = {Redis vs. {{Riak KV Comparison}}},
  howpublished = {https://db-engines.com/en/system/Redis\%3BRiak+KV},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WV8D843S\\Redis\;Riak+KV.html}
}

@article{noauthor_release_2016,
  title = {{Release candidate}},
  year = {2016},
  month = oct,
  journal = {Wikipedia},
  abstract = {Een release candidate (kortweg RC) is een fase in het ontwikkelingsproces van software. Concreet vertaald is dit een kandidaat voor de uiteindelijke versie. Indien er geen bugs meer in deze versie gevonden worden, zal de release candidate de uiteindelijke versie worden. Deze fase komt na de b\`etaversie en voor de release to manufacture (hoewel die laatste zelden voorkomt). Vaak zegt men ook dat deze fase bevroren is wat betreft functies. Hiermee bedoelt men dat er geen extra functies meer in het programma bij zullen komen, maar dat men alleen nog bugs uit het programma haalt. Indien nodig kan een programma meerdere release candidates hebben.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 47770802},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PJUH8XET\\index.html}
}

@misc{noauthor_rest_nodate,
  title = {{{REST}} \textendash{} {{Statelessness}} \textendash{} {{REST API Tutorial}}},
  abstract = {Statelessness means that every HTTP request happens in complete isolation. When the client makes an HTTP request, it includes all information necessary for the server to fulfill that request. The server never relies on information from previous requests.},
  howpublished = {https://restfulapi.net/statelessness/},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VEBA8RE4\\statelessness.html}
}

@misc{noauthor_sklearnlinear_modelridgeclassifier_nodate,
  title = {Sklearn.Linear\_model.{{RidgeClassifier}} \textemdash{} Scikit-Learn 0.22.2 Documentation},
  howpublished = {https://scikit-learn.org/stable/modules/generated/sklearn.linear\_model.RidgeClassifier.html},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UE9FTMDW\\sklearn.linear_model.RidgeClassifier.html}
}

@misc{noauthor_sklearntreedecisiontreeregressor_nodate,
  title = {Sklearn.Tree.{{DecisionTreeRegressor}} \textemdash{} Scikit-Learn 0.22.1 Documentation},
  howpublished = {https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\#sklearn.tree.DecisionTreeRegressor},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KFU26R22\\sklearn.tree.DecisionTreeRegressor.html}
}

@misc{noauthor_slack_nodate,
  title = {Slack | {{Gerrieke}} | {{Neurocast}}},
  howpublished = {https://app.slack.com/client/T6PQE3B1D/DTH901H7V/files/FT32PDK3L},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C9EPRRXR\\FT32PDK3L.html}
}

@misc{noauthor_soa_2021,
  title = {{{SOA}} vs. {{Microservices}}: {{What}}'s the {{Difference}}?},
  shorttitle = {{{SOA}} vs. {{Microservices}}},
  year = {2021},
  month = may,
  abstract = {In this article, we'll explain the basics of service-oriented architecture (SOA) and microservices, touch on their key differences and look at which approach would be best for your situation.},
  howpublished = {https://www.ibm.com/cloud/blog/soa-vs-microservices},
  langid = {american},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KQWW38D9\\soa-vs-microservices.html}
}

@article{noauthor_software_2018,
  title = {Software Release Life Cycle},
  year = {2018},
  month = dec,
  journal = {Wikipedia},
  abstract = {A software release life cycle is the sum of the stages of development and maturity for a piece of computer software: ranging from its initial development to its eventual release, and including updated versions of the released version to help improve software or fix software bugs still present in the software.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 873172951},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PXFN569B\\index.html}
}

@misc{noauthor_stukje-pgo-tjibbe_nodate,
  title = {{Stukje-pgo-tjibbe}},
  journal = {Google Docs},
  abstract = {Nieuwe wetgeving van banken, zorgt ervoor dat data van klanten beschikbaar komt. Dit biedt de mogelijkheid voor leuke oplossingen op het gebied van applicaties! Een app die al gebruik maakt van deze data heet CashFlow. Deze app toont inzage over uitgaven van de gebruiker. Op dit moment kan de app...},
  howpublished = {https://docs.google.com/document/d/1WKVGUMduc5BxYAExOcEMh7RP9EqSyroE0wDIO2m-6Kg/edit?usp=drive\_web\&ouid=100859503642953552577\&usp=embed\_facebook},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PH74QITK\\edit.html}
}

@article{noauthor_systeemtest_2017,
  title = {{Systeemtest}},
  year = {2017},
  month = feb,
  journal = {Wikipedia},
  abstract = {Een systeemtest is een test van een systeem, vaak een computersysteem, bestaande uit software en hardware.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {dutch},
  annotation = {Page Version ID: 48566463},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XMY6TUW7\\index.html}
}

@misc{noauthor_tensorflow_nodate,
  title = {{{TensorFlow}}},
  journal = {TensorFlow},
  abstract = {An open source machine learning library for research and production.},
  howpublished = {https://www.tensorflow.org/},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JYDV5CLF\\www.tensorflow.org.html}
}

@article{noauthor_thread_2018,
  title = {Thread (Computing)},
  year = {2018},
  month = nov,
  journal = {Wikipedia},
  abstract = {In computer science, a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system. The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its dynamically allocated variables and non-thread-local global variables at any given time.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 871213922},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WUZ5XPNC\\index.html}
}

@misc{noauthor_top_nodate,
  title = {Top 10-2017 {{Top}} 10 - {{OWASP}}},
  howpublished = {https://www.owasp.org/index.php/Top\_10-2017\_Top\_10},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4JR47MBX\\Top_10-2017_Top_10.html}
}

@misc{noauthor_top_nodate-1,
  title = {Top 10-2017 {{A3-Sensitive Data Exposure}} - {{OWASP}}},
  howpublished = {https://www.owasp.org/index.php/Top\_10-2017\_A3-Sensitive\_Data\_Exposure},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8JI63XQ4\\Top_10-2017_A3-Sensitive_Data_Exposure.html}
}

@misc{noauthor_top_nodate-2,
  title = {Top 10-2017 {{A1-Injection}} - {{OWASP}}},
  howpublished = {https://www.owasp.org/index.php/Top\_10-2017\_A1-Injection},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PA6DPSNN\\Top_10-2017_A1-Injection.html}
}

@article{noauthor_unix_2019,
  title = {Unix Domain Socket},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  abstract = {A Unix domain socket or IPC socket (inter-process communication socket) is a data communications endpoint for exchanging data between processes executing on the same host operating system. Like named pipes, Unix domain sockets support transmission of a reliable stream of bytes (SOCK\_STREAM, compare to TCP).  In addition, they support ordered and reliable transmission of datagrams (SOCK\_SEQPACKET, compare to SCTP), or unordered and unreliable transmission of datagrams (SOCK\_DGRAM, compare to UDP).  The Unix domain socket facility is a standard component of POSIX operating systems. The API for Unix domain sockets is similar to that of an Internet socket, but rather than using an underlying network protocol, all communication occurs entirely within the operating system kernel.  Unix domain sockets use the file system as their address name space. Processes reference Unix domain sockets as file system inodes, so two processes can communicate by opening the same socket. In addition to sending data, processes may send file descriptors across a Unix domain socket connection using the sendmsg() and recvmsg() system calls. This allows the sending processes to grant the receiving process access to a file descriptor for which the receiving process otherwise does not have access.  This can be used to implement a rudimentary form of capability-based security. For example, this allows the Clam AntiVirus scanner to run as an unprivileged daemon on Linux and BSD, yet still read any file sent to the daemon's Unix domain socket.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 876691015},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZXYVZ2YC\\index.html}
}

@misc{noauthor_uppaal_nodate,
  title = {Uppaal Reduce State Space - {{Google Zoeken}}},
  howpublished = {https://www.google.com/search?q=uppaal+reduce+state+space\&oq=uppaal+reduce+state+space\&aqs=edge.0.69i59j69i57.9177j0j1\&sourceid=chrome\&ie=UTF-8},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2QF6CZVM\\search.html}
}

@misc{noauthor_w3schools_2018,
  title = {{W3Schools}},
  year = {2018},
  month = sep,
  langid = {Engels}
}

@misc{noauthor_wayback_2016,
  title = {Wayback {{Machine}}},
  year = {2016},
  month = apr,
  howpublished = {https://web.archive.org/web/20160417030218/http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf}
}

@article{noauthor_web_2017,
  title = {{Web Server Gateway Interface}},
  year = {2017},
  month = aug,
  journal = {Web Server Gateway interface},
  langid = {dutch}
}

@misc{noauthor_weber_nodate,
  title = {{Weber Original Kettle 57 cm}},
  journal = {https://www.coolblue.nl},
  abstract = {Bestel de Original Kettle 57 cm bij Coolblue. Voor 23.59u? Morgen gratis bezorgd. Coolblue: alles voor een glimlach.},
  howpublished = {https://www.coolblue.nl/product/429737/weber-original-kettle-57-cm.html},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\RMVX6AIU\\weber-original-kettle-57-cm.html}
}

@article{noauthor_webserver_2015,
  title = {{Webserver}},
  year = {2015},
  month = sep,
  journal = {Wikipedia},
  pages = {1},
  langid = {Engels}
}

@misc{noauthor_welcome_nodate,
  title = {Welcome | {{Flask}} ({{A Python Microframework}})},
  howpublished = {http://flask.pocoo.org/},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XKNJGDT6\\flask.pocoo.org.html}
}

@misc{noauthor_what_nodate,
  title = {What Is a {{Container}}},
  journal = {Package Software into Standardized Units for Development, Shipment and Deployment}
}

@misc{noauthor_what_nodate-1,
  title = {What Are the Pros and Cons of Using Key/Value Pair Schemas in a {{SQL}} Database? - {{Quora}}},
  howpublished = {https://www.quora.com/What-are-the-pros-and-cons-of-using-key-value-pair-schemas-in-a-SQL-database},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GPR7UHD7\\What-are-the-pros-and-cons-of-using-key-value-pair-schemas-in-a-SQL-database.html}
}

@article{noauthor_wikipedia_2017,
  title = {{Wikipedia}},
  year = {2017},
  month = aug,
  journal = {Hypertext Transfer Protocol},
  pages = {1},
  langid = {dutch}
}

@misc{noauthor_wise19_ldppdf_nodate,
  title = {Wise19\_ldp.Pdf},
  journal = {Google Docs},
  howpublished = {https://drive.google.com/file/d/1C4buKvjM6Xf4QxzQfsHScrLtI5hGh830/view?usp=embed\_facebook}
}

@misc{noauthor_worldcat_nodate,
  title = {{{WorldCat Link Resolver}}, the {{OpenURL}} Link-Server from {{OCLC}}},
  howpublished = {https://hu-on-worldcat-org.hu.idm.oclc.org/atoztitles/link?stitle=J+Nurs+Scholarsh\&url=https://hu.on.worldcat.org/atoztitles/link?sid=wiley\&id=doi:10.1111\%252Fjnu.12159\&sid=wiley\&iuid=7281731\&date=2015\&dbid=16384\&aulast=Bakken\&volume=47\&doi=10.1016/j.aorn.2016.07.009\&atitle=Nursing+needs+big+data+and+big+data+needs+nursing\&spage=477\&issue=(5)\&epage=484\&suffix=e\_1\_2\_2\_2\_2\&aufirst=S.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\28T6X2CC\\link.html}
}

@misc{noauthor_zorgbehoeften_nodate,
  title = {{Zorgbehoeften van kwetsbare ouderen}},
  journal = {Huisarts \& Wetenschap},
  howpublished = {/artikelen/zorgbehoeften-van-kwetsbare-ouderen},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SQLYU5JK\\zorgbehoeften-van-kwetsbare-ouderen.html}
}

@misc{noauthor_zorgbehoeften_nodate-1,
  title = {{Zorgbehoeften van kwetsbare ouderen}},
  journal = {Huisarts \& Wetenschap},
  howpublished = {/artikelen/zorgbehoeften-van-kwetsbare-ouderen},
  langid = {dutch},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\L7QVV76U\\zorgbehoeften-van-kwetsbare-ouderen.html}
}

@misc{noauthor_zotero_nodate,
  title = {Zotero Fields Cannot Be Inserted Here Error in Word and {{Google Docs}}},
  journal = {Zotero Forums},
  abstract = {Zotero is a powerful, easy-to-use research tool that helps you gather, organize, and analyze sources and then share the results of your research., I am trying to use my Zotero in Google Docs and in word.},
  howpublished = {https://forums.zotero.org/discussion/74825/zotero-fields-cannot-be-inserted-here-error-in-word-and-google-docs},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\G5N7XCFH\\zotero-fields-cannot-be-inserted-here-error-in-word-and-google-docs.html}
}

@article{noble_what_2006,
  title = {What Is a Support Vector Machine?},
  author = {Noble, William S},
  year = {2006},
  month = dec,
  journal = {Nature Biotechnology},
  volume = {24},
  number = {12},
  pages = {1565--1567},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt1206-1565},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TWXG9PXV\\Noble - 2006 - What is a support vector machine.pdf}
}

@article{noorbakhsh_machine_2019,
  title = {Machine {{Learning}} in {{Biology}} and {{Medicine}}},
  author = {Noorbakhsh, Javad and Chandok, Harshpreet and Karuturi, R. Krishna Murthy and George, Joshy},
  year = {2019},
  month = nov,
  journal = {Advances in Molecular Pathology},
  series = {Advances in {{Molecular Pathology}}},
  volume = {2},
  number = {1},
  pages = {143--152},
  issn = {2589-4080},
  doi = {10.1016/j.yamp.2019.07.010},
  langid = {english},
  keywords = {Computational biology,Lifelong learning,Machine learning,Medicine,Reinforcement learning,Supervised learning,Unsupervised learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WWBMVWNM\\Noorbakhsh e.a. - 2019 - Machine Learning in Biology and Medicine.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\23FP44K8\\S2589408019300110.html}
}

@misc{ntitle_ntitle_nodate,
  title = {{{nTitle}}},
  author = {{nTitle}},
  abstract = {nTitle is a decentralized bcommerce software solution built on the Ethereum blockchain for game developers, gamers and influencers. It allows to seamlessly publish, market and trade digital games directly between each other.},
  howpublished = {https://ntitle.network},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C9IEV8GV\\www.ntitle.network.html}
}

@article{nuttall_body_2015,
  title = {Body {{Mass Index}}},
  author = {Nuttall, Frank Q.},
  year = {2015},
  month = may,
  journal = {Nutrition Today},
  volume = {50},
  number = {3},
  pages = {117--128},
  issn = {0029-666X},
  doi = {10.1097/NT.0000000000000092},
  abstract = {The body mass index (BMI) is the metric currently in use for defining anthropometric height/weight characteristics in adults and for classifying (categorizing) them into groups. The common interpretation is that it represents an index of an individual's fatness. It also is widely used as a risk factor for the development of or the prevalence of several health issues. In addition, it is widely used in determining public health policies.The BMI has been useful in population-based studies by virtue of its wide acceptance in defining specific categories of body mass as a health issue. However, it is increasingly clear that BMI is a rather poor indicator of percent of body fat. Importantly, the BMI also does not capture information on the mass of fat in different body sites. The latter is related not only to untoward health issues but to social issues as well. Lastly, current evidence indicates there is a wide range of BMIs over which mortality risk is modest, and this is age related. All of these issues are discussed in this brief review.},
  pmcid = {PMC4890841},
  pmid = {27340299},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9F74UKUC\\Nuttall - 2015 - Body Mass Index.pdf}
}

@article{nuzzo_histograms_2019,
  title = {Histograms: {{A Useful Data Analysis Visualization}}},
  shorttitle = {Histograms},
  author = {Nuzzo, Regina L.},
  year = {2019},
  journal = {PM\&R},
  volume = {11},
  number = {3},
  pages = {309--312},
  issn = {1934-1563},
  doi = {10.1002/pmrj.12145},
  copyright = {\textcopyright{} 2019 American Academy of Physical Medicine and Rehabilitation},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pmrj.12145},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZXCWJN8H\\Nuzzo - 2019 - Histograms A Useful Data Analysis Visualization.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\G3HMBSAM\\pmrj.html}
}

@article{nye_abdominal_2014,
  title = {Abdominal {{Circumference Is Superior}} to {{Body Mass Index}} in {{Estimating Musculoskeletal Injury Risk}}},
  author = {Nye, Nathaniel and Carnahan, David and Jackson, Jonathan and Covey, Carlton and Zarzabal, Lee and Chao, Susan and Bockhorst, Archie and Crawford, Paul},
  year = {2014},
  month = oct,
  journal = {Medicine \& Science in Sports \& Exercise},
  volume = {46},
  number = {10},
  pages = {1951--1959},
  issn = {0195-9131},
  doi = {10.1249/MSS.0000000000000329},
  abstract = {ABSTRACTPurposeThe purpose of this study was to compare body mass index (BMI) and abdominal circumference (AC) in discriminating individual musculoskeletal injury risk within a large population. We also sought to determine whether age or sex modulates the interaction between body habitus and injury},
  langid = {english},
  pmid = {24674973},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\72HSH9T2\\crossref.html}
}

@misc{owasp_owasp_2018,
  title = {{{OWASP Risk Rating Methodology}} - {{OWASP}}},
  author = {{OWASP}},
  year = {2018},
  month = jul,
  howpublished = {https://www.owasp.org/index.php/OWASP\_Risk\_Rating\_Methodology},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LVWT5VW2\\OWASP_Risk_Rating_Methodology.html}
}

@misc{owasp_top_2017,
  title = {Top 10-2017 {{A1-Injection}} - {{OWASP}}},
  author = {{OWASP}},
  year = {2017},
  howpublished = {https://www.owasp.org/index.php/Top\_10-2017\_A1-Injection},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZKKQNW3A\\Top_10-2017_A1-Injection.html}
}

@misc{panita_limpawattana_best_nodate,
  title = {The Best Criteria to Diagnose Metabolic Syndrome in Hypertensive {{Thai}} Patients},
  author = {{Panita Limpawattana} and {kittisak Sawanyawisuth} and {Ploysyne Busaracome} and {Chinching Foocharoen} and {Chalongchai Phitsanuwong} and {Somchit Chumjan} and {Ratchan}},
  journal = {ResearchGate},
  abstract = {PDF | The metabolic syndrome (MS) is commonly found in clinical practice. There are many criteria to diagnose MS. The authors did a cross-sectional study to study the difference among the WHO criteria, the National Cholesterol Educational Program (NCEP) Adult Treatment Panel (ATP...},
  howpublished = {https://www.researchgate.net/publication/5299603\_The\_best\_criteria\_to\_diagnose\_metabolic\_syndrome\_in\_hypertensive\_Thai\_patients},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KETGLD5G\\5299603_The_best_criteria_to_diagnose_metabolic_syndrome_in_hypertensive_Thai_patients.html}
}

@article{parbhoo_combining_2017,
  title = {Combining {{Kernel}} and {{Model Based Learning}} for {{HIV Therapy Selection}}},
  author = {Parbhoo, Sonali and Bogojeska, Jasmina and Zazzi, Maurizio and Roth, Volker and {Doshi-Velez}, Finale},
  year = {2017},
  month = jul,
  journal = {AMIA Summits on Translational Science Proceedings},
  volume = {2017},
  pages = {239--248},
  issn = {2153-4063},
  abstract = {We present a mixture-of-experts approach for HIV therapy selection. The heterogeneity in patient data makes it difficult for one particular model to succeed at providing suitable therapy predictions for all patients. An appropriate means for addressing this heterogeneity is through combining kernel and model-based techniques. These methods capture different kinds of information: kernel-based methods are able to identify clusters of similar patients, and work well when modelling the viral response for these groups. In contrast, model-based methods capture the sequential process of decision making, and are able to find simpler, yet accurate patterns in response for patients outside these groups. We take advantage of this information by proposing a mixture-of-experts model that automatically selects between the methods in order to assign the most appropriate therapy choice to an individual. Overall, we verify that therapy combinations proposed using this approach significantly outperform previous methods.},
  pmcid = {PMC5543338},
  pmid = {28815137},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SUW7GITN\\Parbhoo e.a. - 2017 - Combining Kernel and Model Based Learning for HIV .pdf}
}

@article{peercy_software_1981,
  title = {A Software Maintainability Evaluation Methodology},
  author = {Peercy, David E.},
  year = {1981},
  journal = {IEEE Transactions on Software Engineering},
  number = {4},
  pages = {343--351},
  publisher = {{IEEE}},
  issn = {0098-5589}
}

@inproceedings{piessens_software_2016,
  title = {Software {{Security}}: {{Vulnerabilities}} and {{Countermeasures}} for {{Two Attacker Models}}},
  shorttitle = {Software {{Security}}},
  booktitle = {Proceedings of the 2016 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Piessens, Frank and Verbauwhede, Ingrid},
  year = {2016},
  pages = {990--999},
  publisher = {{Research Publishing Services}},
  doi = {10.3850/9783981537079_0999},
  abstract = {History has shown that attacks against networkconnected software based systems are common and dangerous. An important fraction of these attacks exploit implementation details of the software based system. These attacks \textendash{} sometimes called low-level attacks \textendash{} rely on characteristics of the hardware, compiler or operating system used to execute software programs to make these programs misbehave, or to extract sensitive information from them. With the increased Internet-connectivity of embedded devices, including industrial control systems, sensors as well as consumer devices, there is a substantial risk that similar attacks will target these devices.},
  isbn = {978-3-9815370-7-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7TWD3V7I\\Piessens en Verbauwhede - 2016 - Software Security Vulnerabilities and Countermeas.pdf}
}

@misc{pieter_dubelaar_blockchain_nodate,
  title = {Blockchain \& {{Gaming}}},
  author = {{Pieter Dubelaar}}
}

@misc{proos_shors_2004,
  title = {Shor's Discrete Logarithm Quantum Algorithm for Elliptic Curves},
  author = {Proos, John and Zalka, Christof},
  year = {2004},
  month = jan,
  number = {arXiv:quant-ph/0301141},
  eprint = {quant-ph/0301141},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {We show in some detail how to implement Shor's efficient quantum algorithm for discrete logarithms for the particular case of elliptic curve groups. It turns out that for this problem a smaller quantum computer can solve problems further beyond current computing than for integer factorisation. A 160 bit elliptic curve cryptographic key could be broken on a quantum computer using around 1000 qubits while factoring the security-wise equivalent 1024 bit RSA modulus would require about 2000 qubits. In this paper we only consider elliptic curves over GF(\$p\$) and not yet the equally important ones over GF(\$2\^n\$) or other finite fields. The main technical difficulty is to implement Euclid's gcd algorithm to compute multiplicative inverses modulo \$p\$. As the runtime of Euclid's algorithm depends on the input, one difficulty encountered is the ``quantum halting problem''.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9UX7RSZ9\\Proos en Zalka - 2004 - Shor's discrete logarithm quantum algorithm for el.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\R9T2HIUC\\0301141.html}
}

@misc{proos_shors_2004-1,
  title = {Shor's Discrete Logarithm Quantum Algorithm for Elliptic Curves},
  author = {Proos, John and Zalka, Christof},
  year = {2004},
  month = jan,
  number = {arXiv:quant-ph/0301141},
  eprint = {quant-ph/0301141},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {We show in some detail how to implement Shor's efficient quantum algorithm for discrete logarithms for the particular case of elliptic curve groups. It turns out that for this problem a smaller quantum computer can solve problems further beyond current computing than for integer factorisation. A 160 bit elliptic curve cryptographic key could be broken on a quantum computer using around 1000 qubits while factoring the security-wise equivalent 1024 bit RSA modulus would require about 2000 qubits. In this paper we only consider elliptic curves over GF(\$p\$) and not yet the equally important ones over GF(\$2\^n\$) or other finite fields. The main technical difficulty is to implement Euclid's gcd algorithm to compute multiplicative inverses modulo \$p\$. As the runtime of Euclid's algorithm depends on the input, one difficulty encountered is the ``quantum halting problem''.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WKFZET5N\\Proos en Zalka - 2004 - Shor's discrete logarithm quantum algorithm for el.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\Z6IAFEGJ\\0301141.html}
}

@article{quesada_machine_2019,
  title = {Machine Learning to Predict Cardiovascular Risk},
  author = {Quesada, Jose A. and Lopez-Pineda, Adriana and Gil-Guill{\'e}n, Vicente F. and Durazo-Arvizu, Ram{\'o}n and Orozco-Beltr{\'a}n, Domingo and {L{\'o}pez-Domenech}, Angela and Carratal{\'a}-Munuera, Concepci{\'o}n},
  year = {2019},
  journal = {International Journal of Clinical Practice},
  volume = {73},
  number = {10},
  pages = {e13389},
  issn = {1742-1241},
  doi = {10.1111/ijcp.13389},
  abstract = {Aims To analyse the predictive capacity of 15 machine learning methods for estimating cardiovascular risk in a cohort and to compare them with other risk scales. Methods We calculated cardiovascular risk by means of 15 machine-learning methods and using the SCORE and REGICOR scales and in 38 527 patients in the Spanish ESCARVAL RISK cohort, with 5-year follow-up. We considered patients to be at high risk when the risk of a cardiovascular event was over 5\% (according to SCORE and machine learning methods) or over 10\% (using REGICOR). The area under the receiver operating curve (AUC) and the C-index were calculated, as well as the diagnostic accuracy rate, error rate, sensitivity, specificity, positive and negative predictive values, positive likelihood ratio, and number needed to treat to prevent a harmful outcome. Results The method with the greatest predictive capacity was quadratic discriminant analysis, with an AUC of 0.7086, followed by Naive Bayes and neural networks, with AUCs of 0.7084 and 0.7042, respectively. REGICOR and SCORE ranked 11th and 12th, respectively, in predictive capacity, with AUCs of 0.63. Seven machine learning methods showed a 7\% higher predictive capacity (AUC) as well as higher sensitivity and specificity than the REGICOR and SCORE scales. Conclusions Ten of the 15 machine learning methods tested have a better predictive capacity for cardiovascular events and better classification indicators than the SCORE and REGICOR risk assessment scales commonly used in clinical practice in Spain. Machine learning methods should be considered in the development of future cardiovascular risk scales.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\X84G2R64\\Quesada e.a. - 2019 - Machine learning to predict cardiovascular risk.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\EWEF8LCL\\ijcp.html}
}

@article{raghavendra_categorical_2011,
  title = {Categorical {{Data Analysis}}},
  author = {Raghavendra, Devadiga and Antony, Grace Maria},
  year = {2011},
  month = may,
  journal = {Applied Clinical Trials},
  volume = {20},
  number = {5},
  pages = {46--51},
  issn = {10648542},
  abstract = {The article discusses the categorical data analysis for medical research in the U.S. It offers the strategic methods for data types and models for medical assessment. Medical records are comprised of patient's demographic, risk factors, medical history, and biomedical markers which is usually used as basis for clinical trials. It explains on qualitative and quantitative data. It highlights chi-square test, McNemar test, and Kappa statistics as statistical methods for data variables.},
  keywords = {CHI-squared test,CLINICAL trials,DATA analysis,MEDICAL research,MODEL validation,UNITED States}
}

@book{rahat_khanna_getting_2016,
  title = {{Getting started with Ionic : get up and running with developing effective hybrid mobile apps with Ionic}},
  author = {{Rahat Khanna}},
  year = {2016},
  series = {{Community experience distilled}},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, UK}},
  isbn = {978-1-78439-994-8 1-78439-994-9 1-78439-057-7 978-1-78439-057-0},
  langid = {Engels}
}

@incollection{rahul_soni_nginx_2016,
  title = {Nginx {{Core Architecture}}},
  booktitle = {Nginx: {{From Beginner}} to {{Pro}}},
  author = {{Rahul Soni}},
  year = {2016},
  month = jul,
  publisher = {{Apress}},
  isbn = {978-1-4842-1657-6}
}

@book{rahul_soni_nginx:_2016,
  title = {{Nginx: From beginner to Pro}},
  author = {{Rahul Soni}},
  year = {2016},
  volume = {1 online resource},
  publisher = {{Apress}},
  isbn = {978-1-4842-1657-6},
  langid = {Engels}
}

@article{raj_performance_nodate,
  title = {Performance and Complexity Comparison of Service Oriented Architecture and Microservices Architecture},
  author = {Raj, Vinay and Sadam, Ravichandra},
  pages = {18},
  abstract = {Microservices has emerged as a new architectural style of designing applications to overcome the challenges of service oriented architecture (SOA). With the evolution of microservices architecture, architects have started migrating legacy applications to microservices. However, some of the architects are in chaos whether to migrate the application from SOA to microservices or not. The need for empirical evaluation and comparison of both the SOA and microservices architecture is also on-demand. This work helps the software architects in better understanding of the technical differences between both styles. We, therefore, present a comparison of a web application that is designed using both SOA and microservices architectures. The comparison is presented with two different parameters: 1) complexity with architectural metrics; 2) performance with load testing. It is clear from the results that though the complexity of microservices architecture is high, the response time for processing the requests is very fast compared to SOA services.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XN2YSURP\\Raj en Sadam - Performance and complexity comparison of service o.pdf}
}

@article{ramchand_systematic_nodate,
  title = {A {{Systematic Mapping}} of {{Microservice Patterns}}},
  author = {Ramchand, Mahir Hiro},
  pages = {67},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8KLBPBIK\\Ramchand - A Systematic Mapping of Microservice Patterns.pdf}
}

@article{ramchand_systematic_nodate-1,
  title = {A {{Systematic Mapping}} of {{Microservice Patterns}}},
  author = {Ramchand, Mahir Hiro},
  pages = {67},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PSZVPBJV\\Ramchand - A Systematic Mapping of Microservice Patterns.pdf}
}

@book{raschka_python_2015,
  title = {Python Machine Learning},
  author = {Raschka, Sebastian},
  year = {2015},
  publisher = {{Packt Publishing Ltd}},
  isbn = {1-78355-514-9}
}

@article{raymer_dimensionality_2000,
  title = {Dimensionality Reduction Using Genetic Algorithms},
  author = {Raymer, M.L. and Punch, W.F. and Goodman, E.D. and Kuhn, L.A. and Jain, A.K.},
  year = {2000},
  month = jul,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {4},
  number = {2},
  pages = {164--171},
  issn = {1089778X},
  doi = {10.1109/4235.850656},
  abstract = {Pattern recognition generally requires that objects be described in terms of a set of measurable features. The selection and quality of the features representing each pattern have a considerable bearing on the success of subsequent pattern classification. Feature extraction is the process of deriving new features from the original features in order to reduce the cost of feature measurement, increase classifier efficiency, and allow higher classification accuracy. Many current feature extraction techniques involve linear transformations of the original pattern vectors to new vectors of lower dimensionality. While this is useful for data visualization and increasing classification efficiency, it does not necessarily reduce the number of features that must be measured since each new feature may be a linear combination of all of the features in the original pattern vector. Here, we present a new approach to feature extraction in which feature selection, feature extraction, and classifier training are performed simultaneously using a genetic algorithm. The genetic algorithm optimizes a vector of feature weights, which are used to scale the individual features in the original pattern vectors in either a linear or a nonlinear fashion. A masking vector is also employed to perform simultaneous selection of a subset of the features. We employ this technique in combination with the nearest neighbor classification rule, and compare the results with classical feature selection and extraction techniques, including sequential floating forward feature selection, and linear discriminant analysis. We also present results for the identification of favorable water-binding sites on protein surfaces, an important problem in biochemistry and drug design.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\T5DINUZM\\Raymer e.a. - 2000 - Dimensionality reduction using genetic algorithms.pdf}
}

@article{rees_legal_1999,
  title = {Legal Issues in Electronic Publishing},
  author = {Rees, Christopher},
  year = {1999},
  month = jan,
  journal = {Computer Law \& Security Review},
  volume = {15},
  number = {1},
  pages = {3--7},
  issn = {0267-3649},
  doi = {10.1016/S0267-3649(99)80002-8},
  abstract = {This paper addresses some of the legal considerations that should be borne in mind by the adviser to an electronic publisher. In order to do that it is necessary to understand what electronic publishing entails. Then I shall look at the two principal areas of law that have traditionally governed all publishing activity, namely defamation and copyright to see what developments are occurring in those disciplines that may be of relevance to electronic publishing. Finally, I will take a brief look at the way the new database right may have significant importance for electronic publishers.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\A4SSGGMS\\S0267364999800028.html}
}

@inproceedings{rehman_customer_2014,
  title = {Customer Churn Prediction, Segmentation and Fraud Detection in Telecommunication Industry},
  booktitle = {{{ASE BigData}}/{{SocialInformatics}}/{{PASSAT}}/{{BioMedCom}} 2014 {{Conference}}},
  author = {Rehman, Ahsan and Raza Ali, Abbas},
  year = {2014}
}

@techreport{rescorla_transport_2008,
  type = {Request for {{Comments}}},
  title = {The {{Transport Layer Security}} ({{TLS}}) {{Protocol Version}} 1.2},
  author = {Rescorla, Eric and Dierks, Tim},
  year = {2008},
  month = aug,
  number = {RFC 5246},
  institution = {{Internet Engineering Task Force}},
  doi = {10.17487/RFC5246},
  abstract = {This document specifies Version 1.2 of the Transport Layer Security (TLS) protocol. The TLS protocol provides communications security over the Internet. The protocol allows client/server applications to communicate in a way that is designed to prevent eavesdropping, tampering, or message forgery. [STANDARDS-TRACK]},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9RWY4FNG\\Rescorla en Dierks - 2008 - The Transport Layer Security (TLS) Protocol Versio.pdf}
}

@techreport{rescorla_transport_2018,
  type = {Request for {{Comments}}},
  title = {The {{Transport Layer Security}} ({{TLS}}) {{Protocol Version}} 1.3},
  author = {Rescorla, Eric},
  year = {2018},
  month = aug,
  number = {RFC 8446},
  institution = {{Internet Engineering Task Force}},
  doi = {10.17487/RFC8446},
  abstract = {This document specifies version 1.3 of the Transport Layer Security (TLS) protocol. TLS allows client/server applications to communicate over the Internet in a way that is designed to prevent eavesdropping, tampering, and message forgery. This document updates RFCs 5705 and 6066, and obsoletes RFCs 5077, 5246, and 6961. This document also specifies new requirements for TLS 1.2 implementations.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8GBTUMZQ\\Rescorla - 2018 - The Transport Layer Security (TLS) Protocol Versio.pdf}
}

@article{retscher_trilateration_2019,
  title = {Trilateration {{Approaches}} for {{Indoor Wi-Fi Positioning}}},
  author = {Retscher, Guenther and Kleine, Jonathan and Whitemore, Lisa},
  editor = {Wijaya, D.D.},
  year = {2019},
  journal = {E3S Web of Conferences},
  volume = {94},
  pages = {02002},
  issn = {2267-1242},
  doi = {10.1051/e3sconf/20199402002},
  abstract = {In smartphones several sensors and receivers are embedded which enable positioning in Locationbased Services and other navigation applications. They include GNSS receivers and Wireless Fidelity (WiFi) cards as well as inertial sensors, such as accelerometers, gyroscope and magnetometer. In this paper, indoor Wi-Fi positioning is studied based on trilateration. Three methods are investigated which are a resection, a calculation of the center of gravity point and a differential approach. The first approach is a commonly employed resection using the ranges to the Wi-Fi Access Points (APs) as radii and intersect the circles around the APs. In the second method, the center of gravity in a triangle of APs is calculated with weighting of the received signal strength (RSS) of the Wi-Fi signals. The third approach is developed by analogy to Differential GNSS (DGNSS) and therefore termed Differential Wi-Fi (DWi-Fi). Its advantage is that a real-time modeling of the temporal RSS variations and fluctuations is possible. For that purpose, reference stations realized by low-cost Raspberry Pi units are deployed which serve at the same time as APs. The experiments conducted in a laboratory and entrance of an office building showed that position deviations from the ground truth of around 2 m are achievable with the second and third method. Thereby the positioning accuracies depend mainly on the geometrical point location in the triangle of APs and reference stations and the RSS scan duration.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\597K3JP7\\Retscher e.a. - 2019 - Trilateration Approaches for Indoor Wi-Fi Position.pdf}
}

@book{richards_software_2015,
  title = {Software Architecture Patterns},
  author = {Richards, Mark},
  year = {2015},
  volume = {4},
  publisher = {{O'Reilly Media, Incorporated 1005 Gravenstein Highway North, Sebastopol, CA~\ldots}}
}

@article{richards_software_nodate,
  title = {Software {{Architecture Patterns}}},
  author = {Richards, Mark},
  pages = {54},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\99YAXFHL\\Richards - Software Architecture Patterns.pdf}
}

@article{rios_unifying_2022,
  title = {A Unifying Framework for the Systematic Analysis of {{Git}} Workflows},
  author = {R{\'i}os, Julio C{\'e}sar Cort{\'e}s and Embury, Suzanne M. and Eraslan, Sukru},
  year = {2022},
  journal = {Information and Software Technology},
  volume = {145},
  pages = {106811},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2021.106811},
  abstract = {Context: Git is a popular distributed version control system that provides flexibility and robustness for software development projects. Several workflows have been proposed to codify the way project contributors work collaboratively with Git. Some workflows are highly prescriptive while others allow more leeway but do not provide the same level of code quality assurance, thus, preventing their comparison to determine the most suitable for a specific set of requirements, or to ascertain if a workflow is being properly followed. Objective: In this paper, we propose a novel feature-based framework for describing Git workflows, based on a study of 26 existing instances. The framework enables workflows' comparison, to discern how, and to what extent, they exploit Git capabilities for collaborative software development. Methods: The framework uses feature-based modelling to map Git capabilities, regularly expressed as contribution guidelines, and a set of features that can be impartially applied to all the workflows considered. Through this framework, each workflow was characterised based on their publicly available descriptions. The characterisations were then vectorised and processed using hierarchical clustering to determine workflows' similarities and to identify which features are most popular, and more relevant for discriminatory purposes. Results: Comparative analysis evidenced that some workflows claiming to be closely related, when described and then characterised, turned out to have more differences than similarities. The analysis also showed that most workflows focus on the branching and code integration strategies, whilst others emphasise subtle differences from other popular workflows or describe a specific development route and are, thus, widely reused. Conclusion: The characterisation and clustering analysis demonstrated that our framework can be used to compare and analyse Git workflows.},
  keywords = {Branching,Feature-based modelling,Git,Version control,Workflows}
}

@article{rivest_method_nodate,
  title = {A {{Method}} for {{Obtaining Digital Signatures}} and {{Public-Key Cryptosystems}}},
  author = {Rivest, R L and Shamir, A and Adleman, L},
  pages = {15},
  abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: 1. Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\EFZ7PYGM\\Rivest e.a. - A Method for Obtaining Digital Signatures and Publ.pdf}
}

@misc{rob_kwok_best_2016,
  title = {Best {{Practice}} \#3: {{Reduce App Load Time}} to {{Two}} Seconds to {{Increase Engagement}}},
  author = {{Rob Kwok}},
  year = {2016},
  month = aug
}

@article{rodriguez-barroso_survey_2023,
  title = {Survey on {{Federated Learning Threats}}: Concepts, Taxonomy on Attacks and Defences, Experimental Study and Challenges},
  shorttitle = {Survey on {{Federated Learning Threats}}},
  author = {{Rodr{\'i}guez-Barroso}, Nuria and L{\'o}pez, Daniel Jim{\'e}nez and Luz{\'o}n, M. Victoria and Herrera, Francisco and {Mart{\'i}nez-C{\'a}mara}, Eugenio},
  year = {2023},
  month = feb,
  journal = {Information Fusion},
  volume = {90},
  eprint = {2201.08135},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {148--173},
  issn = {15662535},
  doi = {10.1016/j.inffus.2022.09.011},
  abstract = {Federated learning is a machine learning paradigm that emerges as a solution to the privacy-preservation demands in artificial intelligence. As machine learning, federated learning is threatened by adversarial attacks against the integrity of the learning model and the privacy of data via a distributed approach to tackle local and global learning. This weak point is exacerbated by the inaccessibility of data in federated learning, which makes harder the protection against adversarial attacks and evidences the need to furtherance the research on defence methods to make federated learning a real solution for safeguarding data privacy. In this paper, we present an extensive review of the threats of federated learning, as well as as their corresponding countermeasures, attacks versus defences. This survey provides a taxonomy of adversarial attacks and a taxonomy of defence methods that depict a general picture of this vulnerability of federated learning and how to overcome it. Likewise, we expound guidelines for selecting the most adequate defence method according to the category of the adversarial attack. Besides, we carry out an extensive experimental study from which we draw further conclusions about the behaviour of attacks and defences and the guidelines for selecting the most adequate defence method according to the category of the adversarial attack. This study is finished leading to meditated learned lessons and challenges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WTV9LE2T\\Rodríguez-Barroso e.a. - 2023 - Survey on Federated Learning Threats concepts, ta.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\8R9W2SNA\\2201.html}
}

@book{rodriguez-bustos_how_2012,
  title = {How {{Distributed Version Control Systems}} Impact Open Source Software Projects},
  author = {{Rodriguez-Bustos}, Christian and Aponte, Jairo},
  year = {2012},
  month = jun,
  pages = {39},
  doi = {10.1109/MSR.2012.6224297},
  abstract = {Centralized Version Control Systems have been used by many open source projects for a long time. However, in recent years several widely-known projects have migrated their repositories to Distributed Version Control Systems, such as Mercurial, Bazaar, and Git. Such systems have technical features that allow contributors to work in new ways, as various different workflows are possible. We plan to study this migration process to assess how developers' organization and their contributions are affected. As a first step, we present an analysis of the Mozilla repositories, which migrated from CVS to Mercurial in 2007. This analysis reveals both expected and unexpected aspects of the contributors' activities.}
}

@article{rogers_method_nodate,
  title = {{{METHOD AND APPARATUS FOR VERIFICATION OF A COMPUTER USERS IDENTIFICATION}}, {{BASED ON KEYSTROKE CHARACTERISTICS}}},
  author = {Rogers, Samuel J},
  pages = {24},
  abstract = {A method and apparatus for determining whether a user of a system is an authorized user or an imposter by examining the keystroke characteristics of the user. The authorized user initially enters a number of user training samples on a keyboard. The user training samples are then purified to eliminate training samples which are different from other training samples. The purification can be performed by a self-organizing neural network which has input thereto, authorized user training samples, or both authorized training samples and imposter training samples. The purified user training samples are then compared to a sample to be tested to determine whether the sample is from an authorized user or an imposter. The comparison of the purified samples with the sample to be tested can be performed by a neural network such as a back propagation trained network, an ADALINE unit, a distance method or a linear classifier, discriminate function, or piecewise linear classifier. The result of this testing step indicates whether the user is authorized or an imposter and the user can be granted or denied access to the system.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5H6VQH7M\\Rogers - 75 Inventors Marcus E. Brown, Tuscaloosa, Ala.,.pdf}
}

@incollection{rokach_clustering_2005,
  title = {Clustering Methods},
  booktitle = {Data Mining and Knowledge Discovery Handbook},
  author = {Rokach, Lior and Maimon, Oded},
  year = {2005},
  pages = {321--352},
  publisher = {{Springer}}
}

@article{ronen_critical_nodate,
  title = {Critical {{Review}} of {{Imperfect Forward Secrecy}}},
  author = {Ronen, Eyal and Shamir, Adi},
  pages = {6},
  abstract = {We reviewed the claims made by Adrian et al., in [1] that precomputation of specific DH groups provides the NSA with the ability of mass surveillance of Internet communications and passively eavesdropping to large percentage of IPSec and TLS connections. We have independently reached essentially the same conclusions as Wouters in [2], namely, that the success rate of a hypothetical NSA attack on IPSec, would be much lower than the original estimate which appeared in [1]. More interestingly, we also checked the claimed statistics in [1] related to HTTPS connections (which was not checked by Wouters). These connections use the TLS protocol (or its predecessor SSL) to securely access HTTP servers on the Internet, and is of great interest to intelligence services since it protects access to search engines (such as GOOGLE), to email (such as GMAIL), to social networks (such as FACEBOOK), to financial information (such as CITIBANK and VISA) and to various services (such as ordering books on AMAZON or reserving airline tickets on EXPEDIA). Our independent tests show that even massive precomputation applied to all DH groups will have very limited success in passively breaking interesting HTTPS connections, which are used to access the most popular sites on the web. We have also found several methodological problems in the current implementation and interpretation of Internet wide HTTPS scans by based on the ZMap scanning software [3] used for generating the statistics shown in [1] and in the DROWN attack paper by Aviram et al., [4]. For example not implementing support of URL redirection caused many sites to be excluded from the scans, while not implementing host name validation cause mis-configured sites to be considered as valid sites that will be trusted by browsers. We have also reviewed the Snowden documents referenced by [1]. In those and other Snowden documents we have found clues that the NSA use other attack vectors to decrypt IPSec and TLS traffic. The recent Juniper and Cisco back-door revelations support this hypothesis.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C4G9KS8G\\Ronen en Shamir - Critical Review of Imperfect Forward Secrecy.pdf}
}

@inproceedings{rossi_continuous_2016,
  title = {Continuous Deployment of Mobile Software at Facebook (Showcase)},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Rossi, Chuck and Shibley, Elisa and Su, Shi and Beck, Kent and Savor, Tony and Stumm, Michael},
  year = {2016},
  month = nov,
  pages = {12--23},
  publisher = {{ACM}},
  address = {{Seattle WA USA}},
  doi = {10.1145/2950290.2994157},
  abstract = {Continuous deployment is the practice of releasing software updates to production as soon as it is ready, which is receiving increased adoption in industry. The frequency of updates of mobile software has traditionally lagged the state of practice for cloud-based services for a number of reasons. Mobile versions can only be released periodically. Users can choose when and if to upgrade, which means that several different releases coexist in production. There are hundreds of Android hardware variants, which increases the risk of having errors in the software being deployed.},
  isbn = {978-1-4503-4218-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QJUXVRR5\\Rossi e.a. - 2016 - Continuous deployment of mobile software at facebo.pdf}
}

@inproceedings{rossi_continuous_2016-1,
  title = {Continuous Deployment of Mobile Software at Facebook (Showcase)},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Rossi, Chuck and Shibley, Elisa and Su, Shi and Beck, Kent and Savor, Tony and Stumm, Michael},
  year = {2016},
  month = nov,
  pages = {12--23},
  publisher = {{ACM}},
  address = {{Seattle WA USA}},
  doi = {10.1145/2950290.2994157},
  abstract = {Continuous deployment is the practice of releasing software updates to production as soon as it is ready, which is receiving increased adoption in industry. The frequency of updates of mobile software has traditionally lagged the state of practice for cloud-based services for a number of reasons. Mobile versions can only be released periodically. Users can choose when and if to upgrade, which means that several different releases coexist in production. There are hundreds of Android hardware variants, which increases the risk of having errors in the software being deployed.},
  isbn = {978-1-4503-4218-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8KXM3ENV\\Rossi e.a. - 2016 - Continuous deployment of mobile software at facebo.pdf}
}

@inproceedings{rother_grabcut:_2004,
  title = {"{{GrabCut}}": {{Interactive Foreground Extraction Using Iterated Graph Cuts}}},
  shorttitle = {"{{GrabCut}}"},
  booktitle = {{{ACM SIGGRAPH}} 2004 {{Papers}}},
  author = {Rother, Carsten and Kolmogorov, Vladimir and Blake, Andrew},
  year = {2004},
  series = {{{SIGGRAPH}} '04},
  pages = {309--314},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1186562.1015720},
  abstract = {The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for "border matting" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.},
  keywords = {Alpha Matting,Foreground extraction,Graph Cuts,Image Editing,Interactive Image Segmentation}
}

@article{roy_programming_nodate,
  title = {Programming {{Paradigms}} for {{Dummies}}: {{What Every Programmer Should Know}}},
  author = {Roy, Peter Van},
  pages = {39},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9967RYDM\\Roy - Programming Paradigms for Dummies What Every Prog.pdf}
}

@misc{saniya_khalsa_dynamo_nodate,
  type = {Business},
  title = {Dynamo Db  Pros and Cons},
  author = {Saniya Khalsa},
  year = {07:32:07 UTC}
}

@misc{sanjay_rajak_api_2017,
  title = {{API Latency vs Response time}},
  author = {{Sanjay Rajak}},
  year = {2017},
  month = jul,
  journal = {API latency vs Response time},
  abstract = {Since long i have been asked to show API performance numbers and how is my team working on tasks to improve API response time. I went through few case studies and examples. I think i should share the overview what i found so far about API response time.},
  howpublished = {https://medium.com/@sanjay.rajak/api-latency-vs-response-time-fe87ef71b2f2},
  langid = {Engels}
}

@article{sarni_relationship_2006,
  title = {Relationship between Waist Circumference and Nutritional Status, Lipid Profile and Blood Pressure in Low Socioeconomic Level Pre-School Children},
  author = {Sarni, Roseli Saccardo and de Souza, Fab{\'i}ola Isabel Suano and Schoeps, Denise de Oliveira and Catherino, Priscila and de Oliveira, Maria Carolina Cozzi Pires and Pessotti, Cristiane F{\'e}lix Ximenes and Kochi, Cristiane and Colugnati, Fernando Antonio Basile},
  year = {2006},
  month = aug,
  journal = {Arquivos Brasileiros de Cardiologia},
  volume = {87},
  number = {2},
  pages = {153--158},
  issn = {0066-782X},
  doi = {10.1590/S0066-782X2006001500013},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GJ4U8N6U\\Sarni e.a. - 2006 - Relationship between waist circumference and nutri.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\78MWDMBG\\scielo.html}
}

@incollection{schach_notitle_2011,
  shorttitle = {The {{Object-Oriented Paradigm}}},
  booktitle = {Object-Oriented and Classical Software Engineering},
  author = {Schach, Stephen R.},
  year = {2011},
  edition = {8th ed},
  pages = {18},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  isbn = {978-0-07-337618-9},
  langid = {english},
  lccn = {QA76.758 .S318 2011},
  keywords = {C++ (Computer program language),Object-oriented programming (Computer science),Software engineering,UML (Computer science)},
  annotation = {OCLC: ocn477254661}
}

@article{schaeffer_forecasting_2020,
  title = {{Forecasting client retention \textemdash{} A machine-learning approach}},
  author = {Schaeffer, Satu Elisa and Sanchez, Sara Veronica Rodriguez},
  year = {2020},
  month = jan,
  volume = {52},
  number = {Jan2020},
  pages = {9},
  issn = {0969-6989},
  abstract = {In the age of big data, companies store practically all data on any client transaction. Making use of this data is commonly done with machine-learning techniques so as to turn it into information that can be used to drive business decisions. Our interest lies in using data on prepaid unitary services in a business-to-business setting to forecast client retention: whether a particular client is at risk of being lost before they cease being clients. The purpose of such a forecast is to provide the company with an opportunity to reach out to such clients as an effort to ensure their retention. We work with monthly records of client transactions: each client is represented as a series of purchases and consumptions. We vary (1) the length of the time period used to make the forecast, (2) the length of a period of inactivity after which a client is assumed to be lost, and (3) how far in advance the forecast is made. Our experimental work finds that current machine-learning techniques able to adequately predict, well in advance, which clients will be lost. This knowledge permits a company to focus marketing efforts on such clients as early as three months in advance.},
  langid = {Engels}
}

@article{schaffer_overfitting_1993,
  title = {Overfitting Avoidance as Bias},
  author = {Schaffer, Cullen},
  year = {1993},
  month = feb,
  journal = {Machine Learning},
  volume = {10},
  number = {2},
  pages = {153--178},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993504},
  abstract = {Strategies for increasing predictive accuracy through selective pruning have been widely adopted by researchersin decisiontree induction.It is easyto get the impressionfrom researchreportsthat there are statistical reasons for believingthat these overfitting avoidance strategiesdo increaseaccuracy and that, as a research community, we are making progress toward developing powerful, general methods for guarding against overfitting in inducing decision trees. In fact, any overfitting avoidancestrategy amounts to a form of bias and, as such, may degrade performance instead of improvingit. If pruning methods have often proven successfulin empirical tests, this is due, not to the methods, but to the choice of test problems. As examples in this article illustrate, overfittingavoidancestrategiesare not better or worse, but only more or less appropriate to specific application domains. We are not--and cannot be--making progress toward methods both powerful and general.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NXKLNAEZ\\Schaffer - 1993 - Overfitting avoidance as bias.pdf}
}

@article{schratz_hyperparameter_2019,
  title = {Hyperparameter Tuning and Performance Assessment of Statistical and Machine-Learning Algorithms Using Spatial Data},
  author = {Schratz, Patrick and Muenchow, Jannes and Iturritxa, Eugenia and Richter, Jakob and Brenning, Alexander},
  year = {2019},
  month = aug,
  journal = {Ecological Modelling},
  volume = {406},
  pages = {109--120},
  issn = {0304-3800},
  doi = {10.1016/j.ecolmodel.2019.06.002},
  abstract = {While the application of machine-learning algorithms has been highly simplified in the last years due to their well-documented integration in commonly used statistical programming languages (such as R or Python), there are several practical challenges in the field of ecological modeling related to unbiased performance estimation. One is the influence of spatial autocorrelation in both hyperparameter tuning and performance estimation. Grouped cross-validation strategies have been proposed in recent years in environmental as well as medical contexts to reduce bias in predictive performance. In this study we show the effects of spatial autocorrelation on hyperparameter tuning and performance estimation by comparing several widely used machine-learning algorithms such as boosted regression trees (BRT), k-nearest neighbor (KNN), random forest (RF) and support vector machine (SVM) with traditional parametric algorithms such as logistic regression (GLM) and semi-parametric ones like generalized additive models (GAM) in terms of predictive performance. Spatial and non-spatial cross-validation methods were used to evaluate model performances aiming to obtain bias-reduced performance estimates. A detailed analysis on the sensitivity of hyperparameter tuning when using different resampling methods (spatial/non-spatial) was performed. As a case study the spatial distribution of forest disease (Diplodia sapinea) in the Basque Country (Spain) was investigated using common environmental variables such as temperature, precipitation, soil and lithology as predictors. Random Forest (mean Brier score estimate of 0.166) outperformed all other methods with regard to predictive accuracy. Though the sensitivity to hyperparameter tuning differed between the ML algorithms, there were in most cases no substantial differences between spatial and non-spatial partitioning for hyperparameter tuning. However, spatial hyperparameter tuning maintains consistency with spatial estimation of classifier performance and should be favored over non-spatial hyperparameter optimization. High performance differences (up to 47\%) between the bias-reduced (spatial cross-validation) and overoptimistic (non-spatial cross-validation) cross-validation settings showed the high need to account for the influence of spatial autocorrelation. Overoptimistic performance estimates may lead to false actions in ecological decision making based on biased model predictions.},
  langid = {english},
  keywords = {Hyperparameter tuning,Machine-learning,Spatial autocorrelation,Spatial cross-validation,Spatial modeling},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\3ZR8J7Q4\\S0304380019302145.html}
}

@article{schrider_supervised_2018,
  title = {Supervised {{Machine Learning}} for {{Population Genetics}}: {{A New Paradigm}}},
  shorttitle = {Supervised {{Machine Learning}} for {{Population Genetics}}},
  author = {Schrider, Daniel R. and Kern, Andrew D.},
  year = {2018},
  month = apr,
  journal = {Trends in Genetics},
  volume = {34},
  number = {4},
  pages = {301--312},
  issn = {0168-9525},
  doi = {10.1016/j.tig.2017.12.005},
  abstract = {As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning (ML). We review the fundamentals of ML, discuss recent applications of supervised ML to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised ML is an important and underutilized tool that has considerable potential for the world of evolutionary genomics.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JHLNB5QW\\Schrider en Kern - 2018 - Supervised Machine Learning for Population Genetic.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\YFK3H66W\\S0168952517302251.html}
}

@article{seong-seob_hwang_account-sharing_2009,
  title = {Account-{{Sharing Detection Through Keystroke Dynamics Analysis}}},
  author = {{Seong-seob Hwang} and {Hyoung-joo Lee} and {Sungzoon Cho}},
  year = {2009///Winter2009/2010},
  journal = {International Journal of Electronic Commerce},
  volume = {14},
  number = {2},
  pages = {109--125},
  issn = {10864415},
  doi = {10.2753/JEC1086-4415140204},
  abstract = {Account sharing refers to a situation where multiple individuals share a Web site account to avoid paying a fee or providing personal information. As a result of account sharing, service providers lose revenue, underestimate membership, and have impaired understanding of their customers. A generic framework for detecting account sharing is proposed, using keystroke dynamics. Starting with the observation that a user's keystroke patterns are consistent and distinct from those of other individuals, it is assumed that each user's keystroke patterns form a "cluster" in Euclidean space. The number of sharers can be estimated by the number of clusters. In this paper, the "optimal" number of clusters is estimated based on the Bayesian model-selection framework with Gaussian mixture models obtained using the variational Bayesian approach. In a case study involving 25 passwords and 16 users, the proposed approach performed well in "sharing detection," with a 2 percent false alarm rate, a 2 percent miss rate, and a "total user estimation" error of 7 percent. The proposed approach is viable and merits further investigation.},
  keywords = {Account sharing,BAYESIAN analysis,biometrics,clustering,CODE division multiple access,GAUSSIAN processes,keystroke dynamics,PASSWORDS (Computers),typing pattern,USER charges,Web site account management,WEBSITE access control}
}

@incollection{seoung-hyeon_lee_journal_2014,
  title = {{Journal of Supercomputing}},
  booktitle = {{Hybrid app security protocol for high speed mobile communication}},
  author = {{Seoung-Hyeon Lee} and {Young-Hyuk Kim} and {Jae-Kwang Lee} and {Deok Gyu Lee}},
  year = {2014},
  month = jul,
  number = {72:1715\textendash 1739},
  pages = {1732--1733},
  publisher = {{Journal of Supercomputing}},
  langid = {Engels}
}

@article{serban_hierarchical_2008,
  title = {Hierarchical {{Adaptive Clustering}}},
  author = {{\c S}erban, Gabriela and C{\^a}mpan, Alina},
  year = {2008},
  month = mar,
  journal = {Informatica},
  volume = {19},
  number = {1},
  pages = {101--112},
  publisher = {{IOS Press}},
  issn = {08684952},
  abstract = {This paper studies an adaptive clustering problem. We focus on re-clustering an object set, previously clustered, when the feature set characterizing the objects increases. We propose an adaptive clustering method based on a hierarchical agglomerative approach, Hierarchical Adaptive Clustering (HAC), that adjusts the partitioning into clusters that was established by applying the hierarchical agglomerative clustering algorithm (HACA) (Han and Kamber, 2001) before the feature set changed. We aim to reach the result more efficiently than running HACA again from scratch on the feature-extended object set. Experiments testing the method's efficiency and a practical distributed systems problem in which the HAC method can be efficiently used (the problem of adaptive horizontal fragmentation in object oriented databases) are also reported.},
  keywords = {adaptive clustering,ALGORITHMS,CLUSTER analysis (Statistics),data mining,DATABASES,DOCUMENT clustering,hierarchical agglomerative clustering,INFORMATION storage \& retrieval systems,OBJECT-oriented databases}
}

@article{shahin_continuous_2017,
  title = {Continuous {{Integration}}, {{Delivery}} and {{Deployment}}: {{A Systematic Review}} on {{Approaches}}, {{Tools}}, {{Challenges}} and {{Practices}}},
  shorttitle = {Continuous {{Integration}}, {{Delivery}} and {{Deployment}}},
  author = {Shahin, Mojtaba and Ali Babar, Muhammad and Zhu, Liming},
  year = {2017},
  journal = {IEEE Access},
  volume = {5},
  pages = {3909--3943},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2017.2685629},
  abstract = {Continuous practices, i.e., continuous integration, delivery, and deployment, are the software development industry practices that enable organizations to frequently and reliably release new features and products. With the increasing interest in the literature on continuous practices, it is important to systematically review and synthesize the approaches, tools, challenges, and practices reported for adopting and implementing continuous practices. This paper aimed at systematically reviewing the state of the art of continuous practices to classify approaches and tools, identify challenges and practices in this regard, and identify the gaps for future research. We used the systematic literature review method for reviewing the peer-reviewed papers on continuous practices published between 2004 and June 1, 2016. We applied the thematic analysis method for analyzing the data extracted from reviewing 69 papers selected using predefined criteria. We have identified 30 approaches and associated tools, which facilitate the implementation of continuous practices in the following ways: (1) reducing build and test time in continuous integration (CI); (2) increasing visibility and awareness on build and test results in CI; (3) supporting (semi-) automated continuous testing; (4) detecting violations, flaws, and faults in CI; (5) addressing security and scalability issues in deployment pipeline; and (6) improving dependability and reliability of deployment process. We have also determined a list of critical factors, such as testing (effort and time), team awareness and transparency, good design principles, customer, highly skilled and motivated team, application domain, and appropriate infrastructure that should be carefully considered when introducing continuous practices in a given organization. The majority of the reviewed papers were validation (34.7\%) and evaluation (36.2\%) research types. This paper also reveals that continuous practices have been successfully applied to both greenfield and maintenance projects. Continuous practices have become an important area of software engineering research and practice. While the reported approaches, tools, and practices are addressing a wide range of challenges, there are several challenges and gaps, which require future research work for improving the capturing and reporting of contextual information in the studies reporting different aspects of continuous practices; gaining a deep understanding of how software-intensive systems should be (re-) architected to support continuous practices; and addressing the lack of knowledge and tools for engineering processes of designing and running secure deployment pipelines.},
  keywords = {Bibliographies,continuous delivery,continuous deployment,Continuous integration,continuous software engineering,empirical software engineering,Organizations,Production,Software,Software engineering,systematic literature review,Systematics,Testing},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CGTI8M5R\\Shahin e.a. - 2017 - Continuous Integration, Delivery and Deployment A.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\3DVZBLWG\\7884954.html}
}

@inproceedings{shang_bridging_2012,
  title = {Bridging the Divide between Software Developers and Operators Using Logs},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Shang, Weiyi},
  year = {2012},
  month = jun,
  pages = {1583--1586},
  publisher = {{IEEE}},
  address = {{Zurich}},
  doi = {10.1109/ICSE.2012.6227031},
  abstract = {There is a growing gap between the software development and operation worlds. Software developers rarely divulge development knowledge about the software to operators, while operators rarely communicate field knowledge to developers. To improve the quality and reduce the operational cost of large-scale software systems, bridging the gap between these two worlds is essential. This thesis proposes the use of logs as mechanism to bridge the gap between these two worlds. Logs are messages generated from statements inserted by developers in the source code and are often used by operators for monitoring the field operation of a system. However, the rich knowledge in logs has not yet been fully used because of their non-structured nature, their large scale, and the use of the ad hoc log analysis techniques. Through case studies on large commercial and open source systems, we plan to demonstrate the value of logs as a tool to support developers and operators.},
  isbn = {978-1-4673-1066-6 978-1-4673-1067-3},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\PAX8SW4I\\Shang - 2012 - Bridging the divide between software developers an.pdf}
}

@article{sharma_complete_2015,
  title = {A Complete Survey on Software Architectural Styles and Patterns},
  author = {Sharma, Anubha and Kumar, Manoj and Agarwal, Sonali},
  year = {2015},
  journal = {Procedia Computer Science},
  volume = {70},
  pages = {16--28},
  publisher = {{Elsevier}},
  issn = {1877-0509}
}

@article{sharma_leveraging_2021,
  title = {Leveraging the Power of Quantum Computing for Breaking {{RSA}} Encryption},
  author = {Sharma, Moolchand and Choudhary, Vikas and Bhatia, R. S. and Malik, Sahil and Raina, Anshuman and Khandelwal, Harshit},
  year = {2021},
  month = apr,
  journal = {Cyber-Physical Systems},
  volume = {7},
  number = {2},
  pages = {73--92},
  issn = {2333-5777, 2333-5785},
  doi = {10.1080/23335777.2020.1811384},
  abstract = {Encryption is the process of securing confidential data that bars a third party's access to the information.RSA encryption utilises the property of complexity classes wherein the pro\- blem of prime integer factorization lies inside the NonPolynomial time (NP-Hard) class, which makes it impervious to classical computers. Since it is so hard to break even for a computer, it becomes important to do encryption for all the secure transactions. Although it lies outside the capabilities of traditional computing, the recent developments in the field of quantum computing can be utilised to break RSA Encryption. The approach involves mapping of qubits used in a quantum machine to a constraint satisfaction problem (CSP) and then using them to check for factors. This consists of the use of a Multiplicative Boolean circuit in which the qubits utilised by the machine replaces the variables. These Qubits are then mapped as per the gates involved, and the factorization problem is thus transformed into a CSP pro\- blem, through which, the factors can be easily found. Once known, these factors can be used to calculate the public and private keys effectively breaking the encryption security. We provide a novel approach to highlight the importance of developing Post-Quantum cryptography techniques for pro\- viding a secure channel of communication.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UVI5FYVA\\Sharma e.a. - 2021 - Leveraging the power of quantum computing for brea.pdf}
}

@book{sheyyab_managing_2019,
  title = {Managing {{Quality Assurance Challenges}} of {{DevOps}} through {{Analytics}}},
  author = {Sheyyab, Mahmoud},
  year = {2019},
  month = oct,
  abstract = {ABSTRACT DevOps is an intermarriage between developmental practices and operational modalities. The methodology employs the practices of continuous integration and delivery and places the deployment pipeline as the main requirement to automate, deliver and operate software in a robust way, without compromising on the quality in the software development process. Over time, many systems and tools have been developed to implement the deployment pipeline and support the continuous delivery process. A pipeline splits the process of software delivery into various stages. Each stage is designed to verify the quality of new features from a new perspective to attest to the functionality and prevent either small or big errors from impacting the end users. The pipeline must provide a response and feedback loop to the concerned team and provide a window into the flow of changes that takes place. However, there is no clear rule to define what goes into a pipeline. This paper reviews the challenges of quality assurance of DevOps and provides tentative recommendations to deal with quality issues. Our proposed pipeline with analytic features is expected to provide accurate metrics on a real-time basis.}
}

@book{sheyyab_managing_2019-1,
  title = {Managing {{Quality Assurance Challenges}} of {{DevOps}} through {{Analytics}}},
  author = {Sheyyab, Mahmoud},
  year = {2019},
  month = oct,
  abstract = {ABSTRACT DevOps is an intermarriage between developmental practices and operational modalities. The methodology employs the practices of continuous integration and delivery and places the deployment pipeline as the main requirement to automate, deliver and operate software in a robust way, without compromising on the quality in the software development process. Over time, many systems and tools have been developed to implement the deployment pipeline and support the continuous delivery process. A pipeline splits the process of software delivery into various stages. Each stage is designed to verify the quality of new features from a new perspective to attest to the functionality and prevent either small or big errors from impacting the end users. The pipeline must provide a response and feedback loop to the concerned team and provide a window into the flow of changes that takes place. However, there is no clear rule to define what goes into a pipeline. This paper reviews the challenges of quality assurance of DevOps and provides tentative recommendations to deal with quality issues. Our proposed pipeline with analytic features is expected to provide accurate metrics on a real-time basis.}
}

@article{sidey-gibbons_machine_2019,
  title = {Machine Learning in Medicine: A Practical Introduction},
  shorttitle = {Machine Learning in Medicine},
  author = {{Sidey-Gibbons}, Jenni A. M. and {Sidey-Gibbons}, Chris J.},
  year = {2019},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {64},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0681-4},
  abstract = {Background: Following visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data. Methods: We demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N = 683) was randomly split into evaluation (n = 456) and validation (n = 227) samples. We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment. Results: The trained algorithms were able to classify cell nuclei with high accuracy (.94 - .96), sensitivity (.97 - .99), and specificity (.85 - .94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy=.97, sensitivity = .99, specificity = .95) when algorithms were arranged into a voting ensemble. Conclusions: We use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SNMLI2RP\\Sidey-Gibbons en Sidey-Gibbons - 2019 - Machine learning in medicine a practical introduc.pdf}
}

@article{singleton_economics_2016,
  title = {The {{Economics}} of {{Microservices}}},
  author = {Singleton, Andy},
  year = {2016},
  journal = {IEEE cloud computing},
  volume = {3},
  number = {5},
  pages = {16--20},
  publisher = {{IEEE}},
  address = {{PISCATAWAY}},
  issn = {2325-6095},
  doi = {10.1109/MCC.2016.109},
  abstract = {MICROSERVICES ARE A SOLUTION-PERHAPS THE ONLY SOLUTION-TO THE PROBLEM OF EFFICIENTLY BUILDING AND MANAGING COMPLEX SOFTWARE SYSTEMS.;Microservices address the problem of efficiently building and managing complex software systems. For medium-sized systems, they can deliver cost reduction, quality improvement, agility, and decreased time to market. For large cloud systems, they fundamentally change the rules of the game. However, although microservices approaches offer substantial benefits, a microservices architecture requires extra machinery, which can impose substantial costs. This column explores both the costs and benefits of a microservices architecture.;},
  langid = {english},
  keywords = {Cloud computing,cloud economics,Complexity theory,Computer architecture,Computer Science,Computer Science; Information Systems,Computer Science; Theory \& Methods,Economics,Legacy systems,microservices architecture,Science \& Technology,Service computing,Technology}
}

@misc{stebila_post-quantum_nodate,
  title = {Post-Quantum {{TLS}} without Handshake Signatures},
  author = {Stebila, Douglas, Peter Schwabe and Wiggers, Thom},
  abstract = {We present KEMTLS, an alternative to the TLS 1.3 handshake that uses key-encapsulation mechanisms (KEMs) instead of signatures for server authentication. Among existing post-quantum candidates, signature schemes generally have larger public key/signature sizes compared to the public key/ciphertext sizes of KEMs: by using an IND-CCA-secure KEM for server authentication in post-quantum TLS, we obtain multiple benefits. A size-optimized post-quantum instantiation of KEMTLS requires less than half the bandwidth of a size-optimized post-quantum instantiation of TLS 1.3. In a speed-optimized instantiation, KEMTLS reduces the amount of server CPU cycles by almost 90\% compared to TLS 1.3, while at the same time reducing communication size, reducing the time until the client can start sending encrypted application data, and eliminating code for signatures from the server's trusted code base.Updated measurements with NIST Round-3 schemes as well as small fixes to the security sketch for mutual auth. Corrected version with correct ephemeral key exchange metrics.},
  keywords = {authentication protocols,key-encapsulation mechanisms,NIST PQC,post-quantum,public-key cryptography,TLS,Transport Layer Security}
}

@article{steinegger_overview_nodate,
  title = {Overview of a {{Domain-Driven Design Approach}} to {{Build Microservice-Based Applications}}},
  author = {Steinegger, Roland H and Giessler, Pascal and Hippchen, Benjamin and Abeck, Sebastian},
  pages = {9},
  abstract = {The current trend of building web applications using microservice architectures is based on the domain-driven design (DDD) concept, as described by Evans. Among practitioners, DDD is a widely accepted approach to building applications. Applying and extending the concepts and tasks of DDD is challenging because it lacks a software development process description and classification within existing software development process approaches. For these reasons, we provide a brief overview of a DDD-based software development process for building resource-oriented microservices that takes into consideration the requirements of the desired application. Following the widely accepted engineering approach suggested by Bru\textasciidieresis{} gge et al., the emphasis is on the analysis, design, implementation and testing phases. Furthermore, we classify DDD and microservice-based application into regular software development activities and software architecture concepts. After the process is described, it is applied to a case study in order to demonstrate its potential applications and limitations.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NT6ES9P3\\Steinegger e.a. - Overview of a Domain-Driven Design Approach to Bui.pdf}
}

@article{steinke_towards_2011,
  title = {Towards an {{Understanding}} of {{Web Application Security Threats}} and {{Incidents}}},
  author = {Steinke, Gerhard and Tundrea, Emanuel and Kelly, Kenmoro},
  year = {2011},
  month = oct,
  journal = {Journal of Information Privacy and Security},
  volume = {7},
  pages = {54--69},
  doi = {10.1080/15536548.2011.10855923},
  abstract = {This paper examines a variety of sources that provide web application security vulnerabilities and incident data. In particular, the research tracks the impact of SQL Injection, Cross-Site Scripting and Cross-Site Request Forgery vulnerabilities. A comparison of vulnerability data versus attacks that have actually resulted in data compromises is studied to determine how the type of vulnerabilities relate to actual methods used to steal data. The paper concludes with recommendations for more secure web applications.}
}

@article{stephens_comparison_2014,
  title = {A Comparison of Supervised Classification Methods for the Prediction of Substrate Type Using Multibeam Acoustic and Legacy Grain-Size Data},
  author = {Stephens, David and Diesing, Markus},
  year = {2014},
  journal = {PloS one},
  volume = {9},
  number = {4},
  publisher = {{Public Library of Science}}
}

@book{strauss_basics_nodate,
  title = {Basics of Qualitative Research : Techniques and Procedures for Developing Grounded Theory},
  author = {Strauss, Anselm L and Corbin, Juliet M}
}

@book{strauss_basics_nodate-1,
  title = {Basics of Qualitative Research : Techniques and Procedures for Developing Grounded Theory},
  author = {Strauss, Anselm L and Corbin, Juliet M}
}

@article{su_outlier_2011,
  title = {Outlier Detection},
  author = {Su, Xiaogang and Tsai, Chih-Ling},
  year = {2011},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {1},
  number = {3},
  pages = {261--268},
  issn = {1942-4795},
  doi = {10.1002/widm.19},
  abstract = {Outlier detection is an area of research with a long history which has applications in many fields. This article provides a nontechnical and concise overview of the commonly used approaches for detecting outliers, including classical methods, new challenges posed by real-world massive data, and some of the key advances made in recent years. \textcopyright{} 2011 John Wiley \& Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 261\textendash 268 DOI: 10.1002/widm.19 This article is categorized under: Algorithmic Development {$>$} Scalable Statistical Methods Fundamental Concepts of Data and Knowledge {$>$} Motivation and Emergence of Data Mining Algorithmic Development {$>$} Statistics Technologies {$>$} Structure Discovery and Clustering},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8P3A6J34\\Su en Tsai - 2011 - Outlier detection.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\8REDUBK2\\widm.html}
}

@article{sugiyama_dimensionality_2007,
  title = {Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis},
  author = {Sugiyama, Masashi},
  year = {2007},
  journal = {Journal of machine learning research},
  volume = {8},
  number = {May},
  pages = {1027--1061}
}

@article{suykens_least_1999,
  title = {Least Squares Support Vector Machine Classifiers},
  author = {Suykens, Johan AK and Vandewalle, Joos},
  year = {1999},
  journal = {Neural processing letters},
  volume = {9},
  number = {3},
  pages = {293--300},
  publisher = {{Springer}},
  issn = {1370-4621}
}

@article{tai_soft_2007,
  title = {Soft {{Color Segmentation}} and {{Its Applications}}},
  author = {Tai, Yu-Wing and Jia, Jiaya and Tang, Chi-Keung},
  year = {2007},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {9},
  pages = {1520--1537},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2007.1168},
  abstract = {We propose an automatic approach to soft color segmentation, which produces soft color segments with an appropriate amount of overlapping and transparency essential to synthesizing natural images for a wide range of image-based applications. Although many state-of-the-art and complex techniques are excellent at partitioning an input image to facilitate deriving a semantic description of the scene, to achieve seamless image synthesis, we advocate a segmentation approach designed to maintain spatial and color coherence among soft segments while preserving discontinuities by assigning to each pixel a set of soft labels corresponding to their respective color distributions. We optimize a global objective function, which simultaneously exploits the reliability given by global color statistics and flexibility of local image compositing, leading to an image model where the global color statistics of an image is represented by a Gaussian Mixture Model (GMM), whereas the color of a pixel is explained by a local color mixture model where the weights are defined by the soft labels to the elements of the converged GMM. Transparency is naturally introduced in our probabilistic framework, which infers an optimal mixture of colors at an image pixel. To adequately consider global and local information in the same framework, an alternating optimization scheme is proposed to iteratively solve for the global and local model parameters. Our method is fully automatic and is shown to converge to a good optimal solution. We perform extensive evaluation and comparison and demonstrate that our method achieves good image synthesis results for image-based applications such as image matting, color transfer, image deblurring, and image colorization.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ICQPYEPL\\Tai e.a. - 2007 - Soft Color Segmentation and Its Applications.pdf}
}

@book{talia_data_2016,
  title = {Data {{Analysis}} in the {{Cloud}} : {{Models}}, {{Techniques}} and {{Applications}}},
  shorttitle = {Data {{Analysis}} in the {{Cloud}}},
  author = {Talia, Domenico and Marozzo, Fabrizio and Trunfio, Paolo},
  year = {2016},
  series = {Computer {{Science Reviews}} and {{Trends}}},
  publisher = {{Elsevier}},
  address = {{Amsterdam, Netherlands}},
  abstract = {Data Analysis in the Cloud introduces and discusses models, methods, techniques, and systems to analyze the large number of digital data sources available on the Internet using the computing and storage facilities of the cloud. Coverage includes scalable data mining and knowledge discovery techniques together with cloud computing concepts, models, and systems. Specific sections focus on map-reduce and NoSQL models. The book also includes techniques for conducting high-performance distributed analysis of large data on clouds. Finally, the book examines research trends such as Big Data pervasive computing, data-intensive exascale computing, and massive social network analysis.Introduces data analysis techniques and cloud computing conceptsDescribes cloud-based models and systems for Big Data analyticsProvides examples of the state-of-the-art in cloud data analysisExplains how to develop large-scale data mining applications on cloudsOutlines the main research trends in the area of scalable Big Data analysis},
  isbn = {978-0-12-802881-0},
  langid = {english},
  keywords = {Cloud computing,COMPUTERS / Data Science / General,COMPUTERS / Database Administration \& Management,COMPUTERS / Internet / General,COMPUTERS / Management Information Systems,COMPUTERS / Web / General,Data mining,Quantitative research}
}

@article{tasnim_autoencoder_2017,
  title = {Autoencoder for Wind Power Prediction},
  author = {Tasnim, Sumaira and Rahman, Ashfaqur and Oo, Amanullah Maung Than and Enamul Haque, Md.},
  year = {2017},
  month = dec,
  journal = {Renewables: Wind, Water, and Solar},
  volume = {4},
  number = {1},
  pages = {6},
  issn = {2198-994X},
  doi = {10.1186/s40807-017-0044-x},
  abstract = {Successful integration of renewable energy sources like wind power into smart grids largely depends on accurate prediction of power from these intermittent sources. Production of wind power cannot be controlled as the wind speed can vary based on weather conditions. Accurate prediction of wind power can assist smart grid that intelligently decides on the usage of alternative power sources based on demand forecast. Time series wind speed data are normally used for wind power prediction. In this paper, we have investigated the usage of a set of secondary features obtained using deep learning for wind power prediction. Deep learning is a special form on neural network that is capable of capturing the structural properties of time series data in terms of a set of numeric features. More precisely, we have designed a two-stage autoencoder (a particular type of deep learning) and incorporated the structural features into a prediction framework. Using the structural features, we have achieved as high as 12.63\% better prediction accuracy than traditionally used statistical features.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8UMZJJAD\\Tasnim e.a. - 2017 - Autoencoder for wind power prediction.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\TQDK4J4K\\s40807-017-0044-x.html}
}

@article{teh_keystroke_2010,
  title = {Keystroke Dynamics in Password Authentication Enhancement},
  author = {Teh, Pin Shen and Teoh, Andrew Beng Jin and Tee, Connie and Ong, Thian Song},
  year = {2010},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {37},
  number = {12},
  pages = {8618--8627},
  issn = {09574174},
  doi = {10.1016/j.eswa.2010.06.097},
  abstract = {This paper describes a novel technique to strengthen password authentication system by incorporating multiple keystroke dynamic information under a fusion framework. We capitalize four types of latency as keystroke feature and two methods to calculate the similarity scores between the two given latency. A two layer fusion approach is proposed to enhance the overall performance of the system to achieve near 1.401\% Equal Error Rate (EER). We also introduce two additional modules to increase the flexibility of the proposed system. These modules aim to accommodate exceptional cases, for instance, when a legitimate user is unable to provide his or her normal typing pattern due to reasons such as hand injury. \'O 2010 Elsevier Ltd. All rights reserved.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\FPAWUTD2\\Teh e.a. - 2010 - Keystroke dynamics in password authentication enha.pdf}
}

@article{teh_survey_2013,
  title = {A {{Survey}} of {{Keystroke Dynamics Biometrics}}},
  author = {Teh, Pin Shen and Teoh, Andrew Beng Jin and Yue, Shigang},
  year = {2013},
  month = nov,
  journal = {The Scientific World Journal},
  volume = {2013},
  issn = {2356-6140},
  doi = {10.1155/2013/408280},
  abstract = {Research on keystroke dynamics biometrics has been increasing, especially in the last decade. The main motivation behind this effort is due to the fact that keystroke dynamics biometrics is economical and can be easily integrated into the existing computer security systems with minimal alteration and user intervention. Numerous studies have been conducted in terms of data acquisition devices, feature representations, classification methods, experimental protocols, and evaluations. However, an up-to-date extensive survey and evaluation is not yet available. The objective of this paper is to provide an insightful survey and comparison on keystroke dynamics biometrics research performed throughout the last three decades, as well as offering suggestions and possible future research directions.},
  pmcid = {PMC3835878},
  pmid = {24298216},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\85HH9BCR\\Teh e.a. - 2013 - A Survey of Keystroke Dynamics Biometrics.pdf}
}

@book{test_test_nodate,
  title = {Test},
  author = {{test}, testje}
}

@techreport{tomla_tomla_2018,
  title = {{TOMLA Manual \textendash{} The Offline Multi-functional Language App}},
  author = {{TOMLA}},
  year = {2018},
  month = jul,
  pages = {2--3},
  langid = {engels}
}

@misc{truex_ldp-fed_2020,
  title = {{{LDP-Fed}}: {{Federated Learning}} with {{Local Differential Privacy}}},
  shorttitle = {{{LDP-Fed}}},
  author = {Truex, Stacey and Liu, Ling and Chow, Ka-Ho and Gursoy, Mehmet Emre and Wei, Wenqi},
  year = {2020},
  month = jun,
  number = {arXiv:2006.03637},
  eprint = {2006.03637},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {This paper presents LDP-Fed, a novel federated learning system with a formal privacy guarantee using local differential privacy (LDP). Existing LDP protocols are developed primarily to ensure data privacy in the collection of single numerical or categorical values, such as click count in Web access logs. However, in federated learning model parameter updates are collected iteratively from each participant and consist of high dimensional, continuous values with high precision (10s of digits after the decimal point), making existing LDP protocols inapplicable. To address this challenge in LDP-Fed, we design and develop two novel approaches. First, LDP-Fed's LDP Module provides a formal differential privacy guarantee for the repeated collection of model training parameters in the federated training of large-scale neural networks over multiple individual participants' private datasets. Second, LDP-Fed implements a suite of selection and filtering techniques for perturbing and sharing select parameter updates with the parameter server. We validate our system deployed with a condensed LDP protocol in training deep neural networks on public data. We compare this version of LDP-Fed, coined CLDP-Fed, with other state-of-the-art approaches with respect to model accuracy, privacy preservation, and system capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YPUHCDTN\\Truex e.a. - 2020 - LDP-Fed Federated Learning with Local Differentia.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\7KYXQWVT\\2006.html}
}

@article{tushar_jain_7_2017,
  title = {7 Pre-Launch Mobile App Performance Metrics to Measure},
  author = {{Tushar Jain}},
  year = {2017},
  month = apr,
  journal = {7 pre-launch mobile app performance metrics to measure}
}

@book{vanderplas_python_nodate,
  title = {Python {{Data Science Handbook}}},
  author = {VanderPlas, Jake},
  abstract = {For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data...},
  isbn = {978-1-4919-1205-8},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NLFCTR2L\\0636920034919.html}
}

@inproceedings{vanwinckelen_estimating_nodate,
  title = {On Estimating Model Accuracy with Repeated Cross-Validation},
  booktitle = {{{BeneLearn}} 2012: {{Proceedings}} of the 21st {{Belgian-Dutch Conference}} on {{Machine Learning}}},
  author = {Vanwinckelen, Gitte and Blockeel, Hendrik},
  year = {20120101},
  pages = {39--44},
  abstract = {Evaluation of predictive models is a ubiquitous task in machine learning and data mining. Cross-validation is often used as a means for evaluating models. There appears to be some confusion among researchers, however, about best practices for cross-validation, and about the interpretation of cross-validation results. In particular, repeated cross-validation is often advocated, and so is the reporting of standard deviations, confidence intervals, or an indication of "significance". In this paper, we argue that, under many practical circumstances, when the goal of the experiments is to see how well the model returned by a learner will perform in practice in a particular domain, repeated cross-validation is not useful, and the reporting of confidence intervals or significance is misleading. Our arguments are supported by experimental results.},
  collaborator = {De Baets, Bernard and Manderick, Bernard and Rademaker, Micha{\"e}l and Waegeman, Willem},
  isbn = {978-94-6197-044-2},
  langid = {english},
  keywords = {conditional prediction error,predictive model evaluation,repeated cross-validation}
}

@article{varun_chandola_anomaly_nodate,
  title = {Anomaly Detection: {{A}} Survey},
  author = {{Varun Chandola} and Banerjee, Arindam and Kumar, Vipin},
  journal = {ACM Computing Surveys},
  volume = {V41},
  number = {2009},
  issn = {0360-0300},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with}
}

@article{vasilopoulos_regression_2005,
  title = {Regression {{Analysis Revisited}}},
  author = {Vasilopoulos, Athanasios},
  year = 2005,
  journal = {Review of Business},
  volume = {26},
  number = {3},
  pages = {36--46},
  issn = {00346454},
  abstract = {Regression analysis is at the center of almost every forecasting technique, yet few people are comfortable with the regression methodology. We hope to improve the level of comfort with this article. Briefly we will discuss the theory behind the methodology and then outline a step-by-step procedure, which will allow almost everyone to construct a regression forecasting function for both the linear and multivariate cases. Also discussed, in addition to the model estimation mentioned above, is model testing (to establish significance) and the procedure by which the final regression equation is derived and retained to be used as the forecasting equation. Hand solutions are derived for two small-sample problems (one each for the linear and multivariate cases) and their solutions are compared to the MINITAB-derived solutions to establish confidence in the statistical tool, which is to be used exclusively for larger problems.},
  keywords = {ANALYSIS of variance,DECISION making,FORECASTING,MATHEMATICAL statistics,REGRESSION analysis}
}

@inproceedings{venkataraman_machine_2015,
  title = {Machine Learning Techniques for Building a Large Scale Production Ready Classifier},
  booktitle = {The {{Second International Conference}} on {{Data Mining}}, {{Internet Computing}}, and {{Big Data}} ({{BigData2015}})},
  author = {Venkataraman, Arthi},
  year = {2015},
  pages = {1}
}

@book{vernon_implementing_2013,
  title = {Implementing Domain-Driven Design},
  author = {Vernon, Vaughn},
  year = {2013},
  publisher = {{Addison-Wesley}},
  isbn = {0-13-303988-9}
}

@incollection{vernon_notitle_2013,
  booktitle = {Implementing Domain-Driven Design},
  author = {Vernon, Vaughn},
  year = {2013},
  publisher = {{Addison-Wesley}},
  isbn = {0-13-303988-9}
}

@inproceedings{villani_keystroke_2006,
  title = {Keystroke {{Biometric Recognition Studies}} on {{Long-Text Input}} under {{Ideal}} and {{Application-Oriented Conditions}}},
  author = {Villani, Mary and Tappert, Charles and Ngo, Giang and Simone, J. and Fort, H.St and Cha, Sung-Hyuk},
  year = {2006},
  month = jul,
  pages = {39--39},
  doi = {10.1109/CVPRW.2006.115},
  abstract = {A long-text-input keystroke biometric system was developed for applications such as identifying perpetrators of inappropriate e-mail or fraudulent Internet activity. A Java applet collected raw keystroke data over the Internet, appropriate long-text-input features were extracted, and a pattern classifier made identification decisions. Experiments were conducted on a total of 118 subjects using two input modes - copy and free-text input - and two keyboard types - desktop and laptop keyboards. Results indicate that the keystroke biometric can accurately identify an individual who sends inappropriate email (free text) if sufficient enrollment samples are available and if the same type of keyboard is used to produce the enrollment and questioned samples. For laptop keyboards we obtained 99.5\% accuracy on 36 users, which decreased to 97.9\% on a larger population of 47 users. For desktop keyboards we obtained 98.3\% accuracy on 36 users, which decreased to 93.3\% on a larger population of 93 users. Accuracy decreases significantly when subjects used different keyboard types or different input modes for enrollment and testing.},
  isbn = {978-0-7695-2646-1},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WCGVDRT9\\Villani e.a. - 2006 - Keystroke Biometric Recognition Studies on Long-Te.pdf}
}

@article{vincent_watersheds_1991,
  title = {Watersheds in Digital Spaces: An Efficient Algorithm Based on Immersion Simulations},
  shorttitle = {Watersheds in Digital Spaces},
  author = {Vincent, L. and Soille, P.},
  year = {1991},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {13},
  number = {6},
  pages = {583--598},
  issn = {0162-8828},
  doi = {10.1109/34.87344},
  abstract = {A fast and flexible algorithm for computing watersheds in digital gray-scale images is introduced. A review of watersheds and related motion is first presented, and the major methods to determine watersheds are discussed. The algorithm is based on an immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using of queue of pixel. It is described in detail provided in a pseudo C language. The accuracy of this algorithm is proven to be superior to that of the existing implementations, and it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images (and even to graphs) are straightforward. The algorithm is reported to be faster than any other watershed algorithm. Applications of this algorithm with regard to picture segmentation are presented for magnetic resonance (MR) imagery and for digital elevation models. An example of 3-D watershed is also provided.{$<<$}ETX{$>>$}},
  keywords = {Computational modeling,computerised picture processing,digital elevation models,Digital elevation models,digital gray-scale images,Digital images,Floods,Gray-scale,Image processing,Image segmentation,magnetic resonance imagery,Morphology,Oceans,picture segmentation,pseudo C language,Surfaces,watersheds},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VF527RAY\\87344.html}
}

@article{vincent_watersheds_1991-1,
  title = {Watersheds in {{Digital Spaces}}: {{An Efficient Algorithm Based}} on {{Immersion Simulations}}},
  shorttitle = {Watersheds in {{Digital Spaces}}},
  author = {Vincent, Luc and Soille, Pierre},
  year = {1991},
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {13},
  pages = {583--598},
  doi = {10.1109/34.87344},
  abstract = {In this paper, a fast and flexible algorithm for computing watersheds in digital grayscale images is introduced. A review of watersheds and related notion is first presented, and the major methods to determine watersheds are discussed. The present algorithm is based on an immersion process analogy, in which the flooding of the water in the picture is efficiently simulated using a queue of pixels. It is described in detail and provided in a pseudo C language. We prove the accuracy of this algorithm is superior to that of the existing implementations. Furthermore, it is shown that its adaptation to any kind of digital grid and its generalization to n-dimensional images and even to graphs are straightforward. In addition, its strongest point is that it is faster than any other watershed algorithm. Applications of this algorithm with regard to picture segmentation are presented for MR imagery and for digital elevation models. An example of 3-D watershed is also provided. Lastly, some ideas are given on how to solve complex segmentation tasks using watersheds on graphs. Zndex Terms-Algorithm, digital image, FIFO structure, graph, grid, mathematical morphology, picture segmentation, watersheds.},
  keywords = {Algorithm,Computer simulation,Digital elevation model,Digital image,FIFO (computing and electronics),Graph (discrete mathematics),Grayscale,Grid computing,Immersion (virtual reality),Mathematical morphology,Pixel,Spaces,Watershed (image processing)},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\K95ILDZM\\Vincent en Soille - 1991 - Watersheds in Digital Spaces An Efficient Algorit.pdf}
}

@misc{vladikk_serving_2013,
  type = {{Blog}},
  title = {{Serving Flask with Nginx}},
  author = {{vladikk}},
  year = {2013},
  month = sep,
  journal = {Vladikk},
  howpublished = {https://vladikk.com/2013/09/12/serving-flask-with-nginx-on-ubuntu/},
  langid = {Engels}
}

@article{wang_comprehensive_2020,
  title = {A {{Comprehensive Survey}} on {{Local Differential Privacy Toward Data Statistics}} and {{Analysis}} in {{Crowdsensing}}},
  author = {Wang, Teng and Zhang, Xuefeng and Feng, Jingyu and Yang, Xinyu},
  year = {2020},
  journal = {CoRR},
  volume = {abs/2010.05253},
  eprint = {2010.05253},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@misc{warburton_2021_2021,
  title = {The 2021 {{TLS Telemetry Report}}},
  author = {Warburton, David},
  year = {2021},
  month = oct,
  journal = {F5 Labs},
  abstract = {Creating an encrypted HTTPS website depends on a lot more than simply throwing a digital certificate at it and hoping for the best. In fact, Transport Layer Security (TLS) and HTTPS misconfigurations are now so commonplace that in the 2021 OWASP Top 10, Cryptographic Failures now comes in second place.},
  howpublished = {https://www.f5.com/labs/articles/threat-intelligence/the-2021-tls-telemetry-report},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ARA3E4TC\\the-2021-tls-telemetry-report.html}
}

@inproceedings{wasenmuller_precise_2015,
  title = {Precise and {{Automatic Anthropometric Measurement Extraction Using Template Registration}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{3D Body Scanning Technologies}}, {{Lugano}}, {{Switzerland}}, 27-28 {{October}} 2015},
  author = {Wasenmuller, Oliver and Peters, Jan C. and Golyanik, Vladislav and Stricker, Didier},
  year = {2015},
  month = oct,
  pages = {155--160},
  publisher = {{Hometrica Consulting - Dr. Nicola D'Apuzzo}},
  address = {{Lugano, Switzerland}},
  doi = {10.15221/15.155},
  abstract = {Anthropometric measures build the basis for many applications, such as custom clothing or biometric identity verification. Consequentially, the possibility to automatically extract them from human body scans is of high importance. In this paper we present a new approach based on landmarks and template registration. First, we propose a new method to define anthropometric measures once on a generic template using landmarks. After the initial definition the template can be registered against an individual body scan and the landmarks can be transferred to the scan using our second proposed algorithm. We apply our complete approach to real and synthetic human data and show that it outperforms the state-of-the-art for several measures.},
  isbn = {978-3-033-05270-3},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\KB9MIEDB\\Wasenmuller e.a. - 2015 - Precise and Automatic Anthropometric Measurement E.pdf}
}

@article{wei_machine_2019,
  title = {Machine Learning in Materials Science},
  author = {Wei, Jing and Chu, Xuan and Sun, Xiang-Yu and Xu, Kun and Deng, Hui-Xiong and Chen, Jigen and Wei, Zhongming and Lei, Ming},
  year = {2019},
  journal = {InfoMat},
  volume = {1},
  number = {3},
  pages = {338--358},
  issn = {2567-3165},
  doi = {10.1002/inf2.12028},
  abstract = {Traditional methods of discovering new materials, such as the empirical trial and error method and the density functional theory (DFT)-based method, are unable to keep pace with the development of materials science today due to their long development cycles, low efficiency, and high costs. Accordingly, due to its low computational cost and short development cycle, machine learning is coupled with powerful data processing and high prediction performance and is being widely used in material detection, material analysis, and material design. In this article, we discuss the basic operational procedures in analyzing material properties via machine learning, summarize recent applications of machine learning algorithms to several mature fields in materials science, and discuss the improvements that are required for wide-ranging application.},
  langid = {english},
  keywords = {data processing,deep learning,machine learning,modeling,validation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/inf2.12028},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7QWFVM4L\\inf2.html}
}

@incollection{weinberger_distance_2006,
  title = {Distance {{Metric Learning}} for {{Large Margin Nearest Neighbor Classification}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 18},
  author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence K.},
  editor = {Weiss, Y. and Sch{\"o}lkopf, B. and Platt, J. C.},
  year = {2006},
  pages = {1473--1480},
  publisher = {{MIT Press}},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2WRAHU67\\Weinberger e.a. - 2006 - Distance Metric Learning for Large Margin Nearest .pdf;C\:\\Users\\tjvan\\Zotero\\storage\\7L7ZT2QL\\2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.html}
}

@book{westby_git_2015,
  title = {Git for Teams},
  author = {Westby, Emma Jane Hogbin},
  year = {2015},
  edition = {First edition},
  publisher = {{O'Reilly Media, Inc}},
  address = {{Sebastopol, CA}},
  isbn = {978-1-4919-1118-1},
  langid = {english},
  lccn = {QA76.76.D47 W474 2015},
  keywords = {Application software,Computer software,Development,Distributed processing,Electronic data processing,Git (Computer file),Management,Teams in the workplace},
  annotation = {OCLC: ocn921827412},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WFN77BPG\\Westby - 2015 - Git for teams.pdf}
}

@article{westra_big_2016,
  title = {Big {{Data}} and {{Perioperative Nursing}}},
  author = {Westra, Bonnie L. and Peterson, Jessica J.},
  year = {2016},
  journal = {AORN Journal},
  volume = {104},
  number = {4},
  pages = {286--292},
  issn = {1878-0369},
  doi = {10.1016/j.aorn.2016.07.009},
  abstract = {Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five ``Vs'': volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.},
  copyright = {\textcopyright{} 2016 AORN, Inc},
  langid = {english},
  keywords = {big data,nursing informatics,nursing knowledge,perioperative nursing,quality care},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HB254RV5\\Westra en Peterson - 2016 - Big Data and Perioperative Nursing.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\PCSVDT2N\\j.aorn.2016.07.html}
}

@article{wickham_tidy_2014,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {59},
  number = {1},
  pages = {1--23},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  copyright = {Copyright (c) 2013 Hadley  Wickham},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QKMUM6CQ\\Wickham - 2014 - Tidy Data.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\23WIYBYU\\2168.html}
}

@article{winham_weighted_2013,
  title = {A Weighted Random Forests Approach to Improve Predictive Performance},
  author = {Winham, Stacey J. and Freimuth, Robert R. and Biernacka, Joanna M.},
  year = {2013},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  volume = {6},
  number = {6},
  pages = {496--505},
  issn = {1932-1872},
  doi = {10.1002/sam.11196},
  abstract = {Identifying genetic variants associated with complex disease in high-dimensional data is a challenging problem, and complicated etiologies such as gene\textendash gene interactions are often ignored in analyses. The data-mining method random forests (RF) can handle high dimensions; however, in high-dimensional data, RF is not an effective filter for identifying risk factors associated with the disease trait via complex genetic models such as gene\textendash gene interactions without strong marginal components. In this article we propose an extension called weighted random forests (wRF), which incorporates tree-level weights to emphasize more accurate trees in prediction and calculation of variable importance. We demonstrate through simulation and application to data from a genetic study of addiction that wRF can outperform RF in high-dimensional data, although the improvements are modest and limited to situations with effect sizes that are larger than what is realistic in genetics of complex disease. Thus, the current implementation of wRF is unlikely to improve detection of relevant predictors in high-dimensional genetic data, but may be applicable in other situations where larger effect sizes are anticipated. \textcopyright{} 2013 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2013},
  langid = {english},
  keywords = {gene–gene interactions,genetic data,genome-wide association,high-dimensional data,random forests,weighting},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.11196},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\J22TAUR6\\Winham e.a. - 2013 - A weighted random forests approach to improve pred.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\UVI9UWFT\\sam.html}
}

@article{wong_performance_2015,
  title = {Performance Evaluation of Classification Algorithms by K-Fold and Leave-One-out Cross Validation},
  author = {Wong, Tzu-Tsung},
  year = {2015},
  month = sep,
  journal = {Pattern Recognition},
  volume = {48},
  number = {9},
  pages = {2839--2846},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2015.03.009},
  abstract = {Classification is an essential task for predicting the class values of new instances. Both k-fold and leave-one-out cross validation are very popular for evaluating the performance of classification algorithms. Many data mining literatures introduce the operations for these two kinds of cross validation and the statistical methods that can be used to analyze the resulting accuracies of algorithms, while those contents are generally not all consistent. Analysts can therefore be confused in performing a cross validation procedure. In this paper, the independence assumptions in cross validation are introduced, and the circumstances that satisfy the assumptions are also addressed. The independence assumptions are then used to derive the sampling distributions of the point estimators for k-fold and leave-one-out cross validation. The cross validation procedure to have such sampling distributions is discussed to provide new insights in evaluating the performance of classification algorithms.},
  langid = {english},
  keywords = {-Fold cross validation,Classification,Independence,Leave-one-out cross validation,Sampling distribution},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NPYKJF4M\\Wong - 2015 - Performance evaluation of classification algorithm.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\WS73MIPC\\S0031320315000989.html}
}

@misc{yang_local_2020,
  title = {Local {{Differential Privacy}} and {{Its Applications}}: {{A Comprehensive Survey}}},
  shorttitle = {Local {{Differential Privacy}} and {{Its Applications}}},
  author = {Yang, Mengmeng and Lyu, Lingjuan and Zhao, Jun and Zhu, Tianqing and Lam, Kwok-Yan},
  year = {2020},
  month = aug,
  number = {arXiv:2008.03686},
  eprint = {2008.03686},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {With the fast development of Information Technology, a tremendous amount of data have been generated and collected for research and analysis purposes. As an increasing number of users are growing concerned about their personal information, privacy preservation has become an urgent problem to be solved and has attracted significant attention. Local differential privacy (LDP), as a strong privacy tool, has been widely deployed in the real world in recent years. It breaks the shackles of the trusted third party, and allows users to perturb their data locally, thus providing much stronger privacy protection. This survey provides a comprehensive and structured overview of the local differential privacy technology. We summarise and analyze state-of-the-art research in LDP and compare a range of methods in the context of answering a variety of queries and training different machine learning models. We discuss the practical deployment of local differential privacy and explore its application in various domains. Furthermore, we point out several research gaps, and discuss promising future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\D49D6UZQ\\Yang e.a. - 2020 - Local Differential Privacy and Its Applications A.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\4SNPVQXE\\2008.html}
}

@article{yarygina_exploring_2018,
  title = {Exploring Microservice Security},
  author = {Yarygina, Tetiana},
  year = {2018},
  publisher = {{The University of Bergen}}
}

@article{yarygina_exploring_nodate,
  title = {Exploring {{Microservice Security}}},
  author = {Yarygina, Tetiana},
  pages = {144},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QSJJH3RQ\\Yarygina - Exploring Microservice Security.pdf}
}

@article{yunakovsky_towards_2021,
  title = {Towards Security Recommendations for Public-Key Infrastructures for Production Environments in the Post-Quantum Era},
  author = {Yunakovsky, Sergey E. and Kot, Maxim and Pozhar, Nikolay and Nabokov, Denis and Kudinov, Mikhail and Guglya, Anton and Kiktenko, Evgeniy O. and Kolycheva, Ekaterina and Borisov, Alexander and Fedorov, Aleksey K.},
  year = {2021},
  month = dec,
  journal = {EPJ Quantum Technology},
  volume = {8},
  number = {1},
  pages = {14},
  issn = {2662-4400, 2196-0763},
  doi = {10.1140/epjqt/s40507-021-00104-z},
  abstract = {Quantum computing technologies pose a significant threat to the currently employed public-key cryptography protocols. In this paper, we discuss the impact of the quantum threat on public key infrastructures (PKIs), which are used as a part of security systems for protecting production environments. We analyze security issues of existing models with a focus on requirements for a fast transition to post-quantum solutions. Although our primary focus is on the attacks with quantum computing, we also discuss some security issues that are not directly related to the used cryptographic algorithms but are essential for the overall security of the PKI. We attempt to provide a set of security recommendations regarding the PKI from the viewpoints of attacks with quantum computers.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MZW9QXWN\\Yunakovsky e.a. - 2021 - Towards security recommendations for public-key in.pdf}
}

@incollection{yung_curve25519_2006,
  title = {Curve25519: {{New Diffie-Hellman Speed Records}}},
  shorttitle = {Curve25519},
  booktitle = {Public {{Key Cryptography}} - {{PKC}} 2006},
  author = {Bernstein, Daniel J.},
  editor = {Yung, Moti and Dodis, Yevgeniy and Kiayias, Aggelos and Malkin, Tal},
  year = {2006},
  volume = {3958},
  pages = {207--228},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11745853_14},
  abstract = {This paper explains the design and implementation of a highsecurity elliptic-curve-Diffie-Hellman function achieving record-setting speeds: e.g., 832457 Pentium III cycles (with several side benefits: free key compression, free key validation, and state-of-the-art timing-attack protection), more than twice as fast as other authors' results at the same conjectured security level (with or without the side benefits).},
  isbn = {978-3-540-33851-2 978-3-540-33852-9},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\AFKTE85H\\Bernstein - 2006 - Curve25519 New Diffie-Hellman Speed Records.pdf}
}

@article{zhang_introduction_2016,
  title = {Introduction to Machine Learning: K-Nearest Neighbors},
  shorttitle = {Introduction to Machine Learning},
  author = {Zhang, Zhongheng},
  year = {2016},
  month = jun,
  journal = {Annals of Translational Medicine},
  volume = {4},
  number = {11},
  issn = {2305-5839},
  doi = {10.21037/atm.2016.03.37},
  abstract = {Machine learning techniques have been widely used in many scientific fields, but its use in medical literature is limited partly because of technical difficulties. k-nearest neighbors (kNN) is a simple method of machine learning. The article introduces some basic ideas underlying the kNN algorithm, and then focuses on how to perform kNN modeling with R. The dataset should be prepared before running the knn() function in R. After prediction of outcome with kNN algorithm, the diagnostic performance of the model should be checked. Average accuracy is the mostly widely used statistic to reflect the kNN algorithm. Factors such as k value, distance calculation and choice of appropriate predictors all have significant impact on the model performance.},
  pmcid = {PMC4916348},
  pmid = {27386492},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QMR5ACSN\\Zhang - 2016 - Introduction to machine learning k-nearest neighb.pdf}
}

@article{zhang_learning_2017,
  title = {Learning k for {{kNN Classification}}},
  author = {Zhang, Shichao and Li, Xuelong and Zong, Ming and Zhu, Xiaofeng and Cheng, Debo},
  year = {2017},
  month = apr,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {8},
  number = {3},
  pages = {1--19},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/2990508},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4KICXF7S\\Zhang e.a. - 2017 - Learning iki for kNN Classification.pdf}
}

@article{zhang_learning_2017-1,
  title = {Learning {\emph{k}} for {{kNN Classification}}},
  author = {Zhang, Shichao and Li, Xuelong and Zong, Ming and Zhu, Xiaofeng and Cheng, Debo},
  year = {2017},
  month = apr,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {8},
  number = {3},
  pages = {1--19},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/2990508},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WZYHPQZD\\Zhang e.a. - 2017 - Learning iki for kNN Classification.pdf}
}

@article{zhang_learning_2017-2,
  title = {Learning {\emph{k}} for {{kNN Classification}}},
  author = {Zhang, Shichao and Li, Xuelong and Zong, Ming and Zhu, Xiaofeng and Cheng, Debo},
  year = {2017},
  month = apr,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {8},
  number = {3},
  pages = {1--19},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/2990508},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ENB93KDE\\Zhang e.a. - 2017 - Learning iki for kNN Classification.pdf}
}

@article{zhang_parimputation_2008,
  title = {Parimputation: {{From Imputation}} and {{Null-Imputation}} to {{Partially Imputation}}},
  author = {Zhang, Shichao},
  year = {2008},
  pages = {7},
  abstract = {Missing data imputation is an important step in the process of machine learning and data mining when certain values are missed. Among extant imputation techniques, kNN imputation algorithm is the best one as it is a model free and efficient compared with other methods. However, the value of k must be chosen properly in using kNN imputation. In particular, when some nearest neighbors are far from a missing data, the kNN imputation algorithms are often of low efficiency. In this paper, a new imputation framework is designed. The imputation uses the left or right nearest neighbor for a missing data in a given dataset. Furthermore, a parimputation (partially imputation) strategy is proposed for dealing with the issue of missing data imputation. Specifically, some missing data are imputed when there are some complete data in a small neighborhood of the missing data and, other missing data without imputation are given up in applications, such as data mining and machine learning.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XVGAMBDV\\Zhang - 2008 - Parimputation From Imputation and Null-Imputation.pdf}
}

@article{zhang_time_2003,
  title = {Time Series Forecasting Using a Hybrid {{ARIMA}} and Neural Network Model},
  author = {Zhang, G Peter},
  year = {2003},
  journal = {Neurocomputing},
  volume = {50},
  pages = {159--175},
  publisher = {{Elsevier}},
  issn = {0925-2312}
}

@inproceedings{zhong_keystroke_2012,
  title = {Keystroke Dynamics for User Authentication},
  booktitle = {2012 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Zhong, Yu and Deng, Yunbin and Jain, Anil K.},
  year = {2012},
  month = jun,
  pages = {117--123},
  publisher = {{IEEE}},
  address = {{Providence, RI, USA}},
  doi = {10.1109/CVPRW.2012.6239225},
  abstract = {In this paper we investigate the problem of user authentication using keystroke biometrics. A new distance metric that is effective in dealing with the challenges intrinsic to keystroke dynamics data, i.e., scale variations, feature interactions and redundancies, and outliers is proposed. Our keystroke biometrics algorithms based on this new distance metric are evaluated on the CMU keystroke dynamics benchmark dataset and are shown to be superior to algorithms using traditional distance metrics.},
  isbn = {978-1-4673-1612-5 978-1-4673-1611-8 978-1-4673-1610-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CG98WY3W\\Zhong e.a. - 2012 - Keystroke dynamics for user authentication.pdf}
}

@article{zhou_machine_2017,
  title = {Machine Learning on Big Data: {{Opportunities}} and Challenges},
  author = {Zhou, Lina and Pan, Shimei and Wang, Jianwu and Vasilakos, Athanasios V.},
  year = {2017},
  month = may,
  journal = {Neurocomputing},
  volume = {237},
  pages = {350--361},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.01.026},
  abstract = {Machine learning (ML) is continuously unleashing its power in a wide range of applications. It has been pushed to the forefront in recent years partly owing to the advent of big data. ML algorithms have never been better promised while challenged by big data. Big data enables ML algorithms to uncover more fine-grained patterns and make more timely and accurate predictions than ever before; on the other hand, it presents major challenges to ML such as model scalability and distributed computing. In this paper, we introduce a framework of ML on big data (MLBiD) to guide the discussion of its opportunities and challenges. The framework is centered on ML which follows the phases of preprocessing, learning, and evaluation. In addition, the framework is also comprised of four other components, namely big data, user, domain, and system. The phases of ML and the components of MLBiD provide directions for identification of associated opportunities and challenges and open up future work in many unexplored or under explored research areas.},
  keywords = {Big data,Data preprocessing,Evaluation,Machine learning,Parallelization}
}

@article{zhou_machine_nodate,
  title = {Machine Learning on Big Data: {{Opportunities}} and Challenges},
  author = {Zhou, Lina and Pan, Shimei and Wang, Jianwu and Vasilakos, Athanasios},
  number = {Neurocomputing},
  pages = {350--361},
  abstract = {Machine learning (ML) is continuously unleashing its power in a wide range of applications. It has been pushed to the forefront in recent years partly owing to the advent of big data. ML algorithms have never been better promised while challenged by big data. Big data enables ML algorithms to uncover more fine-grained patterns and make more timely and accurate predictions than ever before; on the other hand, it presents major challenges to ML such as model scalability and distributed computing. In this paper, we introduce a framework of ML on big data (MLBiD) to guide the discussion of its opportunities and challenges. The framework is centered on ML which follows the phases of preprocessing, learning, and evaluation. In addition, the framework is also comprised of four other components, namely big data, user, domain, and system. The phases of ML and the components of MLBiD provide directions for identification of associated opportunities and challenges and open up future work in many unexplored or under explored research areas.}
}

@article{zhu_devops_2016,
  title = {{{DevOps}} and {{Its Practices}}},
  author = {Zhu, Liming and Bass, Len and {Champlin-Scharff}, George},
  year = {2016},
  month = may,
  journal = {IEEE Software},
  volume = {33},
  number = {3},
  pages = {32--34},
  issn = {1937-4194},
  doi = {10.1109/MS.2016.81},
  abstract = {DevOps aims to reduce the time between committing a system change and placing the change into normal production, while ensuring high quality. The article topics in this theme issue include using DevOps to migrate to microservices, adopting DevOps, and DevOps tools. The Web extra at https://youtu.be/NzX6JmwDS0s is an audio recording of Davide Falessi speaking with Len Bass and George Champlin-Scharff about the IEEE Software May/June 2016 theme issue on DevOps and its practices.},
  keywords = {continuous delivery,continuous deployment,DevOps,microservices,software development,software engineering},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\Z35HTIS9\\Zhu e.a. - 2016 - DevOps and Its Practices.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\GWWHRD7Z\\7458765.html}
}

@book{zou_academy_nodate,
  title = {Academy {{Publisher}}},
  author = {Zou, Youfeng and Yu, Fei and Jia, Zongpu and Li, Zichen and Chia, Feng},
  abstract = {Opinions expressed in the papers are those of the author(s) and do not necessarily express the opinions of the editors or the Academy Publisher. The papers are published as presented and without change, in the interests of timely dissemination. AP Catalog Number AP-PROC-CS-10CN007 ISBN 978-952-5726-10-7}
}

@article{zwitter_big_2014,
  title = {Big {{Data}} Ethics},
  author = {Zwitter, Andrej},
  year = {2014},
  month = jul,
  journal = {Big Data \& Society},
  volume = {1},
  number = {2},
  pages = {205395171455925},
  issn = {2053-9517, 2053-9517},
  doi = {10.1177/2053951714559253},
  abstract = {The speed of development in Big Data and associated phenomena, such as social media, has surpassed the capacity of the average consumer to understand his or her actions and their knock-on effects. We are moving towards changes in how ethics has to be perceived: away from individual decisions with specific and knowable outcomes, towards actions by many unaware that they may have taken actions with unintended consequences for anyone. Responses will require a rethinking of ethical choices, the lack thereof and how this will guide scientists, governments, and corporate agencies in handling Big Data. This essay elaborates on the ways Big Data impacts on ethical conceptions.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\K4S9NF33\\Zwitter - 2014 - Big Data ethics.pdf}
}
