@inproceedings{10.1145/1835804.1835848,
  title = {Unsupervised Feature Selection for Multi-Cluster Data},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Cai, Deng and Zhang, Chiyuan and He, Xiaofei},
  year = {2010},
  series = {{{KDD}} '10},
  pages = {333--342},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1835804.1835848},
  abstract = {In many data analysis tasks, one is often confronted with very high dimensional data. Feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering, classification and retrieval. In this paper, we consider the feature selection problem in unsupervised learning scenario, which is particularly difficult due to the absence of class labels that would guide the search for relevant information. The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive. Traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature. These approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset. Inspired from the recent developments on manifold learning and L1-regularized models for subset selection, we propose in this paper a new approach, called Multi-Cluster Feature Selection (MCFS), for unsupervised feature selection. Specifically, we select those features such that the multi-cluster structure of the data can be best preserved. The corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem. Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm.},
  isbn = {978-1-4503-0055-1},
  keywords = {clustering,feature selection,unsupervised}
}

@article{1056489,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  doi = {10.1109/TIT.1982.1056489}
}

@article{9646489,
  title = {{{3D Geo-Indistinguishability}} for {{Indoor Location-Based Services}}},
  author = {Min, Minghui and Xiao, Liang and Ding, Jiahao and Zhang, Hongliang and Li, Shiyin and Pan, Miao and Han, Zhu},
  year = {2022},
  journal = {IEEE Transactions on Wireless Communications},
  volume = {21},
  number = {7},
  pages = {4682--4694},
  doi = {10.1109/TWC.2021.3132464},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QPJZTQGX\\Min e.a. - 2022 - 3D geo-indistinguishability for indoor location-ba.pdf}
}

@inproceedings{9679364,
  title = {Private Distributed {{K-means}} Clustering on Interval Data},
  booktitle = {2021 {{IEEE}} International Performance, Computing, and Communications Conference ({{IPCCC}})},
  author = {Huang, D. and Yao, X. and An, S. and Ren, S.},
  year = {2021},
  month = oct,
  pages = {1--9},
  publisher = {{IEEE Computer Society}},
  address = {{Los Alamitos, CA, USA}},
  doi = {10.1109/IPCCC51483.2021.9679364},
  abstract = {K-means clustering has been heavily employed to mine valuable insights from interval data. Nevertheless, serious privacy leakage concerns are stumbling blocks impeding its widespread application. To quantify the privacy of small and large-scale interval data, we introduce two notions of \&amp;\#x03B1;-Condensed Local Differential Privacy and \&amp;\#x03F5;-Local Differential Privacy, and propose two distance-aware perturbation mechanisms of \&amp;\#x03B1;-exponential and square wave mechanisms. Rigorous theoretical analysis proves that our proposed mechanisms satisfy these two privacy notions. The experimental results built on multiple synthesized and real datasets show that our proposed mechanisms can provide more accurate clustering results than prior work, such as Randomized Response, Generalized Randomized Response, and Optimized Local Hash.},
  keywords = {conferences,correlation,differential privacy,distributed databases,perturbation methods,privacy},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6F8LBU8D\\Huang e.a. - 2021 - Private distributed K-means clustering on interval.pdf}
}

@article{ahmed_k-means_2020-1,
  title = {The K-Means Algorithm: {{A}} Comprehensive Survey and Performance Evaluation},
  author = {Ahmed, Mohiuddin and Seraj, Raihan and Islam, Syed Mohammed Shamsul},
  year = {2020},
  journal = {Electronics},
  volume = {9},
  number = {8},
  pages = {1295},
  publisher = {{MDPI}},
  issn = {2079-9292}
}

@misc{ahuja_utility-preserving_2019,
  title = {A {{Utility-Preserving}} and {{Scalable Technique}} for {{Protecting Location Data}} with {{Geo-Indistinguishability}}},
  year = {2019},
  publisher = {{OpenProceedings.org}},
  doi = {10.5441/002/EDBT.2019.20},
  urldate = {2023-02-26},
  abstract = {Location-based apps provide users with personalized services tailored to their geographical position. This is highly-beneficial for mobile users, who are able to find points of interest close to their location, or connect with nearby friends. However, sharing location data with service providers also introduces privacy concerns. An adversary with access to fine-grained user locations can infer private details about individuals. Geo-indistinguishability (GeoInd) adapts the popular differential privacy (DP) model to make it suitable for protecting users' location information. However, existing techniques that implement GeoInd have major drawbacks. Some solutions, such as the planar Laplace mechanism, significantly lower data utility by adding excessive noise. Other approaches, such as the optimal mechanism, achieve good utility, but only work for small sets of candidate locations due to the use of computationally-expensive linear programming. In most cases, locations are used to answer online queries, so a quick response time is essential. In this paper, we propose a technique that achieves GeoInd and scales to large datasets while preserving data utility. Our central idea is to use the composability property of GeoInd to create a multiple-step algorithm that can be used in conjunction with a spatial index. We preserve utility by applying accurate GeoInd mechanisms and we achieve scalability by pruning the solution search space with the help of the index when seeking high-utility outcomes. Our extensive performance evaluation on top of real location datasets from social media apps shows that the proposed technique outperforms significantly the benchmark in terms of utility and/or computational overhead.},
  collaborator = {Ahuja, Ritesh and Ghinita, Gabriel and Shahabi, Cyrus},
  langid = {english},
  keywords = {Database Technology},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VN38ZL5N\\Ahuja et al. - 2019 - A Utility-Preserving and Scalable Technique for Pr.pdf}
}

@article{alvim_local_nodate,
  title = {Local {{Differential Privacy}} on {{Metric Spaces}}: Optimizing the Trade-off with Utility},
  author = {Alvim, Mario S and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Pazii, Anna},
  abstract = {Local differential privacy (LPD) is a distributed variant of differential privacy (DP) in which the obfuscation of the sensitive information is done at the level of the individual records, and in general it is used to sanitize data that are collected for statistical purposes. LPD has the advantage it does not need to assume a trusted third party. On the other hand LDP in general requires more noise than DP to achieve the same level of protection, with negative consequences on the utility. In practice, utility becomes acceptable only on very large collections of data, and this is the reason why LDP is especially successful among big companies such as Apple and Google, which can count on a huge number of users. In this paper, we propose a variant of LDP suitable for metric spaces, such as location data or energy consumption data, and we show that it provides a much better utility for the same level of privacy.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VXCWK8LH\\Alvim et al. - Local Differential Privacy on Metric Spaces optim.pdf}
}

@incollection{bailey_privacy_2016,
  title = {Privacy {{Aware K-Means Clustering}} with {{High Utility}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Nguyen, Thanh Dai and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
  editor = {Bailey, James and Khan, Latifur and Washio, Takashi and Dobbie, Gill and Huang, Joshua Zhexue and Wang, Ruili},
  year = {2016},
  volume = {9652},
  pages = {388--400},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-31750-2_31},
  urldate = {2023-03-15},
  isbn = {978-3-319-31749-6 978-3-319-31750-2},
  langid = {english}
}

@misc{bashir_information-theoretic_2020,
  title = {An {{Information-Theoretic Perspective}} on {{Overfitting}} and {{Underfitting}}},
  author = {Bashir, Daniel and Montanez, George D. and Sehra, Sonia and Segura, Pedro Sandoval and Lauw, Julius},
  year = {2020},
  month = nov,
  number = {arXiv:2010.06076},
  eprint = {2010.06076},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {We present an information-theoretic framework for understanding overfitting and underfitting in machine learning and prove the formal undecidability of determining whether an arbitrary classification algorithm will overfit a dataset. Measuring algorithm capacity via the information transferred from datasets to models, we consider mismatches between algorithm capacities and datasets to provide a signature for when a model can overfit or underfit a dataset. We present results upper-bounding algorithm capacity, establish its relationship to quantities in the algorithmic search framework for machine learning, and relate our work to recent information-theoretic approaches to generalization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MFVDL5Q9\\Bashir e.a. - 2020 - An Information-Theoretic Perspective on Overfittin.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\ANXC3L8Z\\2010.html}
}

@misc{bozdemir_privacy-preserving_nodate,
  title = {Privacy-Preserving {{Density-based Clustering}}},
  author = {Bozdemir, Beyza and Canard, S{\'e}bastien and Ermis, Orhan and M{\"o}llering, Helen and {\"O}nen, Melek and Schneider, Thomas},
  urldate = {2023-03-30},
  abstract = {Clustering is an unsupervised machine learning technique that outputs clusters containing similar data items. In this work, we investigate privacy-preserving density-based clustering which is, for example, used in financial analytics and medical diagnosis. When (multiple) data owners collaborate or outsource the computation, privacy concerns arise. To address this problem, we design, implement, and evaluate the first practical and fully private density-based clustering scheme based on secure two-party computation. Our protocol privately executes the DBSCAN algorithm without disclosing any information (including the number and size of clusters). It can be used for private clustering between two parties as well as for private outsourcing of an arbitrary number of data owners to two non-colluding servers. Our implementation of the DBSCAN algorithm privately clusters data sets with 400 elements in 7 minutes on commodity hardware. Thereby, it flexibly determines the number of required clusters and is insensitive to outliers, while being only factor 19x slower than today's fastest private K-means protocol (Mohassel et al., PETS'20) which can only be used for specific data sets. We then show how to transfer our newly designed protocol to related clustering algorithms by introducing a private approximation of the TRACLUS algorithm for trajectory clustering which has interesting real-world applications like financial time series forecasts and the investigation of the spread of a disease like COVID-19.},
  keywords = {Clustering,Private Machine Learning,Secure Computation}
}

@misc{brendel_decision-based_2018,
  title = {Decision-{{Based Adversarial Attacks}}: {{Reliable Attacks Against Black-Box Machine Learning Models}}},
  shorttitle = {Decision-{{Based Adversarial Attacks}}},
  author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  year = {2018},
  month = feb,
  number = {arXiv:1712.04248},
  eprint = {1712.04248},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.04248},
  urldate = {2023-04-10},
  abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BWBD3ZFH\\Brendel et al. - 2018 - Decision-Based Adversarial Attacks Reliable Attac.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\4N4IJ62K\\1712.html}
}

@inproceedings{cai_dp-ap_2020,
  title = {{{DP-AP}}: {{Differential Privacy-Preserving Affinity Propagation Clustering}}},
  shorttitle = {{{DP-AP}}},
  booktitle = {2020 {{IEEE}} 14th {{International Conference}} on {{Big Data Science}} and {{Engineering}} ({{BigDataSE}})},
  author = {Cai, Hanbo and Wang, Jinyan and Liu, Xiaohong and Li, Xianxian},
  year = {2020},
  month = dec,
  pages = {73--79},
  publisher = {{IEEE}},
  address = {{Guangzhou, China}},
  doi = {10.1109/BigDataSE50710.2020.00018},
  urldate = {2022-12-22},
  isbn = {978-1-66540-396-2},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LH7Y4ZSA\\Cai e.a. - 2020 - DP-AP Differential Privacy-Preserving Affinity Pr.pdf}
}

@article{calinski_dendrite_1974-1,
  title = {A Dendrite Method for Cluster Analysis},
  author = {Cali{\'n}ski, Tadeusz and Harabasz, Jerzy},
  year = {1974},
  journal = {Communications in Statistics-theory and Methods},
  volume = {3},
  number = {1},
  pages = {1--27},
  publisher = {{Taylor \& Francis}},
  issn = {0090-3272}
}

@misc{carlini_membership_2022,
  title = {Membership {{Inference Attacks From First Principles}}},
  author = {Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  year = {2022},
  month = apr,
  number = {arXiv:2112.03570},
  eprint = {2112.03570},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model's training dataset. These attacks are currently evaluated using average-case "accuracy" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., {$<$}0.1\%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is 10x more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\X2D3DFW4\\Carlini et al. - 2022 - Membership Inference Attacks From First Principles.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\B7TASZ9G\\2112.html}
}

@article{chatzikokolakis_constructing_2015,
  title = {Constructing Elastic Distinguishability Metrics for Location Privacy},
  author = {Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Stronati, Marco},
  year = {2015},
  month = jun,
  journal = {Proceedings on Privacy Enhancing Technologies},
  volume = {2015},
  number = {2},
  eprint = {1503.00756},
  primaryclass = {cs},
  pages = {156--170},
  issn = {2299-0984},
  doi = {10.1515/popets-2015-0023},
  urldate = {2023-03-10},
  abstract = {With the increasing popularity of hand-held devices, location-based applications and services have access to accurate and real-time location information, raising serious privacy concerns for their users. The recently introduced notion of geo-indistinguishability tries to address this problem by adapting the well-known concept of differential privacy to the area of location-based systems. Although geo-indistinguishability presents various appealing aspects, it has the problem of treating space in a uniform way, imposing the addition of the same amount of noise everywhere on the map. In this paper we propose a novel elastic distinguishability metric that warps the geometrical distance, capturing the different degrees of density of each area. As a consequence, the obtained mechanism adapts the level of noise while achieving the same degree of privacy everywhere. We also show how such an elastic metric can easily incorporate the concept of a "geographic fence" that is commonly employed to protect the highly recurrent locations of a user, such as his home or work. We perform an extensive evaluation of our technique by building an elastic metric for Paris' wide metropolitan area, using semantic information from the OpenStreetMap database. We compare the resulting mechanism against the Planar Laplace mechanism satisfying standard geo-indistinguishability, using two real-world datasets from the Gowalla and Brightkite location-based social networks. The results show that the elastic mechanism adapts well to the semantics of each area, adjusting the noise as we move outside the city center, hence offering better overall privacy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BBGUL4GI\\Chatzikokolakis e.a. - 2015 - Constructing elastic distinguishability metrics fo.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\FI7V7IG2\\1503.html}
}

@article{chatzikokolakis_efficient_2017,
  title = {Efficient Utility Improvement for Location Privacy},
  author = {Chatzikokolakis, Konstantinos and Elsalamouny, Ehab and Palamidessi, Catuscia},
  year = {2017},
  journal = {Proceedings on Privacy Enhancing Technologies},
  volume = {2017},
  number = {4},
  pages = {308--328}
}

@article{chatzikokolakis_practical_nodate,
  title = {Practical {{Mechanisms}} for {{Location Privacy}}},
  author = {Chatzikokolakis, Konstantinos and ElSalamouny, Ehab and Palamidessi, Catuscia},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\355U7PAM\\Chatzikokolakis et al. - Practical Mechanisms for Location Privacy.pdf}
}

@misc{chen_hopskipjumpattack_2020,
  title = {{{HopSkipJumpAttack}}: {{A Query-Efficient Decision-Based Attack}}},
  shorttitle = {{{HopSkipJumpAttack}}},
  author = {Chen, Jianbo and Jordan, Michael I. and Wainwright, Martin J.},
  year = {2020},
  month = apr,
  number = {arXiv:1904.02144},
  eprint = {1904.02144},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for \$\textbackslash ell\_2\$ and \$\textbackslash ell\_\textbackslash infty\$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4QQPRZVS\\Chen e.a. - 2020 - HopSkipJumpAttack A Query-Efficient Decision-Base.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\5JE3JZKJ\\1904.html}
}

@article{chicco_ten_2017,
  title = {Ten Quick Tips for Machine Learning in Computational Biology},
  author = {Chicco, Davide},
  year = {2017},
  month = dec,
  journal = {BioData Mining},
  volume = {10},
  pages = {35},
  issn = {1756-0381},
  doi = {10.1186/s13040-017-0155-3},
  urldate = {2023-04-13},
  abstract = {Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.},
  pmcid = {PMC5721660},
  pmid = {29234465},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\79L4QCKN\\Chicco - 2017 - Ten quick tips for machine learning in computation.pdf}
}

@misc{choquette-choo_label-only_2021,
  title = {Label-{{Only Membership Inference Attacks}}},
  author = {{Choquette-Choo}, Christopher A. and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  year = {2021},
  month = dec,
  number = {arXiv:2007.14321},
  eprint = {2007.14321},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WEU62HCZ\\Choquette-Choo e.a. - 2021 - Label-Only Membership Inference Attacks.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\KB6KNX7Y\\2007.html}
}

@article{craenendonck_using_nodate,
  title = {Using {{Internal Validity Measures}} to {{Compare Clustering Algorithms}}},
  author = {Craenendonck, Toon Van and Blockeel, Hendrik},
  abstract = {Recently, significant effort has been made to automate the machine learning process in the context of supervised learning. This automation includes, amongst other things, the selection of an appropriate learning algorithm and corresponding hyperparameters for a particular learning problem. In contrast, such problems are much less studied for unsupervised tasks such as clustering. Nevertheless, users who want to cluster a data set are confronted with similar problems: a clustering algorithm should be selected from the wide variety of available algorithms, and usually some hyperparameters have to be set. In a supervised setting, model search is guided by performance measures that rely on known class labels, such as accuracy. However, these measures are not applicable to clustering as labels are usually not available. Instead, one might use internal validity measures that only rely on properties intrinsic to the data set. Several such measures are defined, and in this paper we study the usefulness of four of them for model selection. We perform experiments with these measures in combination with six clustering algorithms. While some measures are suited to use in hyperparameter optimization for some specific algorithms, we conclude that none of them is suited to compare across very different clustering algorithms.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\3DJL2JPQ\\Craenendonck en Blockeel - Using Internal Validity Measures to Compare Cluste.pdf}
}

@article{davies_cluster_1979,
  title = {A Cluster Separation Measure},
  author = {Davies, David L and Bouldin, Donald W},
  year = {1979},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  number = {2},
  pages = {224--227},
  publisher = {{IEEE}},
  issn = {0162-8828}
}

@article{DBLP:journals/corr/abs-1212-1984,
  title = {Geo-Indistinguishability: {{Differential}} Privacy for Location-Based Systems},
  author = {Andr{\'e}s, Miguel E. and Bordenabe, Nicol{\'a}s Emilio and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia},
  year = {2012},
  journal = {CoRR},
  volume = {abs/1212.1984},
  eprint = {1212.1984},
  archiveprefix = {arxiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-1212-1984.bib},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HVVT4GUR\\Andrés e.a. - 2012 - Geo-indistinguishability Differential privacy for.pdf}
}

@article{del_rey_comprehensive_2020-1,
  title = {A {{Comprehensive Survey}} on {{Local Differential Privacy}}},
  author = {Xiong, Xingxing and Liu, Shubo and Li, Dan and Cai, Zhaohui and Niu, Xiaoguang},
  editor = {Del Rey, Angel M.},
  year = {2020},
  month = oct,
  journal = {Security and Communication Networks},
  volume = {2020},
  pages = {8829523},
  publisher = {{Hindawi}},
  issn = {1939-0114},
  doi = {10.1155/2020/8829523},
  abstract = {With the advent of the era of big data, privacy issues have been becoming a hot topic in public. Local differential privacy (LDP) is a state-of-the-art privacy preservation technique that allows to perform big data analysis (e.g., statistical estimation, statistical learning, and data mining) while guaranteeing each individual participant\&\#x2019;s privacy. In this paper, we present a comprehensive survey of LDP. We first give an overview on the fundamental knowledge of LDP and its frameworks. We then introduce the mainstream privatization mechanisms and methods in detail from the perspective of frequency oracle and give insights into recent studied on private basic statistical estimation (e.g., frequency estimation and mean estimation) and complex statistical estimation (e.g., multivariate distribution estimation and private estimation over complex data) under LDP. Furthermore, we present current research circumstances on LDP including the private statistical learning/inferencing, private statistical data analysis, privacy amplification techniques for LDP, and some application fields under LDP. Finally, we identify future research directions and open challenges for LDP. This survey can serve as a good reference source for the research of LDP to deal with various privacy-related scenarios to be encountered in practice.}
}

@article{demsar_hands-training_2021,
  title = {Hands-on Training about Overfitting},
  author = {Dem{\v s}ar, Janez and Zupan, Bla{\v z}},
  year = {2021},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {17},
  pages = {e1008671},
  doi = {10.1371/journal.pcbi.1008671},
  abstract = {Overfitting is one of the critical problems in developing models by machine learning. With machine learning becoming an essential technology in computational biology, we must include training about overfitting in all courses that introduce this technology to students and practitioners. We here propose a hands-on training for overfitting that is suitable for introductory level courses and can be carried out on its own or embedded within any data science course. We use workflow-based design of machine learning pipelines, experimentation-based teaching, and hands-on approach that focuses on concepts rather than underlying mathematics. We here detail the data analysis workflows we use in training and motivate them from the viewpoint of teaching goals. Our proposed approach relies on Orange, an open-source data science toolbox that combines data visualization and machine learning, and that is tailored for education in machine learning and explorative data analysis.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\W2CVK27X\\Demšar en Zupan - 2021 - Hands-on training about overfitting.pdf}
}

@article{dong_limitations_2018,
  title = {On the Limitations of Existing Notions of Location Privacy},
  author = {Dong, Kai and Guo, Taolin and Ye, Haibo and Li, Xuansong and Ling, Zhen},
  year = {2018},
  month = sep,
  journal = {Future Generation Computer Systems},
  volume = {86},
  pages = {1513--1522},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.05.045},
  urldate = {2023-03-16},
  abstract = {In the context of a single report of location information, existing researches define location privacy by adversary's uncertainty, inaccuracy, or incorrectness of the estimation, or by geo-indistinguishability which is a generalization of differential privacy. Each of these existing notions has problems in some specific scenarios. In this paper we illustrate the limitations of existing notions by constructing such scenarios, and introduce a formal definition on location privacy by quantifying the distance between the prior and posterior distribution over the possible locations. Further more, we show how to construct a near-optimal obfuscation mechanism by solving an optimization problem. We compare our proposed mechanism with the Laplace noise based geo-indistinguishable mechanism, and Shokri's optimal obfuscation mechanism, using both our proposed privacy metric and the traditional metric based on the estimated distance errors. The results show that our proposed metric better describes location privacy and our proposed mechanism makes a better tradeoff between privacy and utility.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C45AELKE\\Dong et al. - 2018 - On the limitations of existing notions of location.pdf}
}

@inproceedings{dwork_differential_2006,
  title = {Differential Privacy},
  booktitle = {Automata, {{Languages}} and {{Programming}}: 33rd {{International Colloquium}}, {{ICALP}} 2006, {{Venice}}, {{Italy}}, {{July}} 10-14, 2006, {{Proceedings}}, {{Part II}} 33},
  author = {Dwork, Cynthia},
  year = {2006},
  pages = {1--12},
  publisher = {{Springer}},
  isbn = {3-540-35907-9}
}

@article{elbatta_dynamic_2013,
  title = {A Dynamic {{Method}} for {{Discovering Density Varied Clusters}}},
  author = {Elbatta, Mohammed and Ashour, Wesam},
  year = {2013},
  month = feb,
  journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
  volume = {6},
  pages = {123--134},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZXC4JZX8\\Elbatta en Ashour - 2013 - A dynamic Method for Discovering Density Varied Cl.pdf}
}

@article{ester_density-based_nodate,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\D68L5F4R\\Ester e.a. - A Density-Based Algorithm for Discovering Clusters.pdf}
}

@article{fahad_survey_2014,
  title = {A {{Survey}} of {{Clustering Algorithms}} for {{Big Data}}: {{Taxonomy}} and {{Empirical Analysis}}},
  shorttitle = {A {{Survey}} of {{Clustering Algorithms}} for {{Big Data}}},
  author = {Fahad, Adil and Alshatri, Najlaa and Tari, Zahir and Alamri, Abdullah and Khalil, Ibrahim and Zomaya, Albert Y. and Foufou, Sebti and Bouras, Abdelaziz},
  year = {2014},
  month = sep,
  journal = {IEEE Transactions on Emerging Topics in Computing},
  volume = {2},
  number = {3},
  pages = {267--279},
  issn = {2168-6750},
  doi = {10.1109/TETC.2014.2330519},
  abstract = {Clustering algorithms have emerged as an alternative powerful meta-learning tool to accurately analyze the massive volume of data generated by modern applications. In particular, their main goal is to categorize data into clusters such that objects are grouped in the same cluster when they are similar according to specific metrics. There is a vast body of knowledge in the area of clustering and there has been attempts to analyze and categorize them for a larger number of applications. However, one of the major issues in using clustering algorithms for big data that causes confusion amongst practitioners is the lack of consensus in the definition of their properties as well as a lack of formal categorization. With the intention of alleviating these problems, this paper introduces concepts and algorithms related to clustering, a concise survey of existing (clustering) algorithms as well as providing a comparison, both from a theoretical and an empirical perspective. From a theoretical perspective, we developed a categorizing framework based on the main properties pointed out in previous studies. Empirically, we conducted extensive experiments where we compared the most representative algorithm from each of the categories using a large number of real (big) data sets. The effectiveness of the candidate clustering algorithms is measured through a number of internal and external validity metrics, stability, runtime, and scalability tests. In addition, we highlighted the set of clustering algorithms that are the best performing for big data.},
  keywords = {Algorithm design and analysis,big data,Big data,Clustering algorithms,Clustering methods,Neural networks,Partitioning algorithms,Taxonomies,unsupervised learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BJSQU8PU\\Fahad e.a. - 2014 - A Survey of Clustering Algorithms for Big Data Ta.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\8AUACR48\\stamp.html}
}

@article{franti_centroid_2014,
  title = {Centroid Index: {{Cluster}} Level Similarity Measure},
  shorttitle = {Centroid Index},
  author = {Fr{\"a}nti, Pasi and Rezaei, Mohammad and Zhao, Qinpei},
  year = {2014},
  month = sep,
  journal = {Pattern Recognition},
  volume = {47},
  number = {9},
  pages = {3034--3045},
  issn = {00313203},
  doi = {10.1016/j.patcog.2014.03.017},
  urldate = {2023-03-22},
  abstract = {In clustering algorithm, one of the main challenges is to solve the global allocation of the clusters instead of just local tuning of the partition borders. Despite this, all external cluster validity indexes calculate only point-level differences of two partitions without any direct information about how similar their cluster-level structures are. In this paper, we introduce a cluster level index called centroid index. The measure is intuitive, simple to implement, fast to compute and applicable in case of model mismatch as well. To a certain extent, we expect it to generalize other clustering models beyond the centroid-based k-means as well.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4IJJ6ZVN\\Fränti e.a. - 2014 - Centroid index Cluster level similarity measure.pdf}
}

@article{frey_clustering_2007,
  title = {Clustering by {{Passing Messages Between Data Points}}},
  author = {Frey, Brendan J. and Dueck, Delbert},
  year = {2007},
  month = feb,
  journal = {Science},
  volume = {315},
  number = {5814},
  pages = {972--976},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1136800},
  urldate = {2023-04-12},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such ``exemplars'' can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called ``affinity propagation,'' which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5QGKVZLN\\Frey en Dueck - 2007 - Clustering by Passing Messages Between Data Points.pdf}
}

@inproceedings{friedman_data_2010,
  title = {Data Mining with Differential Privacy},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Friedman, Arik and Schuster, Assaf},
  year = {2010},
  month = jul,
  pages = {493--502},
  publisher = {{ACM}},
  address = {{Washington DC USA}},
  doi = {10.1145/1835804.1835868},
  urldate = {2023-03-30},
  abstract = {We consider the problem of data mining with formal privacy guarantees, given a data access interface based on the differential privacy framework. Differential privacy requires that computations be insensitive to changes in any particular individual's record, thereby restricting data leaks through the results. The privacy preserving interface ensures unconditionally safe access to the data and does not require from the data miner any expertise in privacy. However, as we show in the paper, a naive utilization of the interface to construct privacy preserving data mining algorithms could lead to inferior data mining results. We address this problem by considering the privacy and the algorithmic requirements simultaneously, focusing on decision tree induction as a sample application. The privacy mechanism has a profound effect on the performance of the methods chosen by the data miner. We demonstrate that this choice could make the difference between an accurate classifier and a completely useless one. Moreover, an improved algorithm can achieve the same level of accuracy and privacy as the naive implementation but with an order of magnitude fewer learning samples.},
  isbn = {978-1-4503-0055-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\F635ZNB5\\Friedman en Schuster - 2010 - Data mining with differential privacy.pdf}
}

@article{hassani_using_2017,
  title = {Using Internal Evaluation Measures to Validate the Quality of Diverse Stream Clustering Algorithms},
  author = {Hassani, Marwan and Seidl, Thomas},
  year = {2017},
  month = aug,
  journal = {Vietnam Journal of Computer Science},
  volume = {4},
  number = {3},
  pages = {171--183},
  issn = {2196-8896},
  doi = {10.1007/s40595-016-0086-9},
  abstract = {Measuring the quality of a clustering algorithm has shown to be as important as the algorithm itself. It is a crucial part of choosing the clustering algorithm that performs best for an input data. Streaming input data have many features that make them much more challenging than static ones. They are endless, varying and emerging with high speeds. This raised new challenges for the clustering algorithms as well as for their evaluation measures. Up till now, external evaluation measures were exclusively used for validating stream clustering algorithms. While external validation requires a ground truth which is not provided in most applications, particularly in the streaming case, internal clustering validation is efficient and realistic. In this article, we analyze the properties and performances of eleven internal clustering measures. In particular, we apply these measures to carefully synthesized stream scenarios to reveal how they react to clusterings on evolving data streams using both k-means-based and density-based clustering algorithms. A series of experimental results show that different from the case with static data, the Calinski-Harabasz index performs the best in coping with common aspects and errors of stream clustering for k-means-based algorithms, while the revised validity index performs the best for density-based ones.}
}

@inproceedings{he_laplacian_2005,
  title = {Laplacian {{Score}} for {{Feature Selection}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
  year = {2005},
  volume = {18},
  publisher = {{MIT Press}},
  urldate = {2023-02-15},
  abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are "wrapper" techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a "filter" method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classification problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efficiency of our algorithm.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MEYJS6AX\\He e.a. - 2005 - Laplacian Score for Feature Selection.pdf}
}

@misc{hu_membership_2022,
  title = {Membership {{Inference Attacks}} on {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Membership {{Inference Attacks}} on {{Machine Learning}}},
  author = {Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Yu, Philip S. and Zhang, Xuyun},
  year = {2022},
  month = feb,
  number = {arXiv:2103.07853},
  eprint = {2103.07853},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-05},
  abstract = {Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this paper, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7FV4U9VE\\Hu e.a. - 2022 - Membership Inference Attacks on Machine Learning .pdf;C\:\\Users\\tjvan\\Zotero\\storage\\57DEQY5Z\\2103.html}
}

@article{hubert_comparing_1985-1,
  title = {Comparing Partitions},
  author = {Hubert, Lawrence and Arabie, Phipps},
  year = {1985},
  journal = {Journal of classification},
  volume = {2},
  pages = {193--218},
  publisher = {{Springer}},
  issn = {0176-4268}
}

@article{jayaraman_evaluating_nodate,
  title = {Evaluating {{Differentially Private Machine Learning}} in {{Practice}}},
  author = {Jayaraman, Bargav and Evans, David},
  abstract = {Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, , about how much information is leaked by a mechanism. When used in privacy-preserving machine learning, the goal is typically to limit what can be inferred from the model about individual training records. However, the calibration of the privacy budget is not well understood. Implementations of privacy-preserving machine learning often select large values of in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, relaxed definitions of differential privacy are often used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is no way to obtain privacy for free\textemdash relaxed definitions of differential privacy that reduce the amount of noise needed to improve utility also increase the measured privacy leakage. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs for complex learning tasks: settings that provide limited accuracy loss provide little effective privacy, and settings that provide strong privacy result in useless models.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SL2RLCCC\\Jayaraman en Evans - Evaluating Diﬀerentially Private Machine Learning .pdf}
}

@inproceedings{keller_balancing_2021,
  title = {Balancing {{Quality}} and {{Efficiency}} in {{Private Clustering}} with {{Affinity Propagation}}:},
  shorttitle = {Balancing {{Quality}} and {{Efficiency}} in {{Private Clustering}} with {{Affinity Propagation}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Security}} and {{Cryptography}}},
  author = {Keller, Hannah and M{\"o}llering, Helen and Schneider, Thomas and Yalame, Hossein},
  year = {2021},
  pages = {173--184},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Online Streaming, --- Select a Country ---}},
  doi = {10.5220/0010547801730184},
  urldate = {2022-12-22},
  abstract = {In many machine learning applications, training data consists of sensitive information from multiple sources.},
  isbn = {978-989-758-524-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ITIR5ZT3\\Keller e.a. - 2021 - Balancing Quality and Efficiency in Private Cluste.pdf}
}

@article{kodinariya_review_2013,
  title = {Review on Determining Number of {{Cluster}} in {{K-Means Clustering}}},
  author = {Kodinariya, Trupti M and Makwana, Prashant R},
  year = {2013},
  journal = {International Journal},
  volume = {1},
  number = {6},
  pages = {90--95}
}

@article{kohavi_wrappers_1997,
  title = {Wrappers for Feature Subset Selection},
  author = {Kohavi, Ron and John, George H.},
  year = {1997},
  month = dec,
  journal = {Artificial Intelligence},
  series = {Relevance},
  volume = {97},
  number = {1},
  pages = {273--324},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(97)00043-X},
  urldate = {2023-02-15},
  abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.},
  langid = {english},
  keywords = {Classification,Feature selection,Filter,Wrapper},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5J9L28B4\\Kohavi en John - 1997 - Wrappers for feature subset selection.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\K2MHWNY7\\S000437029700043X.html}
}

@article{lehtonen_lambert_2016,
  title = {The {{Lambert W}} Function in Ecological and Evolutionary Models},
  author = {Lehtonen, Jussi},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {9},
  pages = {1110--1118},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12568},
  urldate = {2023-03-09},
  abstract = {The Lambert W function is a mathematical function with a long history, but which was named and rigorously defined relatively recently. It is closely related to the logarithmic function and arises from many models in the natural sciences, including a surprising number of problems in ecology and evolution. I describe the basic properties of the function and present examples of its application to models of ecological and evolutionary processes. The Lambert W function makes it possible to solve explicitly several models where this is not possible with elementary functions. I present examples of such models from existing literature, as well as novel models. Solving models explicitly with the Lambert W function can provide deeper insight and a new point of view on a biological problem. Explicit solutions with the Lambert W function are easily amenable to further mathematical operations, such as differentiation and integration. These advantages apply to a wide range of models, from the marginal value theorem to population growth rates and disease epidemics.},
  langid = {english},
  keywords = {calculus,explicit solution,fertilization kinetics,Lambert W function,Lotka–Volterra model,marginal value theorem,mate search,modelling,population growth rate,SIR model},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\S77QF2U8\\Lehtonen - 2016 - The Lambert W function in ecological and evolution.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\QYQTRQAH\\2041-210X.html}
}

@misc{li_membership_2021,
  title = {Membership {{Leakage}} in {{Label-Only Exposures}}},
  author = {Li, Zheng and Zhang, Yang},
  year = {2021},
  month = sep,
  number = {arXiv:2007.15528},
  eprint = {2007.15528},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack, and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6KD2H93N\\Li en Zhang - 2021 - Membership Leakage in Label-Only Exposures.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\DU9RRUMC\\2007.html}
}

@article{liu_privacy_2012,
  title = {Privacy Preserving Distributed {{DBSCAN}} Clustering},
  author = {Liu, Jinfei and Huang, Joshua and Luo, Jun and Xiong, Li},
  year = {2012},
  month = mar,
  journal = {Transactions on Data Privacy},
  volume = {6},
  doi = {10.1145/2320765.2320819},
  abstract = {DBSCAN is a well-known density-based clustering algorithm which offers advantages for finding clusters of arbitrary shapes compared to partitioning and hierarchical clustering methods. However, there are few papers studying the DBSCAN algorithm under the privacy preserving distributed data mining model, in which the data is distributed between two or more parties, and the parties cooperate to obtain the clustering results without revealing the data at the individual parties. In this paper, we address the problem of two-party privacy preserving DBSCAN clustering. We first propose two protocols for privacy preserving DBSCAN clustering over horizontally and vertically partitioned data respectively and then extend them to arbitrarily partitioned data. We also provide analysis of the performance and proof of privacy of our solution.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2AH2MBGH\\Liu e.a. - 2012 - Privacy preserving distributed DBSCAN clustering.pdf}
}

@inproceedings{m_alvim_invited_2018,
  title = {Invited {{Paper}}: {{Local Differential Privacy}} on {{Metric Spaces}}: {{Optimizing}} the {{Trade-Off}} with {{Utility}}},
  booktitle = {2018 {{IEEE}} 31st {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {{M. Alvim} and {K. Chatzikokolakis} and {C. Palamidessi} and {A. Pazii}},
  year = {2018},
  month = jul,
  pages = {262--267},
  doi = {10.1109/CSF.2018.00026},
  isbn = {2374-8303}
}

@article{mitra_unsupervised_2002,
  title = {Unsupervised Feature Selection Using Feature Similarity},
  author = {Mitra, P. and Murthy, C.A. and Pal, S.K.},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {3},
  pages = {301--312},
  issn = {01628828},
  doi = {10.1109/34.990133},
  urldate = {2023-02-15},
  abstract = {\DH In this article, we describe an unsupervised feature selection algorithm suitable for data sets, large in both dimension and size. The method is based on measuring similarity between features whereby redundancy therein is removed. This does not need any search and, therefore, is fast. A new feature similarity measure, called maximum information compression index, is introduced. The algorithm is generic in nature and has the capability of multiscale representation of data sets. The superiority of the algorithm, in terms of speed and performance, is established extensively over various real-life data sets of different sizes and dimensions. It is also demonstrated how redundancy and information loss in feature selection can be quantified with an entropy measure.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\E9SIXSRR\\Mitra e.a. - 2002 - Unsupervised feature selection using feature simil.pdf}
}

@article{moiane_evaluation_2018,
  title = {{{EVALUATION OF THE CLUSTERING PERFORMANCE OF AFFINITY PROPAGATION ALGORITHM CONSIDERING THE INFLUENCE OF PREFERENCE PARAMETER AND DAMPING FACTOR}}},
  author = {Moiane, Andr{\'e} Fenias and Machado, {\'A}lvaro Muriel Lima},
  year = {2018},
  month = dec,
  journal = {Boletim de Ci\^encias Geod\'esicas},
  volume = {24},
  number = {4},
  pages = {426--441},
  issn = {1982-2170, 1413-4853},
  doi = {10.1590/s1982-21702018000400027},
  urldate = {2023-04-12},
  abstract = {The identification of significant underlying data patterns such as image composition and spatial arrangements is fundamental in remote sensing tasks. Therefore, the development of an effective approach for information extraction is crucial to achieve this goal. Affinity propagation (AP) algorithm is a novel powerful technique with the ability of handling with unusual data, containing both categorical and numerical attributes. However, AP has some limitations related to the choice of initial preference parameter, occurrence of oscillations and processing of large data sets. This paper evaluates the clustering performance of AP algorithm taking into account the influence of preference parameter and damping factor. The study was conducted considering the AP algorithm, the adaptive AP and partition AP. According to the experiments, the choice of preference and damping greatly influences on the quality and the final number of clusters.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HQPEFR7T\\Moiane en Machado - 2018 - EVALUATION OF THE CLUSTERING PERFORMANCE OF AFFINI.pdf}
}

@misc{noauthor_adversarial_2023,
  title = {Adversarial {{Robustness Toolbox}} ({{ART}}) v1.14},
  year = {2023},
  month = apr,
  urldate = {2023-04-17},
  abstract = {Adversarial Robustness Toolbox (ART) - Python Library for Machine Learning Security - Evasion, Poisoning, Extraction, Inference - Red and Blue Teams},
  copyright = {MIT},
  howpublished = {Trusted-AI},
  keywords = {adversarial-attacks,adversarial-examples,adversarial-machine-learning,ai,artificial-intelligence,attack,blue-team,evasion,extraction,inference,machine-learning,poisoning,privacy,python,red-team,trusted-ai,trustworthy-ai}
}

@misc{noauthor_wayback_2020,
  title = {Wayback {{Machine}}},
  year = {2020},
  month = sep,
  urldate = {2023-03-16},
  howpublished = {https://web.archive.org/web/20200913222203/https://arxiv.org/ftp/arxiv/papers/1410/1410.7744.pdf},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\M56L7GUH\\2020 - Wayback Machine.pdf}
}

@inproceedings{oya_is_2017,
  title = {Is {{Geo-Indistinguishability What You Are Looking}} For?},
  booktitle = {Proceedings of the 2017 on {{Workshop}} on {{Privacy}} in the {{Electronic Society}}},
  author = {Oya, Simon and Troncoso, Carmela and {P{\'e}rez-Gonz{\'a}lez}, Fernando},
  year = {2017},
  month = oct,
  pages = {137--140},
  publisher = {{ACM}},
  address = {{Dallas Texas USA}},
  doi = {10.1145/3139550.3139555},
  urldate = {2023-02-28},
  abstract = {Since its proposal in 2013, geo-indistinguishability has been consolidated as a formal notion of location privacy, generating a rich body of literature building on this idea. A problem with most of these follow-up works is that they blindly rely on geo-indistinguishability to provide location privacy, ignoring the numerical interpretation of this privacy guarantee. In this paper we provide an alternative formulation of geo-indistinguishability as an adversary error, and use it to show that the privacy vs. utility trade-off that can be obtained is not as appealing as implied by the literature. We also show that although geo-indistinguishability guarantees a lower bound on the adversary's error, this comes at the cost of achieving poorer performance than other noise generation mechanisms in terms of average error, and enabling the possibility of exposing obfuscated locations that are useless from the quality of service point of view.},
  isbn = {978-1-4503-5175-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YFSMMX5M\\Oya et al. - 2017 - Is Geo-Indistinguishability What You Are Looking f.pdf}
}

@article{peng_unsupervised_nodate,
  title = {Unsupervised {{Membership Inference Attacks Against Machine Learning Models}}},
  author = {Peng, Yuefeng and Zhao, Bo and Liu, Hui},
  abstract = {As a form of privacy leakage for machine learning (ML), membership inference (MI) attacks aim to infer whether given data samples have been used to train a target ML model. Existing state-of-the-art MI attacks in black-box settings adopt a so-called shadow model to perform transfer attacks. Such attacks achieve high inference accuracy but have many adversarial assumptions, such as having a dataset from the same distribution as the target model's training data and knowledge of the target model structure. We propose a novel MI attack, called UMIA, which probes the target model in an unsupervised way without any shadow model. We relax all the adversarial assumptions above, demonstrating that MI attacks are applicable without any knowledge about the target model and its training set. We empirically show that, with far fewer adversarial assumptions and computational resources, UMIA can perform on bar with the state-of-the-art supervised MI attack.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8KGUBBPY\\Peng e.a. - Unsupervised Membership Inference Attacks Against .pdf}
}

@article{rand_objective_1971,
  title = {Objective Criteria for the Evaluation of Clustering Methods},
  author = {Rand, William M},
  year = {1971},
  journal = {Journal of the American Statistical association},
  volume = {66},
  number = {336},
  pages = {846--850},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459}
}

@misc{rigaki_survey_2021,
  title = {A {{Survey}} of {{Privacy Attacks}} in {{Machine Learning}}},
  author = {Rigaki, Maria and Garcia, Sebastian},
  year = {2021},
  month = apr,
  number = {arXiv:2007.07646},
  eprint = {2007.07646},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 40 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TYQY8NSE\\Rigaki en Garcia - 2021 - A Survey of Privacy Attacks in Machine Learning.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\9T48F9DU\\2007.html}
}

@article{rousseeuw_silhouettes_1987,
  title = {Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis},
  author = {Rousseeuw, Peter J},
  year = {1987},
  journal = {Journal of computational and applied mathematics},
  volume = {20},
  pages = {53--65},
  publisher = {{Elsevier}},
  issn = {0377-0427}
}

@article{sander_density-based_1998,
  title = {Density-{{Based Clustering}} in {{Spatial Databases}}: {{The Algorithm GDBSCAN}} and {{Its Applications}}},
  shorttitle = {Density-{{Based Clustering}} in {{Spatial Databases}}},
  author = {Sander, J{\"o}rg and Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  year = {1998},
  month = jun,
  journal = {Data Mining and Knowledge Discovery},
  volume = {2},
  number = {2},
  pages = {169--194},
  issn = {1573-756X},
  doi = {10.1023/A:1009745219419},
  urldate = {2023-04-13},
  abstract = {The clustering algorithm DBSCAN relies on a density-based notion of clusters and is designed to discover clusters of arbitrary shape as well as to distinguish noise. In this paper, we generalize this algorithm in two important directions. The generalized algorithm\textemdash called GDBSCAN\textemdash can cluster point objects as well as spatially extended objects according to both, their spatial and their nonspatial attributes. In addition, four applications using 2D points (astronomy), 3D points (biology), 5D points (earth science) and 2D polygons (geography) are presented, demonstrating the applicability of GDBSCAN to real-world problems.},
  langid = {english},
  keywords = {applications,clustering algorithms,efficiency,spatial databases},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7WSLG23Z\\Sander e.a. - 1998 - Density-Based Clustering in Spatial Databases The.pdf}
}

@inproceedings{saputra_effect_2020,
  title = {Effect of Distance Metrics in Determining K-Value in k-Means Clustering Using Elbow and Silhouette Method},
  booktitle = {Sriwijaya {{International Conference}} on {{Information Technology}} and {{Its Applications}} ({{SICONIAN}} 2019)},
  author = {SAPUTRA, Danny Matthew and SAPUTRA, Daniel and OSWARI, Liniyanti D},
  year = {2020},
  pages = {341--346},
  publisher = {{Atlantis Press}},
  isbn = {94-6252-963-9}
}

@article{schubert_dbscan_2017,
  title = {{{DBSCAN Revisited}}, {{Revisited}}: {{Why}} and {{How You Should}} ({{Still}}) {{Use DBSCAN}}},
  shorttitle = {{{DBSCAN Revisited}}, {{Revisited}}},
  author = {Schubert, Erich and Sander, J{\"o}rg and Ester, Martin and Kriegel, Hans Peter and Xu, Xiaowei},
  year = {2017},
  month = sep,
  journal = {ACM Transactions on Database Systems},
  volume = {42},
  number = {3},
  pages = {1--21},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/3068335},
  urldate = {2023-04-13},
  abstract = {At SIGMOD 2015, an article was presented with the title ``DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation'' that won the conference's best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\N7B96WT6\\Schubert e.a. - 2017 - DBSCAN Revisited, Revisited Why and How You Shoul.pdf}
}

@inproceedings{shejwalkar_revisiting_2019,
  title = {Revisiting Utility Metrics for Location Privacy-Preserving Mechanisms},
  booktitle = {Proceedings of the 35th {{Annual Computer Security Applications Conference}}},
  author = {Shejwalkar, Virat and Houmansadr, Amir and {Pishro-Nik}, Hossein and Goeckel, Dennis},
  year = {2019},
  month = dec,
  pages = {313--327},
  publisher = {{ACM}},
  address = {{San Juan Puerto Rico USA}},
  doi = {10.1145/3359789.3359829},
  urldate = {2023-03-17},
  abstract = {The literature has extensively studied various location privacypreserving mechanisms (LPPMs) in order to improve the location privacy of the users of location-based services (LBSes). Such privacy, however, comes at the cost of degrading the utility of the underlying LBSes. The main body of previous work has used a generic distance-only based metric to quantify the quality loss incurred while employing LPPMs. In this paper, we argue that using such generic utility metrics misleads the design and evaluation of LPPMs, since generic utility metrics do not capture the actual utility perceived by the users. We demonstrate this for ride-hailing services, a popular class of LBS with complex utility behavior. Specifically, we design a privacy-preserving ride-hailing service, called PRide, and demonstrate the significant distinction between its generic and tailored metrics. Through various experiments we show the significant implications of using generic utility metrics in the design and evaluation of LPPMs. Our work concludes that LPPM design and evaluation should use utility metrics that are tailored to the individual LBSes.},
  isbn = {978-1-4503-7628-0},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NS6QKNSZ\\Shejwalkar et al. - 2019 - Revisiting utility metrics for location privacy-pr.pdf}
}

@misc{shokri_membership_2017,
  title = {Membership {{Inference Attacks}} against {{Machine Learning Models}}},
  author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  year = {2017},
  month = mar,
  number = {arXiv:1610.05820},
  eprint = {1610.05820},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-03-30},
  abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HVDK3UKI\\Shokri e.a. - 2017 - Membership Inference Attacks against Machine Learn.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\QBBIYVXD\\1610.html}
}

@inproceedings{shokri_privacy-preserving_2015,
  title = {Privacy-Preserving Deep Learning},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC}} Conference on Computer and Communications Security},
  author = {Shokri, Reza and Shmatikov, Vitaly},
  year = {2015},
  pages = {1310--1321}
}

@article{solorio-fernandez_review_2020,
  title = {A Review of Unsupervised Feature Selection Methods},
  author = {{Solorio-Fern{\'a}ndez}, Sa{\'u}l and {Carrasco-Ochoa}, J. Ariel and {Mart{\'i}nez-Trinidad}, Jos{\'e} Fco.},
  year = {2020},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {2},
  pages = {907--948},
  issn = {1573-7462},
  doi = {10.1007/s10462-019-09682-y},
  abstract = {In recent years, unsupervised feature selection methods have raised considerable interest in many research areas; this is mainly due to their ability to identify and select relevant features without needing class label information. In this paper, we provide a comprehensive and structured review of the most relevant and recent unsupervised feature selection methods reported in the literature. We present a taxonomy of these methods and describe the main characteristics and the fundamental ideas they are based on. Additionally, we summarized the advantages and disadvantages of the general lines in which we have categorized the methods analyzed in this review. Moreover, an experimental comparison among the most representative methods of each approach is also presented. Finally, we discuss some important open challenges in this research area.}
}

@article{strehl_cluster_2002,
  title = {Cluster Ensembles---a Knowledge Reuse Framework for Combining Multiple Partitions},
  author = {Strehl, Alexander and Ghosh, Joydeep},
  year = {2002},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Dec},
  pages = {583--617},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CTQ3ZAYE\\Strehl en Ghosh - Cluster Ensembles – A Knowledge Reuse Framework fo.pdf}
}

@misc{sun_distributed_2019,
  title = {Distributed {{Clustering}} in the {{Anonymized Space}} with {{Local Differential Privacy}}},
  author = {Sun, Lin and Zhao, Jun and Ye, Xiaojun},
  year = {2019},
  month = jun,
  number = {arXiv:1906.11441},
  eprint = {1906.11441},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-22},
  abstract = {Clustering and analyzing on collected data can improve user experiences and quality of services in big data, IoT applications. However, directly releasing original data brings potential privacy concerns, which raises challenges and opportunities for privacy-preserving clustering. In this paper, we study the problem of non-interactive clustering in distributed setting under the framework of local differential privacy. We first extend the Bit Vector, a novel anonymization mechanism to be functionality-capable and privacy-preserving. Based on the modified encoding mechanism, we propose kCluster algorithm that can be used for clustering in the anonymized space. We show the modified encoding mechanism can be easily implemented in existing clustering algorithms that only rely on distance information, such as DBSCAN. Theoretical analysis and experimental results validate the effectiveness of the proposed schemes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Databases},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8VTXE9HS\\Sun e.a. - 2019 - Distributed Clustering in the Anonymized Space wit.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\IRX6PFZC\\1906.html}
}

@article{sun_privbv_2022,
  title = {{{PrivBV}}: {{Distance-aware}} Encoding for Distributed Data with Local Differential Privacy},
  shorttitle = {{{PrivBV}}},
  author = {Sun, Lin and Ping, Guolou and Ye, Xiaojun},
  year = {2022},
  month = apr,
  journal = {Tsinghua Science and Technology},
  volume = {27},
  number = {2},
  pages = {412--421},
  issn = {1007-0214},
  doi = {10.26599/TST.2021.9010027},
  abstract = {Recently, local differential privacy (LDP) has been used as the de facto standard for data sharing and analyzing with high-level privacy guarantees. Existing LDP-based mechanisms mainly focus on learning statistical information about the entire population from sensitive data. For the first time in the literature, we use LDP for distance estimation between distributed datato support more complicated data analysis. Specifically, we propose PrivBV\textemdash a locally differentially private bit vector mechanism with a distance-aware property in the anonymized space. We also present an optimization strategy for reducing privacy leakage in the high-dimensional space. The distance-aware property of PrivBV brings new insights into complicated data analysis in distributed environments. As study cases, we show the feasibility of applying PrivBV to privacy-preserving record linkage and non-interactive clustering. Theoretical analysis and experimental results demonstrate the effectiveness of the proposed scheme.},
  keywords = {Data analysis,Differential privacy,Distributed databases,Encoding,Estimation,local differential privacy,non-interactive clustering,Privacy,privacy-preserving data publishing,Sun},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\B8WDVH5J\\Sun e.a. - 2022 - PrivBV Distance-aware encoding for distributed da.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\JPL3C98G\\9552667.html}
}

@article{tibshirani_estimating_2001,
  title = {Estimating the Number of Clusters in a Data Set via the Gap Statistic},
  author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
  year = {2001},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {63},
  number = {2},
  pages = {411--423},
  publisher = {{Wiley Online Library}},
  issn = {1369-7412}
}

@misc{valdenegro-toro_machine_2022,
  title = {Machine {{Learning Students Overfit}} to {{Overfitting}}},
  author = {{Valdenegro-Toro}, Matias and Sabatelli, Matthia},
  year = {2022},
  month = sep,
  number = {arXiv:2209.03032},
  eprint = {2209.03032},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Overfitting and generalization is an important concept in Machine Learning as only models that generalize are interesting for general applications. Yet some students have trouble learning this important concept through lectures and exercises. In this paper we describe common examples of students misunderstanding overfitting, and provide recommendations for possible solutions. We cover student misconceptions about overfitting, about solutions to overfitting, and implementation mistakes that are commonly confused with overfitting issues. We expect that our paper can contribute to improving student understanding and lectures about this important topic.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BS8SVVPZ\\Valdenegro-Toro en Sabatelli - 2022 - Machine Learning Students Overfit to Overfitting.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\E7UT2LZS\\2209.html}
}

@article{vinh_information_nodate-2,
  title = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}: {{Variants}}, {{Properties}}, {{Normalization}} and {{Correction}} for {{Chance}}},
  author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
  abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BB3A2I9Q\\Vinh e.a. - Information Theoretic Measures for Clusterings Com.pdf}
}

@article{wagner_comparing_nodate,
  title = {Comparing {{Clusterings}} - {{An Overview}}},
  author = {Wagner, Silke and Wagner, Dorothea},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\P4IVDUGW\\Wagner en Wagner - Comparing Clusterings - An Overview.pdf}
}

@article{wang_adaptive_2007,
  title = {Adaptive {{Affinity Propagation Clustering}}},
  author = {Wang, Kaijun and Zhang, Junying and Li, Dan and Zhang, Xinna and Guo, Tao},
  year = {2007},
  abstract = {Affinity propagation clustering (AP) has two limitations: it is hard to know what value of parameter `preference' can yield an optimal clustering solution, and oscillations cannot be eliminated automatically if occur. The adaptive AP method is proposed to overcome these limitations, including adaptive scanning of preferences to search space of the number of clusters for finding the optimal clustering solution, adaptive adjustment of damping factors to eliminate oscillations, and adaptive escaping from oscillations when the damping adjustment technique fails. Experimental results on simulated and real data sets show that the adaptive AP is effective and can outperform AP in quality of clustering results.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UFTDF2JZ\\Wang e.a. - 2007 - Adaptive Affinity Propagation Clustering.pdf}
}

@article{warrens_understanding_2022,
  title = {Understanding the {{Adjusted Rand Index}} and {{Other Partition Comparison Indices Based}} on {{Counting Object Pairs}}},
  author = {Warrens, Matthijs J. and {van der Hoef}, Hanneke},
  year = {2022},
  month = nov,
  journal = {Journal of Classification},
  volume = {39},
  number = {3},
  pages = {487--509},
  issn = {1432-1343},
  doi = {10.1007/s00357-022-09413-z},
  abstract = {In unsupervised machine learning, agreement between partitions is commonly assessed with so-called external validity indices. Researchers tend to use and report indices that quantify agreement between two partitions for all clusters simultaneously. Commonly used examples are the Rand index and the adjusted Rand index. Since these overall measures give a general notion of what is going on, their values are usually hard to interpret. The goal of this study is to provide a thorough understanding of the adjusted Rand index as well as many other partition comparison indices based on counting object pairs. It is shown that many overall indices based on the pair-counting approach can be decomposed into indices that reflect the degree of agreement on the level of individual clusters. The decompositions (1) show that the overall indices can be interpreted as summary statistics of the agreement on the cluster level, (2) specify how these overall indices are related to the indices for individual clusters, and (3) show that the overall indices are affected by cluster size imbalance: if cluster sizes are unbalanced these overall measures will primarily reflect the degree of agreement between the partitions on the large clusters, and will provide much less information on the agreement on smaller clusters. Furthermore, the value of Rand-like indices is determined to a large extent by the number of pairs of objects that are not joined in either of the partitions.}
}

@article{wood_differential_2018,
  title = {Differential {{Privacy}}: {{A Primer}} for a {{Non-Technical Audience}}},
  shorttitle = {Differential {{Privacy}}},
  author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
  year = {2018},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3338027},
  urldate = {2023-03-17},
  abstract = {This document is a primer on differential privacy, which is a formal mathematical framework for guaranteeing privacy protection when analyzing or releasing statistical data. Recently emerging from the theoretical computer science literature, differential privacy is now in initial stages of implementation and use in various academic, industry, and government settings. Using intuitive illustrations and limited mathematical formalism, this document provides an introduction to differential privacy for non-technical practitioners, who are increasingly tasked with making decisions with respect to differential privacy as it grows more widespread in use. In particular, the examples in this document illustrate ways in which social scientists can conceptualize the guarantees provided by differential privacy with respect to the decisions they make when managing personal data about research subjects and informing them about the privacy protection they will be afforded.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\Y9WGZU9N\\Wood et al. - 2018 - Differential Privacy A Primer for a Non-Technical.pdf}
}

@article{xia_distributed_2020-1,
  title = {Distributed {{K-Means}} Clustering Guaranteeing Local Differential Privacy},
  author = {Xia, Chang and Hua, Jingyu and Tong, Wei and Zhong, Sheng},
  year = {2020},
  journal = {Computers \& Security},
  volume = {90},
  pages = {101699},
  publisher = {{Elsevier}},
  issn = {0167-4048}
}

@article{yan_efficient_2020,
  title = {An Efficient Unsupervised Feature Selection Procedure through Feature Clustering},
  author = {Yan, Xuyang and Nazmi, Shabnam and Erol, Berat A. and Homaifar, Abdollah and Gebru, Biniam and Tunstel, Edward},
  year = {2020},
  month = mar,
  journal = {Pattern Recognition Letters},
  volume = {131},
  pages = {277--284},
  issn = {01678655},
  doi = {10.1016/j.patrec.2019.12.022},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8Z855R66\\Yan e.a. - 2020 - An efficient unsupervised feature selection proced.pdf}
}

@article{yan_perturb_2022-2,
  title = {Perturb and Optimize Users' Location Privacy Using Geo-Indistinguishability and Location Semantics},
  author = {Yan, Yan and Xu, Fei and Mahmood, Adnan and Dong, Zhuoyue and Sheng, Quan Z.},
  year = {2022},
  month = nov,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {20445},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-24893-0},
  urldate = {2023-02-26},
  abstract = {Location-based services (LBS) are capable of providing location-based information retrieval, traffic navigation, entertainment services, emergency rescues, and several similar services primarily on the premise of the geographic location of users or mobile devices. However, in the process of introducing a new user experience, it is also easy to expose users' specific location which can result in more private information leakage. Hence, the protection of location privacy remains one of the critical issues of the location-based services. Moreover, the areas where humans work and live have different location semantics and sensitivities according to their different social functions. Although the privacy protection of a user's real location can be achieved by the perturbation algorithm, the attackers may employ the semantics information of the perturbed location to infer a user's real location semantics in an attempt to spy on a user's privacy to certain extent. In order to mitigate the above semantics inference attack, and further improve the quality of the location-based services, this paper hereby proposes a user side location perturbation and optimization algorithm based on geo-indistinguishability and location semantics. The perturbation area satisfying geo-indistinguishability is thus generated according to the planar Laplace mechanism and optimized by combining the semantics information and time characteristics of the location. The optimum perturbed location that is able to satisfy the minimum loss of location-based service quality is selected via a linear programming method, and can be employed to replace the real location of the user so as to prevent the leakage of the privacy. Experimental comparison of the actual road network and location semantics dataset manifests that the proposed method reduces approximately 37\% perturbation distance in contrast to the other state-of-the-art methods, maintains considerably lower similarity of location semantics, and improves region counting query accuracy by a margin of around 40\%.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Energy science and technology,Mathematics and computing},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8VFPAXY2\\Yan et al. - 2022 - Perturb and optimize users’ location privacy using.pdf}
}

@inproceedings{yeom_privacy_2018,
  title = {Privacy {{Risk}} in {{Machine Learning}}: {{Analyzing}} the {{Connection}} to {{Overfitting}}},
  shorttitle = {Privacy {{Risk}} in {{Machine Learning}}},
  booktitle = {2018 {{IEEE}} 31st {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  year = {2018},
  month = jul,
  pages = {268--282},
  publisher = {{IEEE}},
  address = {{Oxford}},
  doi = {10.1109/CSF.2018.00027},
  urldate = {2023-03-30},
  abstract = {Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.},
  isbn = {978-1-5386-6680-7},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7SZ28HF8\\Yeom e.a. - 2018 - Privacy Risk in Machine Learning Analyzing the Co.pdf}
}

@article{yuan_research_2019,
  title = {Research on {{K-value}} Selection Method of {{K-means}} Clustering Algorithm},
  author = {Yuan, Chunhui and Yang, Haitao},
  year = {2019},
  journal = {J},
  volume = {2},
  number = {2},
  pages = {226--235},
  publisher = {{MDPI}},
  issn = {2571-8800}
}

@inproceedings{zhao_not_2020,
  title = {Not One but Many {{Tradeoffs}}: {{Privacy Vs}}. {{Utility}} in {{Differentially Private Machine Learning}}},
  shorttitle = {Not One but Many {{Tradeoffs}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGSAC Conference}} on {{Cloud Computing Security Workshop}}},
  author = {Zhao, Benjamin Zi Hao and Kaafar, Mohamed Ali and Kourtellis, Nicolas},
  year = {2020},
  month = nov,
  eprint = {2008.08807},
  primaryclass = {cs},
  pages = {15--26},
  doi = {10.1145/3411495.3421352},
  urldate = {2023-04-08},
  abstract = {Data holders are increasingly seeking to protect their user's privacy, whilst still maximizing their ability to produce machine models with high quality predictions. In this work, we empirically evaluate various implementations of differential privacy (DP), and measure their ability to fend off real-world privacy attacks, in addition to measuring their core goal of providing accurate classifications. We establish an evaluation framework to ensure each of these implementations are fairly evaluated. Our selection of DP implementations add DP noise at different positions within the framework, either at the point of data collection/release, during updates while training of the model, or after training by perturbing learned model parameters. We evaluate each implementation across a range of privacy budgets, and datasets, each implementation providing the same mathematical privacy guarantees. By measuring the models' resistance to real world attacks of membership and attribute inference, and their classification accuracy. we determine which implementations provide the most desirable tradeoff between privacy and utility. We found that the number of classes of a given dataset is unlikely to influence where the privacy and utility tradeoff occurs. Additionally, in the scenario that high privacy constraints are required, perturbing input training data does not trade off as much utility, as compared to noise added later in the ML process.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\F2TURZMZ\\Zhao et al. - 2020 - Not one but many Tradeoffs Privacy Vs. Utility in.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\6985TBUZ\\2008.html}
}
