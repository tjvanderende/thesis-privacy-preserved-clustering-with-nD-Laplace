@inproceedings{10.1145/1835804.1835848,
  title = {Unsupervised Feature Selection for Multi-Cluster Data},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Cai, Deng and Zhang, Chiyuan and He, Xiaofei},
  year = {2010},
  series = {{{KDD}} '10},
  pages = {333--342},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1835804.1835848},
  abstract = {In many data analysis tasks, one is often confronted with very high dimensional data. Feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering, classification and retrieval. In this paper, we consider the feature selection problem in unsupervised learning scenario, which is particularly difficult due to the absence of class labels that would guide the search for relevant information. The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive. Traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature. These approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset. Inspired from the recent developments on manifold learning and L1-regularized models for subset selection, we propose in this paper a new approach, called Multi-Cluster Feature Selection (MCFS), for unsupervised feature selection. Specifically, we select those features such that the multi-cluster structure of the data can be best preserved. The corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem. Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm.},
  isbn = {978-1-4503-0055-1},
  keywords = {clustering,feature selection,unsupervised}
}

@article{1056489,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  doi = {10.1109/TIT.1982.1056489}
}

@article{9646489,
  title = {{{3D Geo-Indistinguishability}} for {{Indoor Location-Based Services}}},
  author = {Min, Minghui and Xiao, Liang and Ding, Jiahao and Zhang, Hongliang and Li, Shiyin and Pan, Miao and Han, Zhu},
  year = {2022},
  journal = {IEEE Transactions on Wireless Communications},
  volume = {21},
  number = {7},
  pages = {4682--4694},
  doi = {10.1109/TWC.2021.3132464},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\QPJZTQGX\\Min e.a. - 2022 - 3D geo-indistinguishability for indoor location-ba.pdf}
}

@inproceedings{9679364,
  title = {Private Distributed {{K-means}} Clustering on Interval Data},
  booktitle = {2021 {{IEEE}} International Performance, Computing, and Communications Conference ({{IPCCC}})},
  author = {Huang, D. and Yao, X. and An, S. and Ren, S.},
  year = {2021},
  month = oct,
  pages = {1--9},
  publisher = {{IEEE Computer Society}},
  address = {{Los Alamitos, CA, USA}},
  doi = {10.1109/IPCCC51483.2021.9679364},
  abstract = {K-means clustering has been heavily employed to mine valuable insights from interval data. Nevertheless, serious privacy leakage concerns are stumbling blocks impeding its widespread application. To quantify the privacy of small and large-scale interval data, we introduce two notions of \&amp;\#x03B1;-Condensed Local Differential Privacy and \&amp;\#x03F5;-Local Differential Privacy, and propose two distance-aware perturbation mechanisms of \&amp;\#x03B1;-exponential and square wave mechanisms. Rigorous theoretical analysis proves that our proposed mechanisms satisfy these two privacy notions. The experimental results built on multiple synthesized and real datasets show that our proposed mechanisms can provide more accurate clustering results than prior work, such as Randomized Response, Generalized Randomized Response, and Optimized Local Hash.},
  keywords = {conferences,correlation,differential privacy,distributed databases,perturbation methods,privacy},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6F8LBU8D\\Huang e.a. - 2021 - Private distributed K-means clustering on interval.pdf}
}

@article{ahmed_k-means_2020-1,
  title = {The K-Means Algorithm: {{A}} Comprehensive Survey and Performance Evaluation},
  author = {Ahmed, Mohiuddin and Seraj, Raihan and Islam, Syed Mohammed Shamsul},
  year = {2020},
  journal = {Electronics},
  volume = {9},
  number = {8},
  pages = {1295},
  publisher = {{MDPI}},
  issn = {2079-9292}
}

@misc{ahuja_utility-preserving_2019,
  title = {A {{Utility-Preserving}} and {{Scalable Technique}} for {{Protecting Location Data}} with {{Geo-Indistinguishability}}},
  author = {Ahuja, Ritesh and Ghinita, Gabriel and Shahabi, Cyrus},
  year = {2019},
  publisher = {{OpenProceedings.org}},
  doi = {10.5441/002/EDBT.2019.20},
  urldate = {2023-02-26},
  abstract = {Location-based apps provide users with personalized services tailored to their geographical position. This is highly-beneficial for mobile users, who are able to find points of interest close to their location, or connect with nearby friends. However, sharing location data with service providers also introduces privacy concerns. An adversary with access to fine-grained user locations can infer private details about individuals. Geo-indistinguishability (GeoInd) adapts the popular differential privacy (DP) model to make it suitable for protecting users' location information. However, existing techniques that implement GeoInd have major drawbacks. Some solutions, such as the planar Laplace mechanism, significantly lower data utility by adding excessive noise. Other approaches, such as the optimal mechanism, achieve good utility, but only work for small sets of candidate locations due to the use of computationally-expensive linear programming. In most cases, locations are used to answer online queries, so a quick response time is essential. In this paper, we propose a technique that achieves GeoInd and scales to large datasets while preserving data utility. Our central idea is to use the composability property of GeoInd to create a multiple-step algorithm that can be used in conjunction with a spatial index. We preserve utility by applying accurate GeoInd mechanisms and we achieve scalability by pruning the solution search space with the help of the index when seeking high-utility outcomes. Our extensive performance evaluation on top of real location datasets from social media apps shows that the proposed technique outperforms significantly the benchmark in terms of utility and/or computational overhead.},
  langid = {english},
  keywords = {Database Technology},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VN38ZL5N\\Ahuja et al. - 2019 - A Utility-Preserving and Scalable Technique for Pr.pdf}
}

@misc{aitsam_differential_2021,
  title = {Differential {{Privacy Made Easy}}},
  author = {Aitsam, Muhammad},
  year = {2021},
  month = dec,
  number = {arXiv:2201.00099},
  eprint = {2201.00099},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-04},
  abstract = {Data privacy is a major issue for many decades, several techniques have been developed to make sure individuals' privacy but still world has seen privacy failures. In 2006, Cynthia Dwork gave the idea of Differential Privacy which gave strong theoretical guarantees for data privacy. Many companies and research institutes developed differential privacy libraries, but in order to get the differentially private results, users have to tune the privacy parameters. In this paper, we minimized these tune-able parameters. The DP-framework is developed which compares the differentially private results of three Python based DP libraries. We also introduced a new very simple DP library (GRAM-DP), so the people with no background of differential privacy can still secure the privacy of the individuals in the dataset while releasing statistical results in public.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JTBF73ML\\Aitsam - 2021 - Differential Privacy Made Easy.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\T5AM4CEC\\2201.html}
}

@article{alvim_local_nodate,
  title = {Local {{Differential Privacy}} on {{Metric Spaces}}: Optimizing the Trade-off with Utility},
  author = {Alvim, Mario S and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Pazii, Anna},
  abstract = {Local differential privacy (LPD) is a distributed variant of differential privacy (DP) in which the obfuscation of the sensitive information is done at the level of the individual records, and in general it is used to sanitize data that are collected for statistical purposes. LPD has the advantage it does not need to assume a trusted third party. On the other hand LDP in general requires more noise than DP to achieve the same level of protection, with negative consequences on the utility. In practice, utility becomes acceptable only on very large collections of data, and this is the reason why LDP is especially successful among big companies such as Apple and Google, which can count on a huge number of users. In this paper, we propose a variant of LDP suitable for metric spaces, such as location data or energy consumption data, and we show that it provides a much better utility for the same level of privacy.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VXCWK8LH\\Alvim et al. - Local Differential Privacy on Metric Spaces optim.pdf}
}

@article{alvim_local_nodate-1,
  title = {Local {{Differential Privacy}} on {{Metric Spaces}}: Optimizing the Trade-off with Utility},
  author = {Alvim, Mario S and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Pazii, Anna},
  abstract = {Local differential privacy (LPD) is a distributed variant of differential privacy (DP) in which the obfuscation of the sensitive information is done at the level of the individual records, and in general it is used to sanitize data that are collected for statistical purposes. LPD has the advantage it does not need to assume a trusted third party. On the other hand LDP in general requires more noise than DP to achieve the same level of protection, with negative consequences on the utility. In practice, utility becomes acceptable only on very large collections of data, and this is the reason why LDP is especially successful among big companies such as Apple and Google, which can count on a huge number of users. In this paper, we propose a variant of LDP suitable for metric spaces, such as location data or energy consumption data, and we show that it provides a much better utility for the same level of privacy.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JDURNA3E\\Alvim et al. - Local Differential Privacy on Metric Spaces optim.pdf}
}

@misc{alvim_metric-based_2018-1,
  title = {Metric-Based Local Differential Privacy for Statistical Applications},
  author = {Alvim, M{\'a}rio S. and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Pazii, Anna},
  year = {2018},
  month = may,
  number = {arXiv:1805.01456},
  eprint = {1805.01456},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-24},
  abstract = {Local differential privacy (LPD) is a distributed variant of differential privacy (DP) in which the obfuscation of the sensitive information is done at the level of the individual records, and in general it is used to sanitize data that are collected for statistical purposes. LPD has the advantage it does not need to assume a trusted third party. On the other hand LDP in general requires more noise than DP to achieve the same level of protection, with negative consequences on the utility. In practice, utility becomes acceptable only on very large collections of data, and this is the reason why LDP is especially successful among big companies such as Apple and Google, which can count on a huge number of users. In this paper, we propose a variant of LDP suitable for metric spaces, such as location data or energy consumption data, and we show that it provides a much better utility for the same level of privacy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MKE6FHYG\\Alvim et al. - 2018 - Metric-based local differential privacy for statis.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\VQ4T8CX6\\1805.html}
}

@incollection{bailey_privacy_2016,
  title = {Privacy {{Aware K-Means Clustering}} with {{High Utility}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Nguyen, Thanh Dai and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
  editor = {Bailey, James and Khan, Latifur and Washio, Takashi and Dobbie, Gill and Huang, Joshua Zhexue and Wang, Ruili},
  year = {2016},
  volume = {9652},
  pages = {388--400},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-31750-2_31},
  urldate = {2023-03-15},
  isbn = {978-3-319-31749-6 978-3-319-31750-2},
  langid = {english}
}

@misc{balle_improving_2018,
  title = {Improving the {{Gaussian Mechanism}} for {{Differential Privacy}}: {{Analytical Calibration}} and {{Optimal Denoising}}},
  shorttitle = {Improving the {{Gaussian Mechanism}} for {{Differential Privacy}}},
  author = {Balle, Borja and Wang, Yu-Xiang},
  year = {2018},
  month = jun,
  number = {arXiv:1805.06530},
  eprint = {1805.06530},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-04},
  abstract = {The Gaussian mechanism is an essential building block used in multitude of differentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime (\$\textbackslash varepsilon \textbackslash to 0\$) and it cannot be extended to the low privacy regime (\$\textbackslash varepsilon \textbackslash to \textbackslash infty\$). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the high-dimensional regime.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZNLKHPG4\\Balle and Wang - 2018 - Improving the Gaussian Mechanism for Differential .pdf;C\:\\Users\\tjvan\\Zotero\\storage\\HFBS6V2H\\1805.html}
}

@misc{baraheem_survey_2022,
  title = {A {{Survey}} on {{Differential Privacy}} with {{Machine Learning}} and {{Future Outlook}}},
  author = {Baraheem, Samah and Yao, Zhongmei},
  year = {2022},
  month = nov,
  number = {arXiv:2211.10708},
  eprint = {2211.10708},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-27},
  abstract = {Nowadays, machine learning models and applications have become increasingly pervasive. With this rapid increase in the development and employment of machine learning models, a concern regarding privacy has risen. Thus, there is a legitimate need to protect the data from leaking and from any attacks. One of the strongest and most prevalent privacy models that can be used to protect machine learning models from any attacks and vulnerabilities is differential privacy (DP). DP is strict and rigid definition of privacy, where it can guarantee that an adversary is not capable to reliably predict if a specific participant is included in the dataset or not. It works by injecting a noise to the data whether to the inputs, the outputs, the ground truth labels, the objective functions, or even to the gradients to alleviate the privacy issue and protect the data. To this end, this survey paper presents different differentially private machine learning algorithms categorized into two main categories (traditional machine learning models vs. deep learning models). Moreover, future research directions for differential privacy with machine learning algorithms are outlined.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\96UJ8546\\Baraheem and Yao - 2022 - A Survey on Differential Privacy with Machine Lear.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\GJPWLYYR\\2211.html}
}

@misc{bashir_information-theoretic_2020,
  title = {An {{Information-Theoretic Perspective}} on {{Overfitting}} and {{Underfitting}}},
  author = {Bashir, Daniel and Montanez, George D. and Sehra, Sonia and Segura, Pedro Sandoval and Lauw, Julius},
  year = {2020},
  month = nov,
  number = {arXiv:2010.06076},
  eprint = {2010.06076},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {We present an information-theoretic framework for understanding overfitting and underfitting in machine learning and prove the formal undecidability of determining whether an arbitrary classification algorithm will overfit a dataset. Measuring algorithm capacity via the information transferred from datasets to models, we consider mismatches between algorithm capacities and datasets to provide a signature for when a model can overfit or underfit a dataset. We present results upper-bounding algorithm capacity, establish its relationship to quantities in the algorithmic search framework for machine learning, and relate our work to recent information-theoretic approaches to generalization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MFVDL5Q9\\Bashir e.a. - 2020 - An Information-Theoretic Perspective on Overfittin.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\ANXC3L8Z\\2010.html}
}

@book{blum_practical_2005,
  title = {Practical Privacy: {{The SulQ}} Framework},
  author = {Blum, Avrim and Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi},
  year = {2005},
  month = jun,
  journal = {Proceedings of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  pages = {138},
  doi = {10.1145/1065167.1065184}
}

@misc{bozdemir_privacy-preserving_nodate,
  title = {Privacy-Preserving {{Density-based Clustering}}},
  author = {Bozdemir, Beyza and Canard, S{\'e}bastien and Ermis, Orhan and M{\"o}llering, Helen and {\"O}nen, Melek and Schneider, Thomas},
  urldate = {2023-03-30},
  abstract = {Clustering is an unsupervised machine learning technique that outputs clusters containing similar data items. In this work, we investigate privacy-preserving density-based clustering which is, for example, used in financial analytics and medical diagnosis. When (multiple) data owners collaborate or outsource the computation, privacy concerns arise. To address this problem, we design, implement, and evaluate the first practical and fully private density-based clustering scheme based on secure two-party computation. Our protocol privately executes the DBSCAN algorithm without disclosing any information (including the number and size of clusters). It can be used for private clustering between two parties as well as for private outsourcing of an arbitrary number of data owners to two non-colluding servers. Our implementation of the DBSCAN algorithm privately clusters data sets with 400 elements in 7 minutes on commodity hardware. Thereby, it flexibly determines the number of required clusters and is insensitive to outliers, while being only factor 19x slower than today's fastest private K-means protocol (Mohassel et al., PETS'20) which can only be used for specific data sets. We then show how to transfer our newly designed protocol to related clustering algorithms by introducing a private approximation of the TRACLUS algorithm for trajectory clustering which has interesting real-world applications like financial time series forecasts and the investigation of the spread of a disease like COVID-19.},
  keywords = {Clustering,Private Machine Learning,Secure Computation}
}

@misc{brendel_decision-based_2018,
  title = {Decision-{{Based Adversarial Attacks}}: {{Reliable Attacks Against Black-Box Machine Learning Models}}},
  shorttitle = {Decision-{{Based Adversarial Attacks}}},
  author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  year = {2018},
  month = feb,
  number = {arXiv:1712.04248},
  eprint = {1712.04248},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.04248},
  urldate = {2023-04-10},
  abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BWBD3ZFH\\Brendel et al. - 2018 - Decision-Based Adversarial Attacks Reliable Attac.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\4N4IJ62K\\1712.html}
}

@inproceedings{cai_dp-ap_2020,
  title = {{{DP-AP}}: {{Differential Privacy-Preserving Affinity Propagation Clustering}}},
  shorttitle = {{{DP-AP}}},
  booktitle = {2020 {{IEEE}} 14th {{International Conference}} on {{Big Data Science}} and {{Engineering}} ({{BigDataSE}})},
  author = {Cai, Hanbo and Wang, Jinyan and Liu, Xiaohong and Li, Xianxian},
  year = {2020},
  month = dec,
  pages = {73--79},
  publisher = {{IEEE}},
  address = {{Guangzhou, China}},
  doi = {10.1109/BigDataSE50710.2020.00018},
  urldate = {2022-12-22},
  isbn = {978-1-66540-396-2},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\LH7Y4ZSA\\Cai e.a. - 2020 - DP-AP Differential Privacy-Preserving Affinity Pr.pdf}
}

@article{calinski_dendrite_1974-1,
  title = {A Dendrite Method for Cluster Analysis},
  author = {Cali{\'n}ski, Tadeusz and Harabasz, Jerzy},
  year = {1974},
  journal = {Communications in Statistics-theory and Methods},
  volume = {3},
  number = {1},
  pages = {1--27},
  publisher = {{Taylor \& Francis}},
  issn = {0090-3272}
}

@misc{carlini_membership_2022,
  title = {Membership {{Inference Attacks From First Principles}}},
  author = {Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  year = {2022},
  month = apr,
  number = {arXiv:2112.03570},
  eprint = {2112.03570},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-14},
  abstract = {A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model's training dataset. These attacks are currently evaluated using average-case "accuracy" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., {$<$}0.1\%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is 10x more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\X2D3DFW4\\Carlini et al. - 2022 - Membership Inference Attacks From First Principles.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\B7TASZ9G\\2112.html}
}

@article{chatzikokolakis_constructing_2015,
  title = {Constructing Elastic Distinguishability Metrics for Location Privacy},
  author = {Chatzikokolakis, Konstantinos and Palamidessi, Catuscia and Stronati, Marco},
  year = {2015},
  month = jun,
  journal = {Proceedings on Privacy Enhancing Technologies},
  volume = {2015},
  number = {2},
  eprint = {1503.00756},
  primaryclass = {cs},
  pages = {156--170},
  issn = {2299-0984},
  doi = {10.1515/popets-2015-0023},
  urldate = {2023-03-10},
  abstract = {With the increasing popularity of hand-held devices, location-based applications and services have access to accurate and real-time location information, raising serious privacy concerns for their users. The recently introduced notion of geo-indistinguishability tries to address this problem by adapting the well-known concept of differential privacy to the area of location-based systems. Although geo-indistinguishability presents various appealing aspects, it has the problem of treating space in a uniform way, imposing the addition of the same amount of noise everywhere on the map. In this paper we propose a novel elastic distinguishability metric that warps the geometrical distance, capturing the different degrees of density of each area. As a consequence, the obtained mechanism adapts the level of noise while achieving the same degree of privacy everywhere. We also show how such an elastic metric can easily incorporate the concept of a "geographic fence" that is commonly employed to protect the highly recurrent locations of a user, such as his home or work. We perform an extensive evaluation of our technique by building an elastic metric for Paris' wide metropolitan area, using semantic information from the OpenStreetMap database. We compare the resulting mechanism against the Planar Laplace mechanism satisfying standard geo-indistinguishability, using two real-world datasets from the Gowalla and Brightkite location-based social networks. The results show that the elastic mechanism adapts well to the semantics of each area, adjusting the noise as we move outside the city center, hence offering better overall privacy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BBGUL4GI\\Chatzikokolakis e.a. - 2015 - Constructing elastic distinguishability metrics fo.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\FI7V7IG2\\1503.html}
}

@article{chatzikokolakis_efficient_2017,
  title = {Efficient Utility Improvement for Location Privacy},
  author = {Chatzikokolakis, Konstantinos and Elsalamouny, Ehab and Palamidessi, Catuscia},
  year = {2017},
  journal = {Proceedings on Privacy Enhancing Technologies},
  volume = {2017},
  number = {4},
  pages = {308--328}
}

@article{chatzikokolakis_practical_nodate,
  title = {Practical {{Mechanisms}} for {{Location Privacy}}},
  author = {Chatzikokolakis, Konstantinos and ElSalamouny, Ehab and Palamidessi, Catuscia},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\355U7PAM\\Chatzikokolakis et al. - Practical Mechanisms for Location Privacy.pdf}
}

@book{chawla_proceedings_2020,
  title = {Proceedings of the {{Fourteenth Annual ACM-SIAM Symposium}} on {{Discrete Algorithms}}},
  editor = {Chawla, Shuchi},
  year = {2020},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, PA}},
  doi = {10.1137/1.9781611975994},
  urldate = {2023-05-15},
  abstract = {We design a new algorithm for the Euclidean k-means problem that operates in the local model of differential privacy. Unlike in the non-private literature, differentially private algorithms for the k-means objective incur both additive and multiplicative errors. Our algorithm significantly reduces the additive error while keeping the multiplicative error the same as in previous state-of-the-art results. Specifically, on a database of size n, our algorithm guarantees O(1) multiplicative error and {$\approx$} n1/2+a additive error for an arbitrarily small constant a {$>$} 0. All previous algorithms in the local model had additive error {$\approx$} n2/3+a. Our techniques extend to k-median clustering.},
  isbn = {978-1-61197-599-4},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NNDWPXSM\\Chawla - 2020 - Proceedings of the Fourteenth Annual ACM-SIAM Symp.pdf}
}

@misc{chen_hopskipjumpattack_2020,
  title = {{{HopSkipJumpAttack}}: {{A Query-Efficient Decision-Based Attack}}},
  shorttitle = {{{HopSkipJumpAttack}}},
  author = {Chen, Jianbo and Jordan, Michael I. and Wainwright, Martin J.},
  year = {2020},
  month = apr,
  number = {arXiv:1904.02144},
  eprint = {1904.02144},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for \$\textbackslash ell\_2\$ and \$\textbackslash ell\_\textbackslash infty\$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4QQPRZVS\\Chen e.a. - 2020 - HopSkipJumpAttack A Query-Efficient Decision-Base.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\5JE3JZKJ\\1904.html}
}

@article{chicco_ten_2017,
  title = {Ten Quick Tips for Machine Learning in Computational Biology},
  author = {Chicco, Davide},
  year = {2017},
  month = dec,
  journal = {BioData Mining},
  volume = {10},
  pages = {35},
  issn = {1756-0381},
  doi = {10.1186/s13040-017-0155-3},
  urldate = {2023-04-13},
  abstract = {Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.},
  pmcid = {PMC5721660},
  pmid = {29234465},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\79L4QCKN\\Chicco - 2017 - Ten quick tips for machine learning in computation.pdf}
}

@misc{choquette-choo_label-only_2021,
  title = {Label-{{Only Membership Inference Attacks}}},
  author = {{Choquette-Choo}, Christopher A. and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  year = {2021},
  month = dec,
  number = {arXiv:2007.14321},
  eprint = {2007.14321},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\WEU62HCZ\\Choquette-Choo e.a. - 2021 - Label-Only Membership Inference Attacks.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\KB6KNX7Y\\2007.html}
}

@inproceedings{cormode_privacy_2018-1,
  title = {Privacy at {{Scale}}: {{Local Differential Privacy}} in {{Practice}}},
  shorttitle = {Privacy at {{Scale}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Cormode, Graham and Jha, Somesh and Kulkarni, Tejas and Li, Ninghui and Srivastava, Divesh and Wang, Tianhao},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {1655--1658},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3183713.3197390},
  urldate = {2023-05-03},
  abstract = {Local differential privacy (LDP), where users randomly perturb their inputs to provide plausible deniability of their data without the need for a trusted party, has been adopted recently by several major technology organizations, including Google, Apple and Microsoft. This tutorial aims to introduce the key technical underpinnings of these deployed systems, to survey current research that addresses related problems within the LDP model, and to identify relevant open problems and research directions for the community.},
  isbn = {978-1-4503-4703-7},
  keywords = {data collection,differential privacy,local differential privacy,privacy},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NHJ6PSHC\\Cormode et al. - 2018 - Privacy at Scale Local Differential Privacy in Pr.pdf}
}

@article{craenendonck_using_nodate,
  title = {Using {{Internal Validity Measures}} to {{Compare Clustering Algorithms}}},
  author = {Craenendonck, Toon Van and Blockeel, Hendrik},
  abstract = {Recently, significant effort has been made to automate the machine learning process in the context of supervised learning. This automation includes, amongst other things, the selection of an appropriate learning algorithm and corresponding hyperparameters for a particular learning problem. In contrast, such problems are much less studied for unsupervised tasks such as clustering. Nevertheless, users who want to cluster a data set are confronted with similar problems: a clustering algorithm should be selected from the wide variety of available algorithms, and usually some hyperparameters have to be set. In a supervised setting, model search is guided by performance measures that rely on known class labels, such as accuracy. However, these measures are not applicable to clustering as labels are usually not available. Instead, one might use internal validity measures that only rely on properties intrinsic to the data set. Several such measures are defined, and in this paper we study the usefulness of four of them for model selection. We perform experiments with these measures in combination with six clustering algorithms. While some measures are suited to use in hyperparameter optimization for some specific algorithms, we conclude that none of them is suited to compare across very different clustering algorithms.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\3DJL2JPQ\\Craenendonck en Blockeel - Using Internal Validity Measures to Compare Cluste.pdf}
}

@article{davies_cluster_1979,
  title = {A Cluster Separation Measure},
  author = {Davies, David L and Bouldin, Donald W},
  year = {1979},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  number = {2},
  pages = {224--227},
  publisher = {{IEEE}},
  issn = {0162-8828}
}

@article{DBLP:journals/corr/abs-1212-1984,
  title = {Geo-Indistinguishability: {{Differential}} Privacy for Location-Based Systems},
  author = {Andr{\'e}s, Miguel E. and Bordenabe, Nicol{\'a}s Emilio and Chatzikokolakis, Konstantinos and Palamidessi, Catuscia},
  year = {2012},
  journal = {CoRR},
  volume = {abs/1212.1984},
  eprint = {1212.1984},
  archiveprefix = {arxiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-1212-1984.bib},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HVVT4GUR\\Andrés e.a. - 2012 - Geo-indistinguishability Differential privacy for.pdf}
}

@article{del_rey_comprehensive_2020-1,
  title = {A {{Comprehensive Survey}} on {{Local Differential Privacy}}},
  author = {Xiong, Xingxing and Liu, Shubo and Li, Dan and Cai, Zhaohui and Niu, Xiaoguang},
  editor = {Del Rey, Angel M.},
  year = {2020},
  month = oct,
  journal = {Security and Communication Networks},
  volume = {2020},
  pages = {8829523},
  publisher = {{Hindawi}},
  issn = {1939-0114},
  doi = {10.1155/2020/8829523},
  abstract = {With the advent of the era of big data, privacy issues have been becoming a hot topic in public. Local differential privacy (LDP) is a state-of-the-art privacy preservation technique that allows to perform big data analysis (e.g., statistical estimation, statistical learning, and data mining) while guaranteeing each individual participant\&\#x2019;s privacy. In this paper, we present a comprehensive survey of LDP. We first give an overview on the fundamental knowledge of LDP and its frameworks. We then introduce the mainstream privatization mechanisms and methods in detail from the perspective of frequency oracle and give insights into recent studied on private basic statistical estimation (e.g., frequency estimation and mean estimation) and complex statistical estimation (e.g., multivariate distribution estimation and private estimation over complex data) under LDP. Furthermore, we present current research circumstances on LDP including the private statistical learning/inferencing, private statistical data analysis, privacy amplification techniques for LDP, and some application fields under LDP. Finally, we identify future research directions and open challenges for LDP. This survey can serve as a good reference source for the research of LDP to deal with various privacy-related scenarios to be encountered in practice.}
}

@article{demsar_hands-training_2021,
  title = {Hands-on Training about Overfitting},
  author = {Dem{\v s}ar, Janez and Zupan, Bla{\v z}},
  year = {2021},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {17},
  pages = {e1008671},
  doi = {10.1371/journal.pcbi.1008671},
  abstract = {Overfitting is one of the critical problems in developing models by machine learning. With machine learning becoming an essential technology in computational biology, we must include training about overfitting in all courses that introduce this technology to students and practitioners. We here propose a hands-on training for overfitting that is suitable for introductory level courses and can be carried out on its own or embedded within any data science course. We use workflow-based design of machine learning pipelines, experimentation-based teaching, and hands-on approach that focuses on concepts rather than underlying mathematics. We here detail the data analysis workflows we use in training and motivate them from the viewpoint of teaching goals. Our proposed approach relies on Orange, an open-source data science toolbox that combines data visualization and machine learning, and that is tailored for education in machine learning and explorative data analysis.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\W2CVK27X\\Demšar en Zupan - 2021 - Hands-on training about overfitting.pdf}
}

@article{dong_limitations_2018,
  title = {On the Limitations of Existing Notions of Location Privacy},
  author = {Dong, Kai and Guo, Taolin and Ye, Haibo and Li, Xuansong and Ling, Zhen},
  year = {2018},
  month = sep,
  journal = {Future Generation Computer Systems},
  volume = {86},
  pages = {1513--1522},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.05.045},
  urldate = {2023-03-16},
  abstract = {In the context of a single report of location information, existing researches define location privacy by adversary's uncertainty, inaccuracy, or incorrectness of the estimation, or by geo-indistinguishability which is a generalization of differential privacy. Each of these existing notions has problems in some specific scenarios. In this paper we illustrate the limitations of existing notions by constructing such scenarios, and introduce a formal definition on location privacy by quantifying the distance between the prior and posterior distribution over the possible locations. Further more, we show how to construct a near-optimal obfuscation mechanism by solving an optimization problem. We compare our proposed mechanism with the Laplace noise based geo-indistinguishable mechanism, and Shokri's optimal obfuscation mechanism, using both our proposed privacy metric and the traditional metric based on the estimated distance errors. The results show that our proposed metric better describes location privacy and our proposed mechanism makes a better tradeoff between privacy and utility.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C45AELKE\\Dong et al. - 2018 - On the limitations of existing notions of location.pdf}
}

@misc{duchi_minimax_2017,
  title = {Minimax {{Optimal Procedures}} for {{Locally Private Estimation}}},
  author = {Duchi, John and Wainwright, Martin and Jordan, Michael},
  year = {2017},
  month = nov,
  number = {arXiv:1604.02390},
  eprint = {1604.02390},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.02390},
  urldate = {2023-05-13},
  abstract = {Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the risk of the resulting statistical estimators. We develop private versions of classical information-theoretic bounds, in particular those due to Le Cam, Fano, and Assouad. These inequalities allow for a precise characterization of statistical rates under local privacy constraints and the development of provably (minimax) optimal estimation procedures. We provide a treatment of several canonical families of problems: mean estimation and median estimation, generalized linear models, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds. Additionally, we present a variety of experimental results for estimation problems involving sensitive data, including salaries, censored blog posts and articles, and drug abuse; these experiments demonstrate the importance of deriving optimal procedures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\E2YPKIEK\\Duchi et al. - 2017 - Minimax Optimal Procedures for Locally Private Est.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\P6YN4GLQ\\1604.html}
}

@misc{duchi_privacy_2013,
  title = {Privacy {{Aware Learning}}},
  author = {Duchi, John C. and Jordan, Michael I. and Wainwright, Martin J.},
  year = {2013},
  month = oct,
  number = {arXiv:1210.2085},
  eprint = {1210.2085},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1210.2085},
  urldate = {2023-05-25},
  abstract = {We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\GPEEJSEY\\Duchi e.a. - 2013 - Privacy Aware Learning.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\Y52F8BR3\\1210.html}
}

@article{dwork_calibrating_nodate,
  title = {Calibrating {{Noise}} to {{Sensitivity}} in {{Private Data Analysis}}},
  author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  abstract = {We continue a line of research initiated in [10, 11] on privacypreserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\C63GKYIP\\Dwork e.a. - Calibrating Noise to Sensitivity in Private Data A.pdf}
}

@inproceedings{dwork_differential_2006,
  title = {Differential Privacy},
  booktitle = {Automata, {{Languages}} and {{Programming}}: 33rd {{International Colloquium}}, {{ICALP}} 2006, {{Venice}}, {{Italy}}, {{July}} 10-14, 2006, {{Proceedings}}, {{Part II}} 33},
  author = {Dwork, Cynthia},
  year = {2006},
  pages = {1--12},
  publisher = {{Springer}},
  isbn = {3-540-35907-9}
}

@article{elbatta_dynamic_2013,
  title = {A Dynamic {{Method}} for {{Discovering Density Varied Clusters}}},
  author = {Elbatta, Mohammed and Ashour, Wesam},
  year = {2013},
  month = feb,
  journal = {International Journal of Signal Processing, Image Processing and Pattern Recognition},
  volume = {6},
  pages = {123--134},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZXC4JZX8\\Elbatta en Ashour - 2013 - A dynamic Method for Discovering Density Varied Cl.pdf}
}

@inproceedings{erlingsson_rappor_2014-2,
  title = {{{RAPPOR}}: {{Randomized Aggregatable Privacy-Preserving Ordinal Response}}},
  shorttitle = {{{RAPPOR}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Erlingsson, {\'U}lfar and Pihur, Vasyl and Korolova, Aleksandra},
  year = {2014},
  month = nov,
  pages = {1054--1067},
  publisher = {{ACM}},
  address = {{Scottsdale Arizona USA}},
  doi = {10.1145/2660267.2660348},
  urldate = {2023-05-04},
  abstract = {Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports.},
  isbn = {978-1-4503-2957-6},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6LLA3T9X\\Erlingsson e.a. - 2014 - RAPPOR Randomized Aggregatable Privacy-Preserving.pdf}
}

@article{ester_density-based_nodate,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\D68L5F4R\\Ester e.a. - A Density-Based Algorithm for Discovering Clusters.pdf}
}

@article{fahad_survey_2014,
  title = {A {{Survey}} of {{Clustering Algorithms}} for {{Big Data}}: {{Taxonomy}} and {{Empirical Analysis}}},
  shorttitle = {A {{Survey}} of {{Clustering Algorithms}} for {{Big Data}}},
  author = {Fahad, Adil and Alshatri, Najlaa and Tari, Zahir and Alamri, Abdullah and Khalil, Ibrahim and Zomaya, Albert Y. and Foufou, Sebti and Bouras, Abdelaziz},
  year = {2014},
  month = sep,
  journal = {IEEE Transactions on Emerging Topics in Computing},
  volume = {2},
  number = {3},
  pages = {267--279},
  issn = {2168-6750},
  doi = {10.1109/TETC.2014.2330519},
  abstract = {Clustering algorithms have emerged as an alternative powerful meta-learning tool to accurately analyze the massive volume of data generated by modern applications. In particular, their main goal is to categorize data into clusters such that objects are grouped in the same cluster when they are similar according to specific metrics. There is a vast body of knowledge in the area of clustering and there has been attempts to analyze and categorize them for a larger number of applications. However, one of the major issues in using clustering algorithms for big data that causes confusion amongst practitioners is the lack of consensus in the definition of their properties as well as a lack of formal categorization. With the intention of alleviating these problems, this paper introduces concepts and algorithms related to clustering, a concise survey of existing (clustering) algorithms as well as providing a comparison, both from a theoretical and an empirical perspective. From a theoretical perspective, we developed a categorizing framework based on the main properties pointed out in previous studies. Empirically, we conducted extensive experiments where we compared the most representative algorithm from each of the categories using a large number of real (big) data sets. The effectiveness of the candidate clustering algorithms is measured through a number of internal and external validity metrics, stability, runtime, and scalability tests. In addition, we highlighted the set of clustering algorithms that are the best performing for big data.},
  keywords = {Algorithm design and analysis,big data,Big data,Clustering algorithms,Clustering methods,Neural networks,Partitioning algorithms,Taxonomies,unsupervised learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BJSQU8PU\\Fahad e.a. - 2014 - A Survey of Clustering Algorithms for Big Data Ta.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\8AUACR48\\stamp.html}
}

@misc{fernandes_generalised_2019,
  title = {Generalised {{Differential Privacy}} for {{Text Document Processing}}},
  author = {Fernandes, Natasha and Dras, Mark and McIver, Annabelle},
  year = {2019},
  month = feb,
  number = {arXiv:1811.10256},
  eprint = {1811.10256},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-11},
  abstract = {We address the problem of how to "obfuscate" texts by removing stylistic clues which can identify authorship, whilst preserving (as much as possible) the content of the text. In this paper we combine ideas from "generalised differential privacy" and machine learning techniques for text processing to model privacy for text documents. We define a privacy mechanism that operates at the level of text documents represented as "bags-of-words" - these representations are typical in machine learning and contain sufficient information to carry out many kinds of classification tasks including topic identification and authorship attribution (of the original documents). We show that our mechanism satisfies privacy with respect to a metric for semantic similarity, thereby providing a balance between utility, defined by the semantic content of texts, with the obfuscation of stylistic clues. We demonstrate our implementation on a "fan fiction" dataset, confirming that it is indeed possible to disguise writing style effectively whilst preserving enough information and variation for accurate content classification tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XQYR7E5I\\Fernandes et al. - 2019 - Generalised Differential Privacy for Text Document.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\ZLA8MZL4\\1811.html}
}

@article{franti_centroid_2014,
  title = {Centroid Index: {{Cluster}} Level Similarity Measure},
  shorttitle = {Centroid Index},
  author = {Fr{\"a}nti, Pasi and Rezaei, Mohammad and Zhao, Qinpei},
  year = {2014},
  month = sep,
  journal = {Pattern Recognition},
  volume = {47},
  number = {9},
  pages = {3034--3045},
  issn = {00313203},
  doi = {10.1016/j.patcog.2014.03.017},
  urldate = {2023-03-22},
  abstract = {In clustering algorithm, one of the main challenges is to solve the global allocation of the clusters instead of just local tuning of the partition borders. Despite this, all external cluster validity indexes calculate only point-level differences of two partitions without any direct information about how similar their cluster-level structures are. In this paper, we introduce a cluster level index called centroid index. The measure is intuitive, simple to implement, fast to compute and applicable in case of model mismatch as well. To a certain extent, we expect it to generalize other clustering models beyond the centroid-based k-means as well.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\4IJJ6ZVN\\Fränti e.a. - 2014 - Centroid index Cluster level similarity measure.pdf}
}

@article{frey_clustering_2007,
  title = {Clustering by {{Passing Messages Between Data Points}}},
  author = {Frey, Brendan J. and Dueck, Delbert},
  year = {2007},
  month = feb,
  journal = {Science},
  volume = {315},
  number = {5814},
  pages = {972--976},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1136800},
  urldate = {2023-04-12},
  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such ``exemplars'' can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called ``affinity propagation,'' which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5QGKVZLN\\Frey en Dueck - 2007 - Clustering by Passing Messages Between Data Points.pdf}
}

@inproceedings{friedman_data_2010,
  title = {Data Mining with Differential Privacy},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Friedman, Arik and Schuster, Assaf},
  year = {2010},
  month = jul,
  pages = {493--502},
  publisher = {{ACM}},
  address = {{Washington DC USA}},
  doi = {10.1145/1835804.1835868},
  urldate = {2023-03-30},
  abstract = {We consider the problem of data mining with formal privacy guarantees, given a data access interface based on the differential privacy framework. Differential privacy requires that computations be insensitive to changes in any particular individual's record, thereby restricting data leaks through the results. The privacy preserving interface ensures unconditionally safe access to the data and does not require from the data miner any expertise in privacy. However, as we show in the paper, a naive utilization of the interface to construct privacy preserving data mining algorithms could lead to inferior data mining results. We address this problem by considering the privacy and the algorithmic requirements simultaneously, focusing on decision tree induction as a sample application. The privacy mechanism has a profound effect on the performance of the methods chosen by the data miner. We demonstrate that this choice could make the difference between an accurate classifier and a completely useless one. Moreover, an improved algorithm can achieve the same level of accuracy and privacy as the naive implementation but with an order of magnitude fewer learning samples.},
  isbn = {978-1-4503-0055-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\F635ZNB5\\Friedman en Schuster - 2010 - Data mining with differential privacy.pdf}
}

@misc{geng_optimal_2013,
  title = {The {{Optimal Mechanism}} in {{Differential Privacy}}},
  author = {Geng, Quan and Viswanath, Pramod},
  year = {2013},
  month = oct,
  number = {arXiv:1212.1186},
  eprint = {1212.1186},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-13},
  abstract = {We derive the optimal \$\textbackslash epsilon\$-differentially private mechanism for single real-valued query function under a very general utility-maximization (or cost-minimization) framework. The class of noise probability distributions in the optimal mechanism has \{\textbackslash em staircase-shaped\} probability density functions which are symmetric (around the origin), monotonically decreasing and geometrically decaying. The staircase mechanism can be viewed as a \{\textbackslash em geometric mixture of uniform probability distributions\}, providing a simple algorithmic description for the mechanism. Furthermore, the staircase mechanism naturally generalizes to discrete query output settings as well as more abstract settings. We explicitly derive the optimal noise probability distributions with minimum expectation of noise amplitude and power. Comparing the optimal performances with those of the Laplacian mechanism, we show that in the high privacy regime (\$\textbackslash epsilon\$ is small), Laplacian mechanism is asymptotically optimal as \$\textbackslash epsilon \textbackslash to 0\$; in the low privacy regime (\$\textbackslash epsilon\$ is large), the minimum expectation of noise amplitude and minimum noise power are \$\textbackslash Theta(\textbackslash Delta e\^\{-\textbackslash frac\{\textbackslash epsilon\}\{2\}\})\$ and \$\textbackslash Theta(\textbackslash Delta\^2 e\^\{-\textbackslash frac\{2\textbackslash epsilon\}\{3\}\})\$ as \$\textbackslash epsilon \textbackslash to +\textbackslash infty\$, while the expectation of noise amplitude and power using the Laplacian mechanism are \$\textbackslash frac\{\textbackslash Delta\}\{\textbackslash epsilon\}\$ and \$\textbackslash frac\{2\textbackslash Delta\^2\}\{\textbackslash epsilon\^2\}\$, where \$\textbackslash Delta\$ is the sensitivity of the query function. We conclude that the gains are more pronounced in the low privacy regime.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Data Structures and Algorithms},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\U7D8VWLC\\Geng and Viswanath - 2013 - The Optimal Mechanism in Differential Privacy.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\FWAXWWV8\\1212.html}
}

@article{geng_staircase_2015,
  title = {The Staircase Mechanism in Differential Privacy},
  author = {Geng, Quan and Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
  year = {2015},
  month = oct,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {9},
  number = {7},
  pages = {1176--1184},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2015.2425831},
  urldate = {2023-05-13},
  abstract = {Adding Laplacian noise is a standard approach in differential privacy to sanitize numerical data before releasing it. In this paper, we propose an alternative noise adding mechanism: the staircase mechanism, which is a geometric mixture of uniform random variables. The staircase mechanism can replace the Laplace mechanism in each instance in the literature and for the same level of differential privacy, the performance in each instance improves; the improvement is particularly stark in medium-low privacy regimes. We show that the staircase mechanism is the optimal noise adding mechanism in a universal context, subject to a conjectured technical lemma (which we also prove to be true for one and two dimensional data).},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\D345PLSC\\Geng et al. - 2015 - The Staircase Mechanism in Differential Privacy.pdf}
}

@article{george_marsaglia_choosing_1972,
  title = {Choosing a {{Point}} from the {{Surface}} of a {{Sphere}}},
  author = {{George Marsaglia}},
  year = {1972},
  month = apr,
  journal = {The Annals of Mathematical Statistics},
  volume = {43},
  number = {2},
  pages = {645--646},
  doi = {10.1214/aoms/1177692644}
}

@article{gorban_high-dimensional_2020,
  title = {High-{{Dimensional Brain}} in a {{High-Dimensional World}}: {{Blessing}} of {{Dimensionality}}},
  shorttitle = {High-{{Dimensional Brain}} in a {{High-Dimensional World}}},
  author = {Gorban, Alexander N. and Makarov, Valery A. and Tyukin, Ivan Y.},
  year = {2020},
  month = jan,
  journal = {Entropy},
  volume = {22},
  number = {1},
  pages = {82},
  issn = {1099-4300},
  doi = {10.3390/e22010082},
  urldate = {2023-05-24},
  abstract = {High-dimensional data and high-dimensional representations of reality are inherent features of modern Artificial Intelligence systems and applications of machine learning. The well-known phenomenon of the ``curse of dimensionality'' states: many problems become exponentially difficult in high dimensions. Recently, the other side of the coin, the ``blessing of dimensionality'', has attracted much attention. It turns out that generic high-dimensional datasets exhibit fairly simple geometric properties. Thus, there is a fundamental tradeoff between complexity and simplicity in high dimensional spaces. Here we present a brief explanatory review of recent ideas, results and hypotheses about the blessing of dimensionality and related simplifying effects relevant to machine learning and neuroscience.},
  pmcid = {PMC7516518},
  pmid = {33285855},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\P2A7W8LD\\Gorban et al. - 2020 - High-Dimensional Brain in a High-Dimensional World.pdf}
}

@misc{hassan_differential_2019,
  title = {Differential {{Privacy Techniques}} for {{Cyber Physical Systems}}: {{A Survey}}},
  shorttitle = {Differential {{Privacy Techniques}} for {{Cyber Physical Systems}}},
  author = {Hassan, Muneeb Ul and Rehmani, Mubashir Husain and Chen, Jinjun},
  year = {2019},
  month = sep,
  number = {arXiv:1812.02282},
  eprint = {1812.02282},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-04},
  abstract = {Modern cyber physical systems (CPSs) has widely being used in our daily lives because of development of information and communication technologies (ICT).With the provision of CPSs, the security and privacy threats associated to these systems are also increasing. Passive attacks are being used by intruders to get access to private information of CPSs. In order to make CPSs data more secure, certain privacy preservation strategies such as encryption, and k-anonymity have been presented in the past. However, with the advances in CPSs architecture, these techniques also needs certain modifications. Meanwhile, differential privacy emerged as an efficient technique to protect CPSs data privacy. In this paper, we present a comprehensive survey of differential privacy techniques for CPSs. In particular, we survey the application and implementation of differential privacy in four major applications of CPSs named as energy systems, transportation systems, healthcare and medical systems, and industrial Internet of things (IIoT). Furthermore, we present open issues, challenges, and future research direction for differential privacy techniques for CPSs. This survey can serve as basis for the development of modern differential privacy techniques to address various problems and data privacy scenarios of CPSs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\VNKHYWPJ\\Hassan et al. - 2019 - Differential Privacy Techniques for Cyber Physical.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\N6PMZLCU\\1812.html}
}

@article{hassani_using_2017,
  title = {Using Internal Evaluation Measures to Validate the Quality of Diverse Stream Clustering Algorithms},
  author = {Hassani, Marwan and Seidl, Thomas},
  year = {2017},
  month = aug,
  journal = {Vietnam Journal of Computer Science},
  volume = {4},
  number = {3},
  pages = {171--183},
  issn = {2196-8896},
  doi = {10.1007/s40595-016-0086-9},
  abstract = {Measuring the quality of a clustering algorithm has shown to be as important as the algorithm itself. It is a crucial part of choosing the clustering algorithm that performs best for an input data. Streaming input data have many features that make them much more challenging than static ones. They are endless, varying and emerging with high speeds. This raised new challenges for the clustering algorithms as well as for their evaluation measures. Up till now, external evaluation measures were exclusively used for validating stream clustering algorithms. While external validation requires a ground truth which is not provided in most applications, particularly in the streaming case, internal clustering validation is efficient and realistic. In this article, we analyze the properties and performances of eleven internal clustering measures. In particular, we apply these measures to carefully synthesized stream scenarios to reveal how they react to clusterings on evolving data streams using both k-means-based and density-based clustering algorithms. A series of experimental results show that different from the case with static data, the Calinski-Harabasz index performs the best in coping with common aspects and errors of stream clustering for k-means-based algorithms, while the revised validity index performs the best for density-based ones.}
}

@inproceedings{he_laplacian_2005,
  title = {Laplacian {{Score}} for {{Feature Selection}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
  year = {2005},
  volume = {18},
  publisher = {{MIT Press}},
  urldate = {2023-02-15},
  abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are "wrapper" techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a "filter" method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classification problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efficiency of our algorithm.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MEYJS6AX\\He e.a. - 2005 - Laplacian Score for Feature Selection.pdf}
}

@inproceedings{ho_differential_2011,
  title = {Differential Privacy for Location Pattern Mining},
  booktitle = {Proceedings of the 4th {{ACM SIGSPATIAL International Workshop}} on {{Security}} and {{Privacy}} in {{GIS}} and {{LBS}} - {{SPRINGL}} '11},
  author = {Ho, Shen-Shyang and Ruan, Shuhua},
  year = {2011},
  pages = {17},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois}},
  doi = {10.1145/2071880.2071884},
  urldate = {2022-12-28},
  abstract = {One main concern for individuals to participate in the data collection of personal location history records is the disclosure of their location and related information when a user queries for statistical or pattern mining results derived from these records. In this paper, we investigate how the privacy goal that the inclusion of one's location history in a statistical database with location pattern mining capabilities does not substantially increase one's privacy risk. In particular, we propose a differentially private pattern mining algorithm for interesting geographic location discovery using a region quadtree spatial decomposition to preprocess the location points followed by applying a density-based clustering algorithm. A differentially private region quadtree is used for both de-noising the spatial domain and identifying the likely geographic regions containing the interesting locations. Then, a differential privacy mechanism is applied to the algorithm outputs, namely: the interesting regions and their corresponding stay point counts. The quadtree spatial decomposition enables one to obtain a localized reduced sensitivity to achieve the differential privacy goal and accurate outputs. Experimental results on synthetic datasets are used to show the feasibility of the proposed privacy preserving location pattern mining algorithm.},
  isbn = {978-1-4503-1032-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TF8DSXWE\\Ho en Ruan - 2011 - Differential privacy for location pattern mining.pdf}
}

@misc{hu_membership_2022,
  title = {Membership {{Inference Attacks}} on {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Membership {{Inference Attacks}} on {{Machine Learning}}},
  author = {Hu, Hongsheng and Salcic, Zoran and Sun, Lichao and Dobbie, Gillian and Yu, Philip S. and Zhang, Xuyun},
  year = {2022},
  month = feb,
  number = {arXiv:2103.07853},
  eprint = {2103.07853},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-05},
  abstract = {Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this paper, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7FV4U9VE\\Hu e.a. - 2022 - Membership Inference Attacks on Machine Learning .pdf;C\:\\Users\\tjvan\\Zotero\\storage\\57DEQY5Z\\2103.html}
}

@article{hubert_comparing_1985-1,
  title = {Comparing Partitions},
  author = {Hubert, Lawrence and Arabie, Phipps},
  year = {1985},
  journal = {Journal of classification},
  volume = {2},
  pages = {193--218},
  publisher = {{Springer}},
  issn = {0176-4268}
}

@article{jayaraman_evaluating_nodate,
  title = {Evaluating {{Differentially Private Machine Learning}} in {{Practice}}},
  author = {Jayaraman, Bargav and Evans, David},
  abstract = {Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, , about how much information is leaked by a mechanism. When used in privacy-preserving machine learning, the goal is typically to limit what can be inferred from the model about individual training records. However, the calibration of the privacy budget is not well understood. Implementations of privacy-preserving machine learning often select large values of in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, relaxed definitions of differential privacy are often used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is no way to obtain privacy for free\textemdash relaxed definitions of differential privacy that reduce the amount of noise needed to improve utility also increase the measured privacy leakage. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs for complex learning tasks: settings that provide limited accuracy loss provide little effective privacy, and settings that provide strong privacy result in useless models.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\SL2RLCCC\\Jayaraman en Evans - Evaluating Diﬀerentially Private Machine Learning .pdf}
}

@misc{ji_differential_2014,
  title = {Differential {{Privacy}} and {{Machine Learning}}: A {{Survey}} and {{Review}}},
  shorttitle = {Differential {{Privacy}} and {{Machine Learning}}},
  author = {Ji, Zhanglong and Lipton, Zachary C. and Elkan, Charles},
  year = {2014},
  month = dec,
  number = {arXiv:1412.7584},
  eprint = {1412.7584},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-04},
  abstract = {The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Databases,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UZ23LGFZ\\Ji et al. - 2014 - Differential Privacy and Machine Learning a Surve.pdf}
}

@misc{kaplan_differentially_2018,
  title = {Differentially {{Private}} K-{{Means}} with {{Constant Multiplicative Error}}},
  author = {Kaplan, Haim and Stemmer, Uri},
  year = {2018},
  month = jul,
  number = {arXiv:1804.08001},
  eprint = {1804.08001},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-14},
  abstract = {We design new differentially private algorithms for the Euclidean k-means problem, both in the centralized model and in the local model of differential privacy. In both models, our algorithms achieve significantly improved error guarantees than the previous state-of-the-art. In addition, in the local model, our algorithm significantly reduces the number of interaction rounds. Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present, for the first time, efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private corsets for k-means clustering in both models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ULNKA2BZ\\Kaplan and Stemmer - 2018 - Differentially Private k-Means with Constant Multi.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\E93GRCN6\\1804.html}
}

@inproceedings{keller_balancing_2021,
  title = {Balancing {{Quality}} and {{Efficiency}} in {{Private Clustering}} with {{Affinity Propagation}}:},
  shorttitle = {Balancing {{Quality}} and {{Efficiency}} in {{Private Clustering}} with {{Affinity Propagation}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Security}} and {{Cryptography}}},
  author = {Keller, Hannah and M{\"o}llering, Helen and Schneider, Thomas and Yalame, Hossein},
  year = {2021},
  pages = {173--184},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Online Streaming, --- Select a Country ---}},
  doi = {10.5220/0010547801730184},
  urldate = {2022-12-22},
  abstract = {In many machine learning applications, training data consists of sensitive information from multiple sources.},
  isbn = {978-989-758-524-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ITIR5ZT3\\Keller e.a. - 2021 - Balancing Quality and Efficiency in Private Cluste.pdf}
}

@article{kodinariya_review_2013,
  title = {Review on Determining Number of {{Cluster}} in {{K-Means Clustering}}},
  author = {Kodinariya, Trupti M and Makwana, Prashant R},
  year = {2013},
  journal = {International Journal},
  volume = {1},
  number = {6},
  pages = {90--95}
}

@article{kohavi_wrappers_1997,
  title = {Wrappers for Feature Subset Selection},
  author = {Kohavi, Ron and John, George H.},
  year = {1997},
  month = dec,
  journal = {Artificial Intelligence},
  series = {Relevance},
  volume = {97},
  number = {1},
  pages = {273--324},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(97)00043-X},
  urldate = {2023-02-15},
  abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.},
  langid = {english},
  keywords = {Classification,Feature selection,Filter,Wrapper},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\5J9L28B4\\Kohavi en John - 1997 - Wrappers for feature subset selection.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\K2MHWNY7\\S000437029700043X.html}
}

@article{lehtonen_lambert_2016,
  title = {The {{Lambert W}} Function in Ecological and Evolutionary Models},
  author = {Lehtonen, Jussi},
  year = {2016},
  journal = {Methods in Ecology and Evolution},
  volume = {7},
  number = {9},
  pages = {1110--1118},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12568},
  urldate = {2023-03-09},
  abstract = {The Lambert W function is a mathematical function with a long history, but which was named and rigorously defined relatively recently. It is closely related to the logarithmic function and arises from many models in the natural sciences, including a surprising number of problems in ecology and evolution. I describe the basic properties of the function and present examples of its application to models of ecological and evolutionary processes. The Lambert W function makes it possible to solve explicitly several models where this is not possible with elementary functions. I present examples of such models from existing literature, as well as novel models. Solving models explicitly with the Lambert W function can provide deeper insight and a new point of view on a biological problem. Explicit solutions with the Lambert W function are easily amenable to further mathematical operations, such as differentiation and integration. These advantages apply to a wide range of models, from the marginal value theorem to population growth rates and disease epidemics.},
  langid = {english},
  keywords = {calculus,explicit solution,fertilization kinetics,Lambert W function,Lotka\textendash Volterra model,marginal value theorem,mate search,modelling,population growth rate,SIR model},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\S77QF2U8\\Lehtonen - 2016 - The Lambert W function in ecological and evolution.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\QYQTRQAH\\2041-210X.html}
}

@misc{li_membership_2021,
  title = {Membership {{Leakage}} in {{Label-Only Exposures}}},
  author = {Li, Zheng and Zhang, Yang},
  year = {2021},
  month = sep,
  number = {arXiv:2007.15528},
  eprint = {2007.15528},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack, and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\6KD2H93N\\Li en Zhang - 2021 - Membership Leakage in Label-Only Exposures.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\DU9RRUMC\\2007.html}
}

@article{liu_privacy_2012,
  title = {Privacy Preserving Distributed {{DBSCAN}} Clustering},
  author = {Liu, Jinfei and Huang, Joshua and Luo, Jun and Xiong, Li},
  year = {2012},
  month = mar,
  journal = {Transactions on Data Privacy},
  volume = {6},
  doi = {10.1145/2320765.2320819},
  abstract = {DBSCAN is a well-known density-based clustering algorithm which offers advantages for finding clusters of arbitrary shapes compared to partitioning and hierarchical clustering methods. However, there are few papers studying the DBSCAN algorithm under the privacy preserving distributed data mining model, in which the data is distributed between two or more parties, and the parties cooperate to obtain the clustering results without revealing the data at the individual parties. In this paper, we address the problem of two-party privacy preserving DBSCAN clustering. We first propose two protocols for privacy preserving DBSCAN clustering over horizontally and vertically partitioned data respectively and then extend them to arbitrarily partitioned data. We also provide analysis of the performance and proof of privacy of our solution.},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\2AH2MBGH\\Liu e.a. - 2012 - Privacy preserving distributed DBSCAN clustering.pdf}
}

@inproceedings{m_alvim_invited_2018,
  title = {Invited {{Paper}}: {{Local Differential Privacy}} on {{Metric Spaces}}: {{Optimizing}} the {{Trade-Off}} with {{Utility}}},
  booktitle = {2018 {{IEEE}} 31st {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {{M. Alvim} and {K. Chatzikokolakis} and {C. Palamidessi} and {A. Pazii}},
  year = {2018},
  month = jul,
  pages = {262--267},
  doi = {10.1109/CSF.2018.00026},
  isbn = {2374-8303}
}

@inproceedings{m_alvim_invited_2018-2,
  title = {Invited {{Paper}}: {{Local Differential Privacy}} on {{Metric Spaces}}: {{Optimizing}} the {{Trade-Off}} with {{Utility}}},
  booktitle = {2018 {{IEEE}} 31st {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {{M. Alvim} and {K. Chatzikokolakis} and {C. Palamidessi} and {A. Pazii}},
  year = {2018},
  month = jul,
  pages = {262--267},
  doi = {10.1109/CSF.2018.00026},
  isbn = {2374-8303}
}

@article{marsaglia_choosing_1972,
  title = {Choosing a Point from the Surface of a Sphere},
  author = {Marsaglia, George},
  year = {1972},
  journal = {The Annals of Mathematical Statistics},
  volume = {43},
  number = {2},
  pages = {645--646},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851}
}

@article{mitra_unsupervised_2002,
  title = {Unsupervised Feature Selection Using Feature Similarity},
  author = {Mitra, P. and Murthy, C.A. and Pal, S.K.},
  year = {2002},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {3},
  pages = {301--312},
  issn = {01628828},
  doi = {10.1109/34.990133},
  urldate = {2023-02-15},
  abstract = {\DH In this article, we describe an unsupervised feature selection algorithm suitable for data sets, large in both dimension and size. The method is based on measuring similarity between features whereby redundancy therein is removed. This does not need any search and, therefore, is fast. A new feature similarity measure, called maximum information compression index, is introduced. The algorithm is generic in nature and has the capability of multiscale representation of data sets. The superiority of the algorithm, in terms of speed and performance, is established extensively over various real-life data sets of different sizes and dimensions. It is also demonstrated how redundancy and information loss in feature selection can be quantified with an entropy measure.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\E9SIXSRR\\Mitra e.a. - 2002 - Unsupervised feature selection using feature simil.pdf}
}

@article{moiane_evaluation_2018,
  title = {{{EVALUATION OF THE CLUSTERING PERFORMANCE OF AFFINITY PROPAGATION ALGORITHM CONSIDERING THE INFLUENCE OF PREFERENCE PARAMETER AND DAMPING FACTOR}}},
  author = {Moiane, Andr{\'e} Fenias and Machado, {\'A}lvaro Muriel Lima},
  year = {2018},
  month = dec,
  journal = {Boletim de Ci\^encias Geod\'esicas},
  volume = {24},
  number = {4},
  pages = {426--441},
  issn = {1982-2170, 1413-4853},
  doi = {10.1590/s1982-21702018000400027},
  urldate = {2023-04-12},
  abstract = {The identification of significant underlying data patterns such as image composition and spatial arrangements is fundamental in remote sensing tasks. Therefore, the development of an effective approach for information extraction is crucial to achieve this goal. Affinity propagation (AP) algorithm is a novel powerful technique with the ability of handling with unusual data, containing both categorical and numerical attributes. However, AP has some limitations related to the choice of initial preference parameter, occurrence of oscillations and processing of large data sets. This paper evaluates the clustering performance of AP algorithm taking into account the influence of preference parameter and damping factor. The study was conducted considering the AP algorithm, the adaptive AP and partition AP. According to the experiments, the choice of preference and damping greatly influences on the quality and the final number of clusters.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HQPEFR7T\\Moiane en Machado - 2018 - EVALUATION OF THE CLUSTERING PERFORMANCE OF AFFINI.pdf}
}

@misc{nguyen_collecting_2016,
  title = {Collecting and {{Analyzing Data}} from {{Smart Device Users}} with {{Local Differential Privacy}}},
  author = {Nguy{\^e}n, Th{\^o}ng T. and Xiao, Xiaokui and Yang, Yin and Hui, Siu Cheung and Shin, Hyejin and Shin, Junbum},
  year = {2016},
  month = jun,
  number = {arXiv:1606.05053},
  eprint = {1606.05053},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-24},
  abstract = {Organizations with a large user base, such as Samsung and Google, can potentially benefit from collecting and mining users' data. However, doing so raises privacy concerns, and risks accidental privacy breaches with serious consequences. Local differential privacy (LDP) techniques address this problem by only collecting randomized answers from each user, with guarantees of plausible deniability; meanwhile, the aggregator can still build accurate models and predictors by analyzing large amounts of such randomized data. So far, existing LDP solutions either have severely restricted functionality, or focus mainly on theoretical aspects such as asymptotical bounds rather than practical usability and performance. Motivated by this, we propose Harmony, a practical, accurate and efficient system for collecting and analyzing data from smart device users, while satisfying LDP. Harmony applies to multi-dimensional data containing both numerical and categorical attributes, and supports both basic statistics (e.g., mean and frequency estimates), and complex machine learning tasks (e.g., linear regression, logistic regression and SVM classification). Experiments using real data confirm Harmony's effectiveness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7Y4VRWXH\\Nguyên et al. - 2016 - Collecting and Analyzing Data from Smart Device Us.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\36II3QW2\\1606.html}
}

@inproceedings{nissim_clustering_2018,
  title = {Clustering {{Algorithms}} for the {{Centralized}} and {{Local Models}}},
  booktitle = {Proceedings of {{Algorithmic Learning Theory}}},
  author = {Nissim, Kobbi and Stemmer, Uri},
  year = {2018},
  month = apr,
  pages = {619--653},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-04-25},
  abstract = {We revisit the problem of finding a minimum enclosing ball with differential privacy: Given a set of \$n\$ points in the \$d\$-dimensional Euclidean space and an integer \$t{$\leq$}n\$,  the goal is to find a ball of the smallest radius \$r\_\{opt\}\$ enclosing at least \$t\$ input points. The problem is motivated by its various applications to differential privacy, including the sample and aggregate technique, private data exploration, and clustering.   Without privacy concerns, minimum enclosing ball has a polynomial time approximation scheme (PTAS), which computes a ball of radius almost \$r\_\{opt\}\$ (the problem is NP-hard to solve exactly). In contrast, under differential privacy, until this work, only a \$O(\textbackslash sqrt\{\textbackslash log n\})\$-approximation algorithm was known.   We provide new constructions of differentially private algorithms for minimum enclosing ball achieving constant factor approximation to \$r\_\{opt\}\$ both in the centralized model (where a trusted curator collects the sensitive information and analyzes it with differential privacy) and in the local model (where each respondent randomizes her answers to the data curator to protect her privacy).   We demonstrate how to use our algorithms as a building block for approximating \$k\$-means in both models.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\IGT9G6P6\\Nissim and Stemmer - 2018 - Clustering Algorithms for the Centralized and Loca.pdf}
}

@inproceedings{nissim_smooth_2007,
  title = {Smooth Sensitivity and Sampling in Private Data Analysis},
  booktitle = {Proceedings of the Thirty-Ninth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
  year = {2007},
  month = jun,
  pages = {75--84},
  publisher = {{ACM}},
  address = {{San Diego California USA}},
  doi = {10.1145/1250790.1250803},
  urldate = {2023-05-04},
  abstract = {We introduce a new, generic framework for private data analysis. The goal of private data analysis is to release aggregate information about a data set while protecting the privacy of the individuals whose information the data set contains. Our framework allows one to release functions f of the data with instance-specific additive noise. That is, the noise magnitude is determined not only by the function we want to release, but also by the database itself. One of the challenges is to ensure that the noise magnitude does not leak information about the database. To address that, we calibrate the noise magnitude to the smooth sensitivity of f on the database x \textemdash{} a measure of variability of f in the neighborhood of the instance x. The new framework greatly expands the applicability of output perturbation, a technique for protecting individuals' privacy by adding a small amount of random noise to the released statistics. To our knowledge, this is the first formal analysis of the effect of instance-specific noise in the context of data privacy.},
  isbn = {978-1-59593-631-8},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\JCRGPZCJ\\Nissim e.a. - 2007 - Smooth sensitivity and sampling in private data an.pdf}
}

@misc{noauthor_adversarial_2023,
  title = {Adversarial {{Robustness Toolbox}} ({{ART}}) v1.14},
  year = {2023},
  month = apr,
  urldate = {2023-04-17},
  abstract = {Adversarial Robustness Toolbox (ART) - Python Library for Machine Learning Security - Evasion, Poisoning, Extraction, Inference - Red and Blue Teams},
  copyright = {MIT},
  howpublished = {Trusted-AI},
  keywords = {adversarial-attacks,adversarial-examples,adversarial-machine-learning,ai,artificial-intelligence,attack,blue-team,evasion,extraction,inference,machine-learning,poisoning,privacy,python,red-team,trusted-ai,trustworthy-ai}
}

@misc{noauthor_wayback_2020,
  title = {Wayback {{Machine}}},
  year = {2020},
  month = sep,
  urldate = {2023-03-16},
  howpublished = {https://web.archive.org/web/20200913222203/https://arxiv.org/ftp/arxiv/papers/1410/1410.7744.pdf},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\M56L7GUH\\2020 - Wayback Machine.pdf}
}

@inproceedings{oya_is_2017,
  title = {Is {{Geo-Indistinguishability What You Are Looking}} For?},
  booktitle = {Proceedings of the 2017 on {{Workshop}} on {{Privacy}} in the {{Electronic Society}}},
  author = {Oya, Simon and Troncoso, Carmela and {P{\'e}rez-Gonz{\'a}lez}, Fernando},
  year = {2017},
  month = oct,
  pages = {137--140},
  publisher = {{ACM}},
  address = {{Dallas Texas USA}},
  doi = {10.1145/3139550.3139555},
  urldate = {2023-02-28},
  abstract = {Since its proposal in 2013, geo-indistinguishability has been consolidated as a formal notion of location privacy, generating a rich body of literature building on this idea. A problem with most of these follow-up works is that they blindly rely on geo-indistinguishability to provide location privacy, ignoring the numerical interpretation of this privacy guarantee. In this paper we provide an alternative formulation of geo-indistinguishability as an adversary error, and use it to show that the privacy vs. utility trade-off that can be obtained is not as appealing as implied by the literature. We also show that although geo-indistinguishability guarantees a lower bound on the adversary's error, this comes at the cost of achieving poorer performance than other noise generation mechanisms in terms of average error, and enabling the possibility of exposing obfuscated locations that are useless from the quality of service point of view.},
  isbn = {978-1-4503-5175-1},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\YFSMMX5M\\Oya et al. - 2017 - Is Geo-Indistinguishability What You Are Looking f.pdf}
}

@article{peng_unsupervised_nodate,
  title = {Unsupervised {{Membership Inference Attacks Against Machine Learning Models}}},
  author = {Peng, Yuefeng and Zhao, Bo and Liu, Hui},
  abstract = {As a form of privacy leakage for machine learning (ML), membership inference (MI) attacks aim to infer whether given data samples have been used to train a target ML model. Existing state-of-the-art MI attacks in black-box settings adopt a so-called shadow model to perform transfer attacks. Such attacks achieve high inference accuracy but have many adversarial assumptions, such as having a dataset from the same distribution as the target model's training data and knowledge of the target model structure. We propose a novel MI attack, called UMIA, which probes the target model in an unsupervised way without any shadow model. We relax all the adversarial assumptions above, demonstrating that MI attacks are applicable without any knowledge about the target model and its training set. We empirically show that, with far fewer adversarial assumptions and computational resources, UMIA can perform on bar with the state-of-the-art supervised MI attack.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8KGUBBPY\\Peng e.a. - Unsupervised Membership Inference Attacks Against .pdf}
}

@article{rand_objective_1971,
  title = {Objective Criteria for the Evaluation of Clustering Methods},
  author = {Rand, William M},
  year = {1971},
  journal = {Journal of the American Statistical association},
  volume = {66},
  number = {336},
  pages = {846--850},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459}
}

@misc{rigaki_survey_2021,
  title = {A {{Survey}} of {{Privacy Attacks}} in {{Machine Learning}}},
  author = {Rigaki, Maria and Garcia, Sebastian},
  year = {2021},
  month = apr,
  number = {arXiv:2007.07646},
  eprint = {2007.07646},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 40 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TYQY8NSE\\Rigaki en Garcia - 2021 - A Survey of Privacy Attacks in Machine Learning.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\9T48F9DU\\2007.html}
}

@article{rousseeuw_silhouettes_1987,
  title = {Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis},
  author = {Rousseeuw, Peter J},
  year = {1987},
  journal = {Journal of computational and applied mathematics},
  volume = {20},
  pages = {53--65},
  publisher = {{Elsevier}},
  issn = {0377-0427}
}

@article{sander_density-based_1998,
  title = {Density-{{Based Clustering}} in {{Spatial Databases}}: {{The Algorithm GDBSCAN}} and {{Its Applications}}},
  shorttitle = {Density-{{Based Clustering}} in {{Spatial Databases}}},
  author = {Sander, J{\"o}rg and Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  year = {1998},
  month = jun,
  journal = {Data Mining and Knowledge Discovery},
  volume = {2},
  number = {2},
  pages = {169--194},
  issn = {1573-756X},
  doi = {10.1023/A:1009745219419},
  urldate = {2023-04-13},
  abstract = {The clustering algorithm DBSCAN relies on a density-based notion of clusters and is designed to discover clusters of arbitrary shape as well as to distinguish noise. In this paper, we generalize this algorithm in two important directions. The generalized algorithm\textemdash called GDBSCAN\textemdash can cluster point objects as well as spatially extended objects according to both, their spatial and their nonspatial attributes. In addition, four applications using 2D points (astronomy), 3D points (biology), 5D points (earth science) and 2D polygons (geography) are presented, demonstrating the applicability of GDBSCAN to real-world problems.},
  langid = {english},
  keywords = {applications,clustering algorithms,efficiency,spatial databases},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7WSLG23Z\\Sander e.a. - 1998 - Density-Based Clustering in Spatial Databases The.pdf}
}

@inproceedings{saputra_effect_2020,
  title = {Effect of Distance Metrics in Determining K-Value in k-Means Clustering Using Elbow and Silhouette Method},
  booktitle = {Sriwijaya {{International Conference}} on {{Information Technology}} and {{Its Applications}} ({{SICONIAN}} 2019)},
  author = {SAPUTRA, Danny Matthew and SAPUTRA, Daniel and OSWARI, Liniyanti D},
  year = {2020},
  pages = {341--346},
  publisher = {{Atlantis Press}},
  isbn = {94-6252-963-9}
}

@article{schubert_dbscan_2017,
  title = {{{DBSCAN Revisited}}, {{Revisited}}: {{Why}} and {{How You Should}} ({{Still}}) {{Use DBSCAN}}},
  shorttitle = {{{DBSCAN Revisited}}, {{Revisited}}},
  author = {Schubert, Erich and Sander, J{\"o}rg and Ester, Martin and Kriegel, Hans Peter and Xu, Xiaowei},
  year = {2017},
  month = sep,
  journal = {ACM Transactions on Database Systems},
  volume = {42},
  number = {3},
  pages = {1--21},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/3068335},
  urldate = {2023-04-13},
  abstract = {At SIGMOD 2015, an article was presented with the title ``DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation'' that won the conference's best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\N7B96WT6\\Schubert e.a. - 2017 - DBSCAN Revisited, Revisited Why and How You Shoul.pdf}
}

@inproceedings{shejwalkar_revisiting_2019,
  title = {Revisiting Utility Metrics for Location Privacy-Preserving Mechanisms},
  booktitle = {Proceedings of the 35th {{Annual Computer Security Applications Conference}}},
  author = {Shejwalkar, Virat and Houmansadr, Amir and {Pishro-Nik}, Hossein and Goeckel, Dennis},
  year = {2019},
  month = dec,
  pages = {313--327},
  publisher = {{ACM}},
  address = {{San Juan Puerto Rico USA}},
  doi = {10.1145/3359789.3359829},
  urldate = {2023-03-17},
  abstract = {The literature has extensively studied various location privacypreserving mechanisms (LPPMs) in order to improve the location privacy of the users of location-based services (LBSes). Such privacy, however, comes at the cost of degrading the utility of the underlying LBSes. The main body of previous work has used a generic distance-only based metric to quantify the quality loss incurred while employing LPPMs. In this paper, we argue that using such generic utility metrics misleads the design and evaluation of LPPMs, since generic utility metrics do not capture the actual utility perceived by the users. We demonstrate this for ride-hailing services, a popular class of LBS with complex utility behavior. Specifically, we design a privacy-preserving ride-hailing service, called PRide, and demonstrate the significant distinction between its generic and tailored metrics. Through various experiments we show the significant implications of using generic utility metrics in the design and evaluation of LPPMs. Our work concludes that LPPM design and evaluation should use utility metrics that are tailored to the individual LBSes.},
  isbn = {978-1-4503-7628-0},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NS6QKNSZ\\Shejwalkar et al. - 2019 - Revisiting utility metrics for location privacy-pr.pdf}
}

@misc{shokri_membership_2017,
  title = {Membership {{Inference Attacks}} against {{Machine Learning Models}}},
  author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  year = {2017},
  month = mar,
  number = {arXiv:1610.05820},
  eprint = {1610.05820},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-03-30},
  abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\HVDK3UKI\\Shokri e.a. - 2017 - Membership Inference Attacks against Machine Learn.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\QBBIYVXD\\1610.html}
}

@inproceedings{shokri_privacy-preserving_2015,
  title = {Privacy-Preserving Deep Learning},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC}} Conference on Computer and Communications Security},
  author = {Shokri, Reza and Shmatikov, Vitaly},
  year = {2015},
  pages = {1310--1321}
}

@article{solorio-fernandez_review_2020,
  title = {A Review of Unsupervised Feature Selection Methods},
  author = {{Solorio-Fern{\'a}ndez}, Sa{\'u}l and {Carrasco-Ochoa}, J. Ariel and {Mart{\'i}nez-Trinidad}, Jos{\'e} Fco.},
  year = {2020},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {2},
  pages = {907--948},
  issn = {1573-7462},
  doi = {10.1007/s10462-019-09682-y},
  abstract = {In recent years, unsupervised feature selection methods have raised considerable interest in many research areas; this is mainly due to their ability to identify and select relevant features without needing class label information. In this paper, we provide a comprehensive and structured review of the most relevant and recent unsupervised feature selection methods reported in the literature. We present a taxonomy of these methods and describe the main characteristics and the fundamental ideas they are based on. Additionally, we summarized the advantages and disadvantages of the general lines in which we have categorized the methods analyzed in this review. Moreover, an experimental comparison among the most representative methods of each approach is also presented. Finally, we discuss some important open challenges in this research area.}
}

@article{soria-comas_optimal_2013,
  title = {Optimal Data-Independent Noise for Differential Privacy},
  author = {{Soria-Comas}, Jordi and {Domingo-Ferrer}, Josep},
  year = {2013},
  month = nov,
  journal = {Information Sciences},
  volume = {250},
  pages = {200--214},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2013.07.004},
  urldate = {2023-05-25},
  abstract = {{$\epsilon$}-Differential privacy is a property that seeks to characterize privacy in data sets. It is formulated as a query-response method, and computationally achieved by output perturbation. Several noise-addition methods to implement such output perturbation have been proposed in the literature. We focus on data-independent noise, that is, noise whose distribution is constant across data sets. Our goal is to find the optimal data-independent noise distribution to achieve {$\epsilon$}-differential privacy. We propose a general optimality criterion based on the concentration of the probability mass of the noise distribution around zero, and we show that any noise optimal under this criterion must be optimal under any other sensible criterion. We also show that the Laplace distribution, commonly used for noise in {$\epsilon$}-differential privacy, is not optimal, and we build the optimal data-independent noise distribution. We compare the Laplace and the optimal data-independent noise distributions. For univariate query functions, both introduce a similar level of distortion; for multivariate query functions, optimal data-independent noise offers responses with substantially better data quality.},
  langid = {english},
  keywords = {Data privacy,Differential privacy,Noise addition,Privacy-preserving data mining,Statistical disclosure control},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\9CH6P5VY\\Soria-Comas en Domingo-Ferrer - 2013 - Optimal data-independent noise for differential pr.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\JSSWRYY6\\S0020025513005094.html}
}

@article{stemmer_locally_2021,
  title = {Locally Private K-Means Clustering},
  author = {Stemmer, Uri},
  year = {2021},
  journal = {The Journal of Machine Learning Research},
  volume = {22},
  number = {1},
  pages = {7964--7993},
  publisher = {{JMLRORG}},
  issn = {1532-4435}
}

@article{strehl_cluster_2002,
  title = {Cluster Ensembles---a Knowledge Reuse Framework for Combining Multiple Partitions},
  author = {Strehl, Alexander and Ghosh, Joydeep},
  year = {2002},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Dec},
  pages = {583--617},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\CTQ3ZAYE\\Strehl en Ghosh - Cluster Ensembles – A Knowledge Reuse Framework fo.pdf}
}

@misc{su_differentially_2015,
  title = {Differentially {{Private}} \$k\$-{{Means Clustering}}},
  author = {Su, Dong and Cao, Jianneng and Li, Ninghui and Bertino, Elisa and Jin, Hongxia},
  year = {2015},
  month = apr,
  number = {arXiv:1504.05998},
  eprint = {1504.05998},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1504.05998},
  urldate = {2023-05-14},
  abstract = {There are two broad approaches for differentially private data analysis. The interactive approach aims at developing customized differentially private algorithms for various data mining tasks. The non-interactive approach aims at developing differentially private algorithms that can output a synopsis of the input dataset, which can then be used to support various data mining tasks. In this paper we study the tradeoff of interactive vs. non-interactive approaches and propose a hybrid approach that combines interactive and non-interactive, using \$k\$-means clustering as an example. In the hybrid approach to differentially private \$k\$-means clustering, one first uses a non-interactive mechanism to publish a synopsis of the input dataset, then applies the standard \$k\$-means clustering algorithm to learn \$k\$ cluster centroids, and finally uses an interactive approach to further improve these cluster centroids. We analyze the error behavior of both non-interactive and interactive approaches and use such analysis to decide how to allocate privacy budget between the non-interactive step and the interactive step. Results from extensive experiments support our analysis and demonstrate the effectiveness of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\NLFCUWY5\\Su et al. - 2015 - Differentially Private $k$-Means Clustering.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\ZMQ4SR8U\\1504.html}
}

@misc{su_differentially_2015-1,
  title = {Differentially {{Private}} \$k\$-{{Means Clustering}}},
  author = {Su, Dong and Cao, Jianneng and Li, Ninghui and Bertino, Elisa and Jin, Hongxia},
  year = {2015},
  month = apr,
  number = {arXiv:1504.05998},
  eprint = {1504.05998},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1504.05998},
  urldate = {2023-05-13},
  abstract = {There are two broad approaches for differentially private data analysis. The interactive approach aims at developing customized differentially private algorithms for various data mining tasks. The non-interactive approach aims at developing differentially private algorithms that can output a synopsis of the input dataset, which can then be used to support various data mining tasks. In this paper we study the tradeoff of interactive vs. non-interactive approaches and propose a hybrid approach that combines interactive and non-interactive, using \$k\$-means clustering as an example. In the hybrid approach to differentially private \$k\$-means clustering, one first uses a non-interactive mechanism to publish a synopsis of the input dataset, then applies the standard \$k\$-means clustering algorithm to learn \$k\$ cluster centroids, and finally uses an interactive approach to further improve these cluster centroids. We analyze the error behavior of both non-interactive and interactive approaches and use such analysis to decide how to allocate privacy budget between the non-interactive step and the interactive step. Results from extensive experiments support our analysis and demonstrate the effectiveness of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\N43DH4MK\\Su et al. - 2015 - Differentially Private $k$-Means Clustering.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\VDQZ96GA\\1504.html}
}

@misc{su_differentially_2015-2,
  title = {Differentially {{Private}} \$k\$-{{Means Clustering}}},
  author = {Su, Dong and Cao, Jianneng and Li, Ninghui and Bertino, Elisa and Jin, Hongxia},
  year = {2015},
  month = apr,
  number = {arXiv:1504.05998},
  eprint = {1504.05998},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1504.05998},
  urldate = {2023-05-27},
  abstract = {There are two broad approaches for differentially private data analysis. The interactive approach aims at developing customized differentially private algorithms for various data mining tasks. The non-interactive approach aims at developing differentially private algorithms that can output a synopsis of the input dataset, which can then be used to support various data mining tasks. In this paper we study the tradeoff of interactive vs. non-interactive approaches and propose a hybrid approach that combines interactive and non-interactive, using \$k\$-means clustering as an example. In the hybrid approach to differentially private \$k\$-means clustering, one first uses a non-interactive mechanism to publish a synopsis of the input dataset, then applies the standard \$k\$-means clustering algorithm to learn \$k\$ cluster centroids, and finally uses an interactive approach to further improve these cluster centroids. We analyze the error behavior of both non-interactive and interactive approaches and use such analysis to decide how to allocate privacy budget between the non-interactive step and the interactive step. Results from extensive experiments support our analysis and demonstrate the effectiveness of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\XQMLE6TE\\Su e.a. - 2015 - Differentially Private $k$-Means Clustering.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\5CMB2C96\\1504.html}
}

@misc{sun_distributed_2019,
  title = {Distributed {{Clustering}} in the {{Anonymized Space}} with {{Local Differential Privacy}}},
  author = {Sun, Lin and Zhao, Jun and Ye, Xiaojun},
  year = {2019},
  month = jun,
  number = {arXiv:1906.11441},
  eprint = {1906.11441},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-12-22},
  abstract = {Clustering and analyzing on collected data can improve user experiences and quality of services in big data, IoT applications. However, directly releasing original data brings potential privacy concerns, which raises challenges and opportunities for privacy-preserving clustering. In this paper, we study the problem of non-interactive clustering in distributed setting under the framework of local differential privacy. We first extend the Bit Vector, a novel anonymization mechanism to be functionality-capable and privacy-preserving. Based on the modified encoding mechanism, we propose kCluster algorithm that can be used for clustering in the anonymized space. We show the modified encoding mechanism can be easily implemented in existing clustering algorithms that only rely on distance information, such as DBSCAN. Theoretical analysis and experimental results validate the effectiveness of the proposed schemes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Databases},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8VTXE9HS\\Sun e.a. - 2019 - Distributed Clustering in the Anonymized Space wit.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\IRX6PFZC\\1906.html}
}

@article{sun_privbv_2022,
  title = {{{PrivBV}}: {{Distance-aware}} Encoding for Distributed Data with Local Differential Privacy},
  shorttitle = {{{PrivBV}}},
  author = {Sun, Lin and Ping, Guolou and Ye, Xiaojun},
  year = {2022},
  month = apr,
  journal = {Tsinghua Science and Technology},
  volume = {27},
  number = {2},
  pages = {412--421},
  issn = {1007-0214},
  doi = {10.26599/TST.2021.9010027},
  abstract = {Recently, local differential privacy (LDP) has been used as the de facto standard for data sharing and analyzing with high-level privacy guarantees. Existing LDP-based mechanisms mainly focus on learning statistical information about the entire population from sensitive data. For the first time in the literature, we use LDP for distance estimation between distributed datato support more complicated data analysis. Specifically, we propose PrivBV\textemdash a locally differentially private bit vector mechanism with a distance-aware property in the anonymized space. We also present an optimization strategy for reducing privacy leakage in the high-dimensional space. The distance-aware property of PrivBV brings new insights into complicated data analysis in distributed environments. As study cases, we show the feasibility of applying PrivBV to privacy-preserving record linkage and non-interactive clustering. Theoretical analysis and experimental results demonstrate the effectiveness of the proposed scheme.},
  keywords = {Data analysis,Differential privacy,Distributed databases,Encoding,Estimation,local differential privacy,non-interactive clustering,Privacy,privacy-preserving data publishing,Sun},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\B8WDVH5J\\Sun e.a. - 2022 - PrivBV Distance-aware encoding for distributed da.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\JPL3C98G\\9552667.html}
}

@article{tibshirani_estimating_2001,
  title = {Estimating the Number of Clusters in a Data Set via the Gap Statistic},
  author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
  year = {2001},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {63},
  number = {2},
  pages = {411--423},
  publisher = {{Wiley Online Library}},
  issn = {1369-7412}
}

@misc{valdenegro-toro_machine_2022,
  title = {Machine {{Learning Students Overfit}} to {{Overfitting}}},
  author = {{Valdenegro-Toro}, Matias and Sabatelli, Matthia},
  year = {2022},
  month = sep,
  number = {arXiv:2209.03032},
  eprint = {2209.03032},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-13},
  abstract = {Overfitting and generalization is an important concept in Machine Learning as only models that generalize are interesting for general applications. Yet some students have trouble learning this important concept through lectures and exercises. In this paper we describe common examples of students misunderstanding overfitting, and provide recommendations for possible solutions. We cover student misconceptions about overfitting, about solutions to overfitting, and implementation mistakes that are commonly confused with overfitting issues. We expect that our paper can contribute to improving student understanding and lectures about this important topic.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BS8SVVPZ\\Valdenegro-Toro en Sabatelli - 2022 - Machine Learning Students Overfit to Overfitting.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\E7UT2LZS\\2209.html}
}

@article{vinh_information_nodate-2,
  title = {Information {{Theoretic Measures}} for {{Clusterings Comparison}}: {{Variants}}, {{Properties}}, {{Normalization}} and {{Correction}} for {{Chance}}},
  author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
  abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0, 1] range better than other normalized variants.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\BB3A2I9Q\\Vinh e.a. - Information Theoretic Measures for Clusterings Com.pdf}
}

@article{wagner_comparing_nodate,
  title = {Comparing {{Clusterings}} - {{An Overview}}},
  author = {Wagner, Silke and Wagner, Dorothea},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\P4IVDUGW\\Wagner en Wagner - Comparing Clusterings - An Overview.pdf}
}

@article{wang_adaptive_2007,
  title = {Adaptive {{Affinity Propagation Clustering}}},
  author = {Wang, Kaijun and Zhang, Junying and Li, Dan and Zhang, Xinna and Guo, Tao},
  year = {2007},
  abstract = {Affinity propagation clustering (AP) has two limitations: it is hard to know what value of parameter `preference' can yield an optimal clustering solution, and oscillations cannot be eliminated automatically if occur. The adaptive AP method is proposed to overcome these limitations, including adaptive scanning of preferences to search space of the number of clusters for finding the optimal clustering solution, adaptive adjustment of damping factors to eliminate oscillations, and adaptive escaping from oscillations when the damping adjustment technique fails. Experimental results on simulated and real data sets show that the adaptive AP is effective and can outperform AP in quality of clustering results.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\UFTDF2JZ\\Wang e.a. - 2007 - Adaptive Affinity Propagation Clustering.pdf}
}

@misc{wang_collecting_2019,
  title = {Collecting and {{Analyzing Multidimensional Data}} with {{Local Differential Privacy}}},
  author = {Wang, Ning and Xiao, Xiaokui and Yang, Yin and Zhao, Jun and Hui, Siu Cheung and Shin, Hyejin and Shin, Junbum and Yu, Ge},
  year = {2019},
  month = jun,
  number = {arXiv:1907.00782},
  eprint = {1907.00782},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-13},
  abstract = {Local differential privacy (LDP) is a recently proposed privacy standard for collecting and analyzing data, which has been used, e.g., in the Chrome browser, iOS and macOS. In LDP, each user perturbs her information locally, and only sends the randomized version to an aggregator who performs analyses, which protects both the users and the aggregator against private information leaks. Although LDP has attracted much research attention in recent years, the majority of existing work focuses on applying LDP to complex data and/or analysis tasks. In this paper, we point out that the fundamental problem of collecting multidimensional data under LDP has not been addressed sufficiently, and there remains much room for improvement even for basic tasks such as computing the mean value over a single numeric attribute under LDP. Motivated by this, we first propose novel LDP mechanisms for collecting a numeric attribute, whose accuracy is at least no worse (and usually better) than existing solutions in terms of worst-case noise variance. Then, we extend these mechanisms to multidimensional data that can contain both numeric and categorical attributes, where our mechanisms always outperform existing solutions regarding worst-case noise variance. As a case study, we apply our solutions to build an LDP-compliant stochastic gradient descent algorithm (SGD), which powers many important machine learning tasks. Experiments using real datasets confirm the effectiveness of our methods, and their advantages over existing solutions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Databases,Computer Science - Machine Learning,{Local differential privacy, multidimensional data, stochastic gradient descent}},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\TQHUB828\\Wang et al. - 2019 - Collecting and Analyzing Multidimensional Data wit.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\LFBZ2YWQ\\1907.html}
}

@article{wang_locally_nodate,
  title = {Locally {{Differentially Private Protocols}} for {{Frequency Estimation}}},
  author = {Wang, Tianhao and Blocki, Jeremiah and Li, Ninghui and Jha, Somesh},
  abstract = {Protocols satisfying Local Differential Privacy (LDP) enable parties to collect aggregate information about a population while protecting each user's privacy, without relying on a trusted third party. LDP protocols (such as Google's RAPPOR) have been deployed in real-world scenarios. In these protocols, a user encodes his private information and perturbs the encoded value locally before sending it to an aggregator, who combines values that users contribute to infer statistics about the population. In this paper, we introduce a framework that generalizes several LDP protocols proposed in the literature. Our framework yields a simple and fast aggregation algorithm, whose accuracy can be precisely analyzed. Our in-depth analysis enables us to choose optimal parameters, resulting in two new protocols (i.e., Optimized Unary Encoding and Optimized Local Hashing) that provide better utility than protocols previously proposed. We present precise conditions for when each proposed protocol should be used, and perform experiments that demonstrate the advantage of our proposed protocols.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\MG77ETR5\\Wang et al. - Locally Differentially Private Protocols for Frequ.pdf}
}

@article{warner_randomized_1965-1,
  title = {Randomized Response: {{A}} Survey Technique for Eliminating Evasive Answer Bias},
  author = {Warner, Stanley L},
  year = {1965},
  journal = {Journal of the American Statistical Association},
  volume = {60},
  number = {309},
  pages = {63--69},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459}
}

@article{warrens_understanding_2022,
  title = {Understanding the {{Adjusted Rand Index}} and {{Other Partition Comparison Indices Based}} on {{Counting Object Pairs}}},
  author = {Warrens, Matthijs J. and {van der Hoef}, Hanneke},
  year = {2022},
  month = nov,
  journal = {Journal of Classification},
  volume = {39},
  number = {3},
  pages = {487--509},
  issn = {1432-1343},
  doi = {10.1007/s00357-022-09413-z},
  abstract = {In unsupervised machine learning, agreement between partitions is commonly assessed with so-called external validity indices. Researchers tend to use and report indices that quantify agreement between two partitions for all clusters simultaneously. Commonly used examples are the Rand index and the adjusted Rand index. Since these overall measures give a general notion of what is going on, their values are usually hard to interpret. The goal of this study is to provide a thorough understanding of the adjusted Rand index as well as many other partition comparison indices based on counting object pairs. It is shown that many overall indices based on the pair-counting approach can be decomposed into indices that reflect the degree of agreement on the level of individual clusters. The decompositions (1) show that the overall indices can be interpreted as summary statistics of the agreement on the cluster level, (2) specify how these overall indices are related to the indices for individual clusters, and (3) show that the overall indices are affected by cluster size imbalance: if cluster sizes are unbalanced these overall measures will primarily reflect the degree of agreement between the partitions on the large clusters, and will provide much less information on the agreement on smaller clusters. Furthermore, the value of Rand-like indices is determined to a large extent by the number of pairs of objects that are not joined in either of the partitions.}
}

@article{wood_differential_2018,
  title = {Differential {{Privacy}}: {{A Primer}} for a {{Non-Technical Audience}}},
  shorttitle = {Differential {{Privacy}}},
  author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
  year = {2018},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3338027},
  urldate = {2023-03-17},
  abstract = {This document is a primer on differential privacy, which is a formal mathematical framework for guaranteeing privacy protection when analyzing or releasing statistical data. Recently emerging from the theoretical computer science literature, differential privacy is now in initial stages of implementation and use in various academic, industry, and government settings. Using intuitive illustrations and limited mathematical formalism, this document provides an introduction to differential privacy for non-technical practitioners, who are increasingly tasked with making decisions with respect to differential privacy as it grows more widespread in use. In particular, the examples in this document illustrate ways in which social scientists can conceptualize the guarantees provided by differential privacy with respect to the decisions they make when managing personal data about research subjects and informing them about the privacy protection they will be afforded.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\Y9WGZU9N\\Wood et al. - 2018 - Differential Privacy A Primer for a Non-Technical.pdf}
}

@article{xia_distributed_2020-1,
  title = {Distributed {{K-Means}} Clustering Guaranteeing Local Differential Privacy},
  author = {Xia, Chang and Hua, Jingyu and Tong, Wei and Zhong, Sheng},
  year = {2020},
  journal = {Computers \& Security},
  volume = {90},
  pages = {101699},
  publisher = {{Elsevier}},
  issn = {0167-4048}
}

@article{yan_efficient_2020,
  title = {An Efficient Unsupervised Feature Selection Procedure through Feature Clustering},
  author = {Yan, Xuyang and Nazmi, Shabnam and Erol, Berat A. and Homaifar, Abdollah and Gebru, Biniam and Tunstel, Edward},
  year = {2020},
  month = mar,
  journal = {Pattern Recognition Letters},
  volume = {131},
  pages = {277--284},
  issn = {01678655},
  doi = {10.1016/j.patrec.2019.12.022},
  urldate = {2023-02-15},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8Z855R66\\Yan e.a. - 2020 - An efficient unsupervised feature selection proced.pdf}
}

@article{yan_perturb_2022-2,
  title = {Perturb and Optimize Users' Location Privacy Using Geo-Indistinguishability and Location Semantics},
  author = {Yan, Yan and Xu, Fei and Mahmood, Adnan and Dong, Zhuoyue and Sheng, Quan Z.},
  year = {2022},
  month = nov,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {20445},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-24893-0},
  urldate = {2023-02-26},
  abstract = {Location-based services (LBS) are capable of providing location-based information retrieval, traffic navigation, entertainment services, emergency rescues, and several similar services primarily on the premise of the geographic location of users or mobile devices. However, in the process of introducing a new user experience, it is also easy to expose users' specific location which can result in more private information leakage. Hence, the protection of location privacy remains one of the critical issues of the location-based services. Moreover, the areas where humans work and live have different location semantics and sensitivities according to their different social functions. Although the privacy protection of a user's real location can be achieved by the perturbation algorithm, the attackers may employ the semantics information of the perturbed location to infer a user's real location semantics in an attempt to spy on a user's privacy to certain extent. In order to mitigate the above semantics inference attack, and further improve the quality of the location-based services, this paper hereby proposes a user side location perturbation and optimization algorithm based on geo-indistinguishability and location semantics. The perturbation area satisfying geo-indistinguishability is thus generated according to the planar Laplace mechanism and optimized by combining the semantics information and time characteristics of the location. The optimum perturbed location that is able to satisfy the minimum loss of location-based service quality is selected via a linear programming method, and can be employed to replace the real location of the user so as to prevent the leakage of the privacy. Experimental comparison of the actual road network and location semantics dataset manifests that the proposed method reduces approximately 37\% perturbation distance in contrast to the other state-of-the-art methods, maintains considerably lower similarity of location semantics, and improves region counting query accuracy by a margin of around 40\%.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Energy science and technology,Mathematics and computing},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\8VFPAXY2\\Yan et al. - 2022 - Perturb and optimize users’ location privacy using.pdf}
}

@article{yang_k-means_2022,
  title = {K-{{Means Clustering}} with {{Local Distance Privacy}}},
  author = {Yang, Mengmeng and Huang, Longxia and Tang, Chenghua},
  year = {2022},
  journal = {Big Data Mining and Analytics},
  abstract = {With the development of information technology, a mass of data are generated every day. Collecting and analysing these data help service providers improve their services and gain an advantage in the fierce market competition. K-means clustering has been widely used for cluster analysis in real life. However, these analyses are based on users' data, which disclose users' privacy. Local differential privacy has attracted lots of attention recently due to its strong privacy guarantee and has been applied for clustering analysis. However, existing K-means clustering methods with local differential privacy protection cannot get an ideal clustering result due to the large amount of noise introduced to the whole dataset to ensure the privacy guarantee. To solve this problem, we propose a novel method that provides local distance privacy for users who participate in the clustering analysis. Instead of making the users' records in-distinguish from each other in high-dimensional space, we map the user's record into a one-dimensional distance space and make the records in such a distance space not be distinguished from each other. To be specific, we generate a noisy distance first and then synthesise the high-dimensional data record. We propose a Bounded Laplace Method (BLM) and a Cluster Indistinguishable Method (CIM) to sample such a noisy distance, which satisfies the local differential privacy guarantee and local dE -privacy guarantee, respectively. Furthermore, we introduce a way to generate synthetic data records in high-dimensional space. Our experimental evaluation results show that our methods outperform the traditional methods significantly.},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\ZHY4NLX7\\Yang et al. - 2022 - K-Means Clustering with Local Distance Privacy.pdf}
}

@inproceedings{yeom_privacy_2018,
  title = {Privacy {{Risk}} in {{Machine Learning}}: {{Analyzing}} the {{Connection}} to {{Overfitting}}},
  shorttitle = {Privacy {{Risk}} in {{Machine Learning}}},
  booktitle = {2018 {{IEEE}} 31st {{Computer Security Foundations Symposium}} ({{CSF}})},
  author = {Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  year = {2018},
  month = jul,
  pages = {268--282},
  publisher = {{IEEE}},
  address = {{Oxford}},
  doi = {10.1109/CSF.2018.00027},
  urldate = {2023-03-30},
  abstract = {Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.},
  isbn = {978-1-5386-6680-7},
  langid = {english},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\7SZ28HF8\\Yeom e.a. - 2018 - Privacy Risk in Machine Learning Analyzing the Co.pdf}
}

@article{yuan_privacypreserving_2021,
  title = {Privacy-preserving Mechanism for Mixed Data Clustering with Local Differential Privacy},
  author = {Yuan, Liujie and Zhang, Shaobo and Zhu, Gengming and Alinani, Karim},
  year = {2021},
  month = jul,
  journal = {Concurrency and Computation: Practice and Experience},
  issn = {1532-0626, 1532-0634},
  doi = {10.1002/cpe.6503},
  urldate = {2023-05-24},
  langid = {english}
}

@article{yuan_research_2019,
  title = {Research on {{K-value}} Selection Method of {{K-means}} Clustering Algorithm},
  author = {Yuan, Chunhui and Yang, Haitao},
  year = {2019},
  journal = {J},
  volume = {2},
  number = {2},
  pages = {226--235},
  publisher = {{MDPI}},
  issn = {2571-8800}
}

@inproceedings{zhao_not_2020,
  title = {Not One but Many {{Tradeoffs}}: {{Privacy Vs}}. {{Utility}} in {{Differentially Private Machine Learning}}},
  shorttitle = {Not One but Many {{Tradeoffs}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGSAC Conference}} on {{Cloud Computing Security Workshop}}},
  author = {Zhao, Benjamin Zi Hao and Kaafar, Mohamed Ali and Kourtellis, Nicolas},
  year = {2020},
  month = nov,
  eprint = {2008.08807},
  primaryclass = {cs},
  pages = {15--26},
  doi = {10.1145/3411495.3421352},
  urldate = {2023-04-08},
  abstract = {Data holders are increasingly seeking to protect their user's privacy, whilst still maximizing their ability to produce machine models with high quality predictions. In this work, we empirically evaluate various implementations of differential privacy (DP), and measure their ability to fend off real-world privacy attacks, in addition to measuring their core goal of providing accurate classifications. We establish an evaluation framework to ensure each of these implementations are fairly evaluated. Our selection of DP implementations add DP noise at different positions within the framework, either at the point of data collection/release, during updates while training of the model, or after training by perturbing learned model parameters. We evaluate each implementation across a range of privacy budgets, and datasets, each implementation providing the same mathematical privacy guarantees. By measuring the models' resistance to real world attacks of membership and attribute inference, and their classification accuracy. we determine which implementations provide the most desirable tradeoff between privacy and utility. We found that the number of classes of a given dataset is unlikely to influence where the privacy and utility tradeoff occurs. Additionally, in the scenario that high privacy constraints are required, perturbing input training data does not trade off as much utility, as compared to noise added later in the ML process.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {C\:\\Users\\tjvan\\Zotero\\storage\\F2TURZMZ\\Zhao et al. - 2020 - Not one but many Tradeoffs Privacy Vs. Utility in.pdf;C\:\\Users\\tjvan\\Zotero\\storage\\6985TBUZ\\2008.html}
}
